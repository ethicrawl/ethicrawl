{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ethicrawl","text":"<p>Ethicrawl is a Python library for ethical, professional-grade web crawling. It automatically respects robots.txt, enforces rate limits, and offers robust sitemap parsing and domain control\u2014making it easy to build reliable and responsible crawlers.</p>"},{"location":"#project-goals","title":"Project Goals","text":"<p>Ethicrawl is built on the principle that web crawling should be:</p> <ul> <li>Ethical by Design: Automatically respects robots.txt and rate limits, ensuring responsible web crawling.</li> <li>Server-Safe: Prevents accidental overloading with built-in safeguards.</li> <li>Feature-Rich: Includes robust sitemap parsing, domain control, and flexible configuration.</li> <li>Extensible &amp; Customizable: Easily adapts to diverse crawling needs through flexible settings and clean architecture.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Robots.txt Compliance: Automatic parsing and enforcement of robots.txt rules</li> <li>Rate Limiting: Built-in, configurable request rate management</li> <li>Sitemap Support: Parse and filter XML sitemaps to discover content</li> <li>Domain Control: Explicit whitelisting for cross-domain access</li> <li>Flexible Configuration: Easily configure all aspects of crawling behavior</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Comprehensive documentation is available at https://ethicrawl.github.io/ethicrawl/</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version from PyPI:</p> <pre><code>pip install ethicrawl\n</code></pre> <p>For development:</p> <pre><code># Clone the repository\ngit clone https://github.com/ethicrawl/ethicrawl.git\n\n# Navigate to the directory\ncd ethicrawl\n\n# Create and activate a virtual environment (optional but recommended)\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\nvenv\\Scripts\\activate     # Windows\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from ethicrawl import Ethicrawl\nfrom ethicrawl.error import RobotDisallowedError\n\n# Create and bind to a domain\nethicrawl = Ethicrawl()\nethicrawl.bind(\"https://example.com\")\n\n# Get a page - robots.txt rules automatically respected\ntry:\n    response = ethicrawl.get(\"https://example.com/page.html\")\nexcept RobotDisallowedError:\n    print(\"The site prohibits fetching the page\")\n\n# Release resources when done\nethicrawl.unbind()\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 License - See LICENSE file for details.</p>"},{"location":"standards/","title":"Coding Standards","text":""},{"location":"standards/#python-code-standards","title":"Python Code Standards","text":""},{"location":"standards/#general-requirements","title":"General Requirements","text":"<ul> <li>Python Version: We target Python 3.10 and above</li> <li>Code may work on earlier versions but is not tested or supported</li> <li>Line Endings: Use UNIX-style line endings (LF, <code>\\n</code>)</li> <li>Indentation: Use 4 spaces for indentation (no tabs)</li> <li>Maximum Line Length: 88 characters (Black default)</li> </ul>"},{"location":"standards/#code-formatting","title":"Code Formatting","text":"<p>We use Black as our code formatter with default settings:</p> <pre><code># Install Black\npip install black\n\n# Format a single file\nblack path/to/file.py\n\n# Format all Python files in a directory\nblack directory_name/\n</code></pre>"},{"location":"standards/#import-organization","title":"Import Organization","text":"<p>Imports should be grouped in the following order, with a blank line between each group:</p> <ul> <li>Standard library imports</li> <li>Third-party imports</li> <li>Local application imports</li> </ul> <pre><code># Standard library\nimport os\nfrom datetime import datetime\n\n# Third-party\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Local\nfrom ethicrawl.core import Resource\nfrom ethicrawl.config import Config\n</code></pre>"},{"location":"standards/#type-hints","title":"Type Hints","text":"<p>Use type hints for all public methods and functions:</p> <pre><code>def process_url(url: str, timeout: Optional[int] = None) -&gt; Dict[str, Any]:\n    \"\"\"Process the given URL and return results.\"\"\"\n</code></pre>"},{"location":"standards/#methodfunction-docstrings","title":"Method/Function Docstrings","text":"<pre><code>def can_fetch(self, url: str, user_agent: str = None) -&gt; bool:\n    \"\"\"Check if a URL can be fetched according to robots.txt rules.\n\n    Args:\n        url: The URL to check against robots.txt rules\n        user_agent: Optional user agent string to use for checking.\n            Defaults to the client's configured user agent.\n\n    Returns:\n        True if the URL is allowed, False otherwise.\n\n    Raises:\n        RobotDisallowedError: If access is denied and raise_on_disallow=True\n        ValueError: If the URL is malformed\n    \"\"\"\n</code></pre>"},{"location":"standards/#error-handling-style-guide-for-ethicrawl","title":"Error Handling Style Guide for Ethicrawl","text":"<p>A consistent approach to error handling improves code readability and helps developers understand and handle errors effectively. This document outlines our standards for raising exceptions in the Ethicrawl codebase.</p>"},{"location":"standards/#general-principles","title":"General Principles","text":"<ol> <li>Be specific - Use the most specific exception type appropriate for the error</li> <li>Be descriptive - Error messages should help users identify and fix problems</li> <li>Be consistent - Follow the same patterns throughout the codebase</li> </ol>"},{"location":"standards/#standard-exception-types","title":"Standard Exception Types","text":""},{"location":"standards/#typeerror","title":"TypeError","text":"<p>Use for incorrect argument types or invalid operations on types.</p>"},{"location":"standards/#format","title":"Format:","text":"<pre><code>raise TypeError(f\"Expected {expected_type}, got {type(actual).__name__}\")\n</code></pre>"},{"location":"standards/#examples","title":"Examples:","text":"<pre><code>raise TypeError(f\"Expected string, Url, or Resource, got {type(resource).__name__}\")\nraise TypeError(\"headers must be a Headers instance or dictionary\")\n</code></pre>"},{"location":"standards/#valueerror","title":"ValueError","text":"<p>Use for arguments that have the right type but an invalid value.</p>"},{"location":"standards/#format_1","title":"Format:","text":"<pre><code>raise ValueError(f\"{parameter_name} must be {constraint}\")\n</code></pre>"},{"location":"standards/#examples_1","title":"Examples:","text":"<pre><code>raise ValueError(\"jitter must be between 0.0 and 1.0\")\nraise ValueError(\"max_retries cannot be negative\")\n</code></pre>"},{"location":"standards/#domain-specific-exceptions","title":"Domain-Specific Exceptions","text":"<p>Create custom exceptions for domain-specific errors. All custom exceptions should:</p> <ol> <li>Inherit from appropriate base exceptions</li> <li>Include \"Error\" in the class name</li> <li>Be placed in a relevant _error.py module</li> </ol>"},{"location":"standards/#example","title":"Example:","text":"<pre><code>class RobotDisallowedError(ValueError):\n    \"\"\"Raised when access to a URL is disallowed by robots.txt rules.\"\"\"\n</code></pre>"},{"location":"standards/#error-message-guidelines","title":"Error Message Guidelines","text":"<ol> <li>Be specific about what went wrong</li> <li>Provide the invalid value when helpful</li> <li>Suggest a fix when possible</li> <li>Use consistent terminology across similar errors</li> </ol>"},{"location":"standards/#documenting-exceptions","title":"Documenting Exceptions","text":"<pre><code>def function_name():\n    \"\"\"Function description.\n\n    Args:\n        param_name: Parameter description.\n\n    Returns:\n        Return description.\n\n    Raises:\n        ExceptionType: Condition when raised.\n        AnotherException: Another condition.\n    \"\"\"\n</code></pre> <p>Always document exceptions in docstrings:</p>"},{"location":"standards/#error-assertions","title":"Error Assertions","text":"<p>For internal logic verification, use assertions with descriptive messages:</p> <pre><code>assert isinstance(item, Resource), f\"Expected Resource, got {type(item).__name__}\"\n</code></pre>"},{"location":"standards/#documentation","title":"Documentation","text":"<p>We follow Google-style docstrings for all code documentation.</p>"},{"location":"standards/#documentation-focus-areas","title":"Documentation Focus Areas","text":"<ul> <li>All public APIs (methods, classes, and modules) must have comprehensive docstrings</li> <li>Private methods with complex logic should have docstrings</li> <li>Simple private methods or properties may omit docstrings</li> <li>Focus on documentation that enhances IDE tooltips and developer experience</li> </ul>"},{"location":"standards/#docstring-format","title":"Docstring Format","text":""},{"location":"standards/#module-docstrings","title":"Module Docstrings","text":"<pre><code>\"\"\"Module for handling robots.txt parsing and permission checking.\n\nThis module provides functionality for fetching, parsing and checking\npermissions against robots.txt files according to the Robots Exclusion\nProtocol.\n\"\"\"\n</code></pre>"},{"location":"standards/#class-docstrings","title":"Class Docstrings","text":"<pre><code>class Robot:\n    \"\"\"Representation of a robots.txt file with permission checking.\n\n    This class handles fetching and parsing robots.txt files and provides\n    methods to check if URLs can be accessed according to the rules.\n\n    Attributes:\n        url: The URL of the robots.txt file\n        sitemaps: List of sitemap URLs found in robots.txt\n    \"\"\"\n</code></pre>"},{"location":"standards/#methodfunction-docstrings_1","title":"Method/Function Docstrings","text":"<pre><code>def can_fetch(self, url: str, user_agent: str = None) -&gt; bool:\n    \"\"\"Check if a URL can be fetched according to robots.txt rules.\n\n    Args:\n        url: The URL to check against robots.txt rules\n        user_agent: Optional user agent string to use for checking.\n            Defaults to the client's configured user agent.\n\n    Returns:\n        True if the URL is allowed, False otherwise.\n\n    Raises:\n        RobotDisallowedError: If access is denied and raise_on_disallow=True\n        ValueError: If the URL is malformed\n\n    Example:\n        &gt;&gt;&gt; robot = Robot(\"https://example.com/robots.txt\")\n        &gt;&gt;&gt; robot.can_fetch(\"https://example.com/allowed\")\n        True\n        &gt;&gt;&gt; robot.can_fetch(\"https://example.com/disallowed\")\n        False\n</code></pre>"},{"location":"standards/#property-docstrings","title":"Property Docstrings","text":"<pre><code>@property\ndef sitemaps(self) -&gt; List[str]:\n    \"\"\"List of sitemap URLs found in robots.txt.\n\n    Returns:\n        List of sitemap URLs as strings.\n    \"\"\"\n</code></pre>"},{"location":"standards/#constructor-docstrings","title":"Constructor Docstrings","text":"<pre><code>def __init__(self, url: str, context: Context = None):\n    \"\"\"Initialize a Robot instance.\n\n    Args:\n        url: URL to the robots.txt file\n        context: Optional context for the request.\n            If not provided, a default context will be created.\n    \"\"\"\n</code></pre>"},{"location":"standards/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>All public methods, classes, and modules must have docstrings (100% coverage)</li> <li>Private methods with complex logic should have docstrings</li> <li>Simple private methods or properties may omit docstrings</li> <li>The overall project requires a minimum of 95% docstring coverage as measured by interrogate</li> </ul>"},{"location":"standards/#special-cases","title":"Special Cases","text":""},{"location":"standards/#private-methods","title":"Private Methods","text":"<p>Private methods (starting with underscore) should have docstrings if they: - Contain complex logic - Are called from multiple places - Would benefit from documentation for maintainers</p>"},{"location":"standards/#one-line-docstrings","title":"One-line Docstrings","text":"<p>For simple methods with obvious behavior, a one-line docstring is acceptable:</p> <pre><code>def is_allowed(self, url: str) -&gt; bool:\n    \"\"\"Return True if the URL is allowed by robots.txt.\"\"\"\n</code></pre>"},{"location":"standards/#verification-approach","title":"Verification Approach","text":"<p>Documentation quality is verified through code review rather than automated metrics. Reviewers should confirm that:</p> <ul> <li>Public APIs have clear, complete docstrings</li> <li>Examples are provided for non-obvious usage</li> <li>Type information is present in docstrings</li> <li>Error cases and exceptions are documented</li> </ul>"},{"location":"standards/#documentation-language-guidelines","title":"Documentation Language Guidelines","text":"<p>When writing documentation, follow these language principles:</p> <ol> <li>Be objective - Avoid subjective descriptors like \"elegant\", \"beautiful\", or \"clever\"</li> <li>Be precise - Focus on what code does, not subjective quality judgments</li> <li>Be technical - Use concrete technical terms rather than metaphorical language</li> <li>Be consistent - Maintain a neutral, professional tone throughout documentation</li> </ol>"},{"location":"standards/#examples_2","title":"Examples:","text":"<p>Avoid: <pre><code>\"\"\"This elegant pattern enables seamless chaining of operations.\"\"\"\n</code></pre></p> <p>Better: <pre><code>\"\"\"This design allows operation outputs to serve as inputs to other operations.\"\"\"\n</code></pre></p> <p>Avoid <pre><code>\"\"\"Beautiful integration between components creates a powerful system.\"\"\"\n</code></pre></p> <p>Better <code>Components communicate through well-defined interfaces that enable extensibility.</code></p>"},{"location":"standards/#logging-standards-for-ethicrawl","title":"Logging Standards for Ethicrawl","text":"<p>Good logging practices are essential for troubleshooting, monitoring, and understanding application behavior.</p>"},{"location":"standards/#log-levels-and-their-uses","title":"Log Levels and Their Uses","text":""},{"location":"standards/#critical-loggingcritical-50","title":"CRITICAL (logging.CRITICAL, 50)","text":"<p>Use for severe errors that prevent core functionality from working.</p>"},{"location":"standards/#when-to-use","title":"When to use:","text":"<ul> <li>Application cannot continue functioning</li> <li>Data corruption or loss has occurred</li> <li>Security breaches or compromises</li> <li>Resource exhaustion that threatens system stability</li> </ul>"},{"location":"standards/#examples_3","title":"Examples:","text":"<pre><code>logger.critical(f\"Failed to initialize client: {error}\")\nlogger.critical(f\"Persistent storage corruption detected in {file_path}\")\n</code></pre>"},{"location":"standards/#error-loggingerror-40","title":"ERROR (logging.ERROR, 40)","text":"<p>Use for runtime errors that prevent specific operations from completing but don't crash the application.</p>"},{"location":"standards/#when-to-use_1","title":"When to use:","text":"<ul> <li>Failed HTTP requests</li> <li>Failed data processing operations</li> <li>Configuration errors</li> <li>External service unavailability</li> <li>Unexpected exceptions in non-critical paths</li> </ul>"},{"location":"standards/#examples_4","title":"Examples:","text":"<pre><code>logger.error(f\"HTTP request failed: {status_code} {reason}\")\nlogger.error(f\"Failed to parse sitemap at {url}: {error_message}\")\n</code></pre>"},{"location":"standards/#warning-loggingwarning-30","title":"WARNING (logging.WARNING, 30)","text":"<p>Use for conditions that might cause problems but allow operations to continue.</p>"},{"location":"standards/#when-to-use_2","title":"When to use:","text":"<ul> <li>Deprecated feature usage</li> <li>Slow response times</li> <li>Retrying operations after recoverable failures</li> <li>Access denied for certain operations</li> <li>Unexpected data formats that can be handled</li> <li>Rate limiting being applied</li> </ul>"},{"location":"standards/#examples_5","title":"Examples:","text":"<pre><code>logger.warning(f\"URL disallowed by robots.txt: {url}\")\nlogger.warning(f\"Slow response from {domain}: {response_time}s\")\nlogger.warning(f\"Retrying request ({retry_count}/{max_retries})\")\n</code></pre>"},{"location":"standards/#info-logginginfo-20","title":"INFO (logging.INFO, 20)","text":"<p>Use for normal operational events and milestones.</p>"},{"location":"standards/#when-to-use_3","title":"When to use:","text":"<ul> <li>Application startup and shutdown</li> <li>Configuration settings</li> <li>Successful site binding and crawling</li> <li>Processing milestones</li> <li>Summary information about operations</li> <li>Changes to application state</li> </ul>"},{"location":"standards/#examples_6","title":"Examples:","text":"<pre><code>logger.info(f\"Bound to site: {url}\")\nlogger.info(f\"Robots.txt processed: {allowed_count} allowed paths, {disallowed_count} disallowed\")\nlogger.info(f\"Processed {page_count} pages in {duration}s\")\n</code></pre>"},{"location":"standards/#debug-loggingdebug-10","title":"DEBUG (logging.DEBUG, 10)","text":"<p>Use for detailed information useful during development and debugging.</p>"},{"location":"standards/#when-to-use_4","title":"When to use:","text":"<ul> <li>Function entry/exit points</li> <li>Variable values and state changes</li> <li>Decision logic paths</li> <li>Low-level HTTP details</li> <li>Parsing steps</li> <li>Rate limiting details</li> </ul>"},{"location":"standards/#examples_7","title":"Examples:","text":"<pre><code>logger.debug(f\"Processing URL: {url}\")\nlogger.debug(f\"Page found in cache, age: {cache_age}s\")\nlogger.debug(f\"Parser state: {current_state}\")\n</code></pre>"},{"location":"standards/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li>Be Concise and Specific</li> <li>Include exactly what happened and where</li> <li> <p>Use active voice (e.g., \"Failed to connect\" instead of \"Connection failure occurred\")</p> </li> <li> <p>Include Context</p> </li> <li>Always include relevant identifiers (URLs, IDs, component names)</li> <li>Include relevant variable values</li> <li> <p>For errors, include exception messages and/or stack traces</p> </li> <li> <p>Be Consistent</p> </li> <li>Use consistent terminology across similar log messages</li> <li>Use consistent formatting for similar events</li> <li> <p>Use sentence case for log messages (capitalize first word)</p> </li> <li> <p>Avoid Sensitive Information</p> </li> <li>No authentication credentials</li> <li>No personal data</li> <li> <p>No sensitive headers or tokens</p> </li> <li> <p>Use Structured Fields for Machine Parsing</p> </li> <li>Place structured data at the end of the message</li> <li>Use consistent key-value format: <code>key=value</code></li> </ol>"},{"location":"standards/#component-specific-guidelines","title":"Component-Specific Guidelines","text":"<p>Each component should have a consistent logging identity:</p> <ol> <li>Robot/Robots.txt</li> <li>INFO: Robots.txt fetching and parsing results</li> <li>WARNING: Disallowed access attempts</li> <li> <p>ERROR: Failed to fetch/parse robots.txt</p> </li> <li> <p>HTTP Client</p> </li> <li>DEBUG: Request details</li> <li>INFO: Rate limiting information</li> <li>WARNING: Retries and slow responses</li> <li> <p>ERROR: Failed requests</p> </li> <li> <p>Sitemap</p> </li> <li>INFO: Sitemap discovery and parsing</li> <li>WARNING: Malformed but recoverable sitemap content</li> <li>ERROR: Failed sitemap fetching/parsing</li> </ol>"},{"location":"api/client.http/","title":"HttpClient Module","text":"<p>Client interfaces for making HTTP requests to resources.</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpClient","title":"<code>HttpClient</code>","text":"<p>               Bases: <code>Client</code></p> <p>HTTP client implementation with configurable transports and rate limiting.</p> <p>This client provides a flexible HTTP interface with the following features: - Configurable backend transport (Requests or Selenium Chrome) - Built-in rate limiting with jitter to avoid detection - Header management with User-Agent control - Automatic retry with exponential backoff - Detailed logging of request/response cycles</p> <p>The client can use either a simple RequestsTransport for basic HTTP operations or a ChromeTransport for JavaScript-rendered content.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>min_interval</code> <code>float</code> <p>Minimum time between requests in seconds</p> <code>jitter</code> <code>float</code> <p>Random time variation added to rate limiting</p> <code>headers</code> <code>Headers</code> <p>Default headers to send with each request</p> <code>last_request_time</code> <code>float</code> <p>Timestamp of the last request</p> <code>user_agent</code> <code>str</code> <p>User agent string used for requests</p> Example <p>from ethicrawl.client.http import HttpClient from ethicrawl.core import Resource client = HttpClient(rate_limit=1.0)  # 1 request per second response = client.get(Resource(\"https://example.com\")) print(response.status_code) 200</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpClient--switch-to-chrome-for-javascript-heavy-sites","title":"Switch to Chrome for JavaScript-heavy sites","text":"<p>chrome_client = client.with_chrome(headless=True) js_response = chrome_client.get(Resource(\"https://spa-example.com\"))</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpClient.__init__","title":"<code>__init__(context=None, transport=None, timeout=10, rate_limit=1.0, jitter=0.5, headers=None, chrome_params=None)</code>","text":"<p>Initialize an HTTP client with configurable transport and rate limiting.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Context for the client. If None, a default context with a dummy URL will be created.</p> <code>None</code> <code>transport</code> <code>Transport</code> <p>Custom transport implementation. If None, either ChromeTransport or RequestsTransport will be used.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>10</code> <code>rate_limit</code> <code>float</code> <p>Maximum requests per second. Set to 0 for no limit.</p> <code>1.0</code> <code>jitter</code> <code>float</code> <p>Random variation (0-1) to add to rate limiting</p> <code>0.5</code> <code>headers</code> <code>dict</code> <p>Default headers to send with each request</p> <code>None</code> <code>chrome_params</code> <code>dict</code> <p>Parameters for ChromeTransport if used</p> <code>None</code>"},{"location":"api/client.http/#ethicrawl.client.http.HttpClient.get","title":"<code>get(resource, timeout=None, headers=None)</code>","text":"<p>Make a GET request to the specified resource.</p> <p>This method applies rate limiting, handles headers, and logs the result. For JavaScript-heavy sites, use with_chrome() first to switch to a Chrome-based transport.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to request</p> required <code>timeout</code> <code>int</code> <p>Request-specific timeout that overrides the client's default timeout</p> <code>None</code> <code>headers</code> <code>dict</code> <p>Additional headers for this request</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HttpResponse</code> <code>HttpResponse</code> <p>Response object with status, headers and content</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If resource is not a Resource instance</p> <code>IOError</code> <p>If the HTTP request fails for any reason</p> Example <p>client = HttpClient() response = client.get(Resource(\"https://example.com\")) if response.status_code == 200: ...     print(f\"Got {len(response.content)} bytes\")</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpClient.with_chrome","title":"<code>with_chrome(headless=True, wait_time=3, timeout=30, rate_limit=0.5, jitter=0.3)</code>","text":"<p>Create a new HttpClient instance using Chrome/Selenium transport.</p> <p>This creates a new client that can render JavaScript and interact with dynamic web applications.</p> <p>Parameters:</p> Name Type Description Default <code>headless</code> <code>bool</code> <p>Whether to run Chrome in headless mode</p> <code>True</code> <code>wait_time</code> <code>int</code> <p>Default time to wait for page elements in seconds</p> <code>3</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <code>rate_limit</code> <code>float</code> <p>Maximum requests per second</p> <code>0.5</code> <code>jitter</code> <code>float</code> <p>Random variation factor for rate limiting</p> <code>0.3</code> <p>Returns:</p> Name Type Description <code>HttpClient</code> <code>HttpClient</code> <p>A new client instance configured to use Chrome</p> Example <p>client = HttpClient() chrome = client.with_chrome(headless=True) response = chrome.get(Resource(\"https://single-page-app.com\"))</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpRequest","title":"<code>HttpRequest</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Request</code></p> <p>HTTP-specific request implementation with timeout and header management.</p> <p>This class extends the base Request with HTTP-specific functionality, including configurable timeout and header handling. It automatically applies default headers from the global configuration while allowing custom headers to take precedence.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>The target URL (inherited from Request)</p> <code>headers</code> <code>Headers</code> <p>HTTP headers to send with the request</p> <code>_timeout</code> <code>float</code> <p>Request timeout in seconds</p> Example <p>from ethicrawl.client.http import HttpRequest from ethicrawl.core import Url req = HttpRequest(Url(\"https://example.com\")) req.headers[\"User-Agent\"] = \"EthiCrawl/1.0\" req.timeout = 15.0</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpRequest.timeout","title":"<code>timeout</code>  <code>property</code> <code>writable</code>","text":"<p>Get the request timeout in seconds.</p> <p>Returns:</p> Type Description <code>float</code> <p>The timeout value in seconds</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpRequest.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize and validate the request after creation.</p> <p>Ensures headers are a proper Headers instance and applies default headers from configuration if not already present.</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpResponse","title":"<code>HttpResponse</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Response</code></p> <p>HTTP-specific response implementation with status codes and text content.</p> <p>This class extends the base Response with HTTP-specific attributes and behaviors, including status code, headers, and separate text content representation. It provides robust validation and a comprehensive string representation for debugging and logging.</p> <p>The HttpResponse maintains the connection between the original request and the response while enforcing type safety and data consistency.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>HttpRequest</code> <p>The request that generated this response</p> <code>status_code</code> <code>int</code> <p>HTTP status code (200, 404, etc.)</p> <code>headers</code> <code>Headers</code> <p>HTTP response headers</p> <code>content</code> <code>bytes</code> <p>Binary content of the response (inherited from Response)</p> <code>text</code> <code>str</code> <p>Text content decoded from binary content (for text responses)</p> <code>url</code> <code>Url</code> <p>The response URL, which may differ from request URL after redirects</p> Example <p>from ethicrawl.client.http import HttpRequest, HttpResponse from ethicrawl.core import Resource, Headers req = HttpRequest(Resource(\"https://example.com\")) resp = HttpResponse( ...     request=req, ...     status_code=200, ...     content=b\"Example\", ...     text=\"Example\", ...     headers=Headers({\"Content-Type\": \"text/html\"}) ... ) resp.status_code 200 \"html\" in resp.text True</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpResponse.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the response attributes after initialization.</p> <p>Performs type checking and value validation for: - Status code (must be int between 100-599) - Request (must be HttpRequest instance) - Content (must be bytes or None) - Text (must be str or None)</p> <p>Also calls the parent class post_init for further validation.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any attribute has an invalid type</p> <code>ValueError</code> <p>If status_code is outside valid HTTP range (100-599)</p>"},{"location":"api/client.http/#ethicrawl.client.http.HttpResponse.__str__","title":"<code>__str__()</code>","text":"<p>Format a human-readable representation of the response.</p> <p>Creates a formatted multi-line string containing: - Status code - URL (showing both response URL and request URL if they differ) - Headers - Content summary (preview for text, byte count for binary) - Text preview for text content types (up to 300 chars)</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the response with formatted content preview</p>"},{"location":"api/client/","title":"Client Module","text":"<p>Client interfaces for making requests to resources.</p>"},{"location":"api/client/#ethicrawl.client.Client","title":"<code>Client</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for all clients.</p> <p>This defines the contract that any client implementation must follow, whether it's an HTTP client, file client, or other protocol. The Client abstraction enables dependency injection throughout the system and allows for easy swapping of implementations for different protocols or testing.</p> <p>All client implementations must provide at least the get() method to fetch resources. Specific implementations may add additional methods or parameters as needed.</p>"},{"location":"api/client/#ethicrawl.client.Client.get","title":"<code>get(resource)</code>  <code>abstractmethod</code>","text":"<p>Fetch a resource.</p> <p>Retrieves the content associated with the provided resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The Resource to fetch</p> required <p>Returns:</p> Type Description <code>Response</code> <p>A Response object containing the result</p>"},{"location":"api/client/#ethicrawl.client.NoneClient","title":"<code>NoneClient</code>","text":"<p>               Bases: <code>Client</code></p> <p>Null object implementation of Client that returns empty responses.</p> <p>This implementation serves as a placeholder when no client is needed or available. It follows the Null Object pattern to avoid null checks throughout the codebase. The NoneClient always returns empty responses without actually making any network requests.</p> <p>This class is useful for: - Providing a default when no client is specified - Testing components that need a Client but shouldn't make real requests - Stubbing functionality during development</p>"},{"location":"api/client/#ethicrawl.client.NoneClient.get","title":"<code>get(resource)</code>","text":"<p>Return an empty response without doing any work.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource that would be requested (unused)</p> required <p>Returns:</p> Type Description <code>Response</code> <p>An empty Response object with the same URL</p>"},{"location":"api/client/#ethicrawl.client.Request","title":"<code>Request</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Request representation of an operation to be performed on a resource.</p> <p>Request extends Resource to represent an operation that can be performed. This maintains the URL identity of the resource while providing a foundation for additional request-specific properties like headers, parameters, or operation types.</p> <p>This class serves as a base class for protocol-specific request implementations, such as HttpRequest.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the resource (inherited from Resource)</p>"},{"location":"api/client/#ethicrawl.client.Response","title":"<code>Response</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Response representation of a resource request operation.</p> <p>Response extends Resource to represent the result of a client request operation. This ensures that responses maintain the URL identity of their source resource while adding request tracking and content storage capabilities.</p> <p>By inheriting from Resource, Response objects can be used anywhere a Resource is expected, which enables chaining of operations. This design provides a consistent pattern where the output of one operation can serve as the input to another.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the response (inherited from Resource)</p> <code>request</code> <code>Request</code> <p>The Request object that generated this response</p> <code>content</code> <code>bytes</code> <p>Binary content returned by the operation (empty bytes by default)</p> Example <p>from ethicrawl.client import Response, Request from ethicrawl.core import Resource, Url req = Request(Url(\"https://example.com\")) resp = Response(req.url, req, b\"Example\") resp.url == req.url  # Maintains resource identity True len(resp.content) 22</p>"},{"location":"api/client/#ethicrawl.client.Response.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate response attributes.</p> <p>Performs type checking and value validation: - Ensures content is bytes or None - Ensures request is a Request instance</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any attribute has an invalid type</p>"},{"location":"api/client/#ethicrawl.client.Transport","title":"<code>Transport</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for HTTP transport implementations.</p> <p>Transport defines the interface for making HTTP requests through various backends. Concrete implementations handle the actual request logic using different libraries or mechanisms (e.g., requests, selenium, etc.).</p> <p>This abstraction allows swapping transport mechanisms without changing the client interface, supporting different use cases such as basic HTTP requests or full browser automation.</p>"},{"location":"api/client/#ethicrawl.client.Transport.user_agent","title":"<code>user_agent</code>  <code>property</code> <code>writable</code>","text":"<p>Get the User-Agent string used by this transport.</p> <p>Returns:</p> Type Description <code>str</code> <p>The current User-Agent string</p>"},{"location":"api/client/#ethicrawl.client.Transport.get","title":"<code>get(request)</code>  <code>abstractmethod</code>","text":"<p>Make a GET request using the provided request object.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The request to perform</p> required <p>Returns:</p> Type Description <code>Response</code> <p>Response object containing the result</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass</p>"},{"location":"api/client/#ethicrawl.client.Transport.head","title":"<code>head(request)</code>","text":"<p>Make a HEAD request.</p> <p>Default implementation raises NotImplementedError. See GitHub issue #18 for planned implementation of HEAD and other HTTP verbs.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The request to perform</p> required <p>Returns:</p> Type Description <code>Response</code> <p>Response object containing headers and status</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Default implementation raises this exception</p>"},{"location":"api/config/","title":"Config Module","text":"<p>Configuration system for Ethicrawl.</p>"},{"location":"api/config/#ethicrawl.config.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for configuration components.</p> <p>All configuration classes inherit from this class to ensure a consistent interface and behavior across the configuration system. Configuration objects can be converted to dictionaries, serialized, and represented as strings with consistent formatting.</p> Example <p>from abc import ABC from ethicrawl.config import BaseConfig</p> <p>class MyConfig(BaseConfig): ...     def init(self, name=\"default\", value=42): ...         self.name = name ...         self.value = value ... ...     def to_dict(self) -&gt; dict: ...         return {\"name\": self.name, \"value\": self.value}</p> <p>config = MyConfig(\"test\", 100) config.to_dict() {'name': 'test', 'value': 100} print(config) {   \"name\": \"test\",   \"value\": 100 }</p>"},{"location":"api/config/#ethicrawl.config.BaseConfig.__repr__","title":"<code>__repr__()</code>","text":"<p>Default string representation showing config values.</p> <p>Returns:</p> Type Description <code>str</code> <p>String in format ClassName({config values})</p>"},{"location":"api/config/#ethicrawl.config.BaseConfig.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty-printed JSON representation of the configuration</p>"},{"location":"api/config/#ethicrawl.config.BaseConfig.to_dict","title":"<code>to_dict()</code>  <code>abstractmethod</code>","text":"<p>Convert configuration to a dictionary representation.</p> <p>Implementations must produce a JSON-serializable dictionary that fully represents the configuration state.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation of the configuration</p>"},{"location":"api/config/#ethicrawl.config.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>Global configuration singleton for Ethicrawl.</p> <p>This class provides a centralized, thread-safe configuration system for all components of Ethicrawl. It implements the Singleton pattern to ensure consistent settings throughout the application.</p> <p>The configuration is organized into sections (http, logger, sitemap) with each section containing component-specific settings.</p> Thread Safety <p>All configuration updates are protected by a reentrant lock, ensuring thread-safe operation in multi-threaded crawling scenarios.</p> Integration Features <ul> <li>Convert to/from dictionaries for integration with external config systems</li> <li>JSON serialization for storage or transmission</li> <li>Hierarchical structure matches common config formats</li> </ul> <p>Attributes:</p> Name Type Description <code>http</code> <code>HttpConfig</code> <p>HTTP-specific configuration (user agent, headers, timeout)</p> <code>logger</code> <code>LoggerConfig</code> <p>Logging configuration (levels, format, output)</p> <code>sitemap</code> <code>SitemapConfig</code> <p>Sitemap parsing configuration (limits, defaults)</p> Example <p>from ethicrawl.config import Config config = Config()  # Get the global instance config.http.user_agent = \"MyCustomBot/1.0\" config.logger.level = \"DEBUG\"</p>"},{"location":"api/config/#ethicrawl.config.Config--thread-safe-update-of-multiple-settings-at-once","title":"Thread-safe update of multiple settings at once","text":"<p>config.update({ ...     \"http\": {\"timeout\": 30}, ...     \"logger\": {\"component_levels\": {\"robots\": \"DEBUG\"}} ... })</p>"},{"location":"api/config/#ethicrawl.config.Config--get-a-snapshot-for-thread-safe-reading","title":"Get a snapshot for thread-safe reading","text":"<p>snapshot = config.get_snapshot() print(snapshot.http.timeout) 30</p>"},{"location":"api/config/#ethicrawl.config.Config--export-config-for-integration-with-external-systems","title":"Export config for integration with external systems","text":"<p>config_dict = config.to_dict() config_json = str(config)</p>"},{"location":"api/config/#ethicrawl.config.Config.__str__","title":"<code>__str__()</code>","text":"<p>Format the configuration as a JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted JSON representation of the configuration</p>"},{"location":"api/config/#ethicrawl.config.Config.get_snapshot","title":"<code>get_snapshot()</code>","text":"<p>Create a thread-safe deep copy of the current configuration.</p> <p>Returns:</p> Type Description <code>Config</code> <p>A deep copy of the current Config object</p>"},{"location":"api/config/#ethicrawl.config.Config.reset","title":"<code>reset()</code>  <code>classmethod</code>","text":"<p>Reset the singleton instance to default values.</p> <p>Removes the existing instance from the singleton registry, causing a new instance to be created on next access.</p> Example <p>Config.reset()  # Reset to defaults config = Config()  # Get fresh instance</p>"},{"location":"api/config/#ethicrawl.config.Config.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the configuration to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing all configuration sections</p>"},{"location":"api/config/#ethicrawl.config.Config.update","title":"<code>update(config_dict)</code>","text":"<p>Update configuration from a dictionary.</p> <p>Updates configuration sections based on a nested dictionary structure. The dictionary should have section names as top-level keys and property-value pairs as nested dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict[str, Any]</code> <p>Dictionary with configuration settings</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If trying to set a property that doesn't exist</p> Example <p>config.update({ ...     \"http\": { ...         \"user_agent\": \"CustomBot/1.0\", ...         \"timeout\": 30 ...     }, ...     \"logger\": { ...         \"level\": \"DEBUG\" ...     } ... })</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig","title":"<code>HttpConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>HTTP client configuration settings for Ethicrawl.</p> <p>This class manages all HTTP-specific configuration options including timeouts, rate limiting, retries, user agent settings, headers, and proxy configuration. It provides validation for all values to ensure they're within safe and reasonable ranges.</p> <p>All setters perform type checking and value validation to prevent invalid configurations. The class integrates with the global Config singleton for system-wide settings.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 30.0)</p> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts for failed requests (default: 3)</p> <code>retry_delay</code> <code>float</code> <p>Base delay between retries in seconds (default: 1.0)</p> <code>rate_limit</code> <code>float | None</code> <p>Maximum requests per second (default: 0.5)</p> <code>jitter</code> <code>float</code> <p>Random variation factor for rate limiting (default: 0.2)</p> <code>user_agent</code> <code>str</code> <p>User agent string for requests (default: \"Ethicrawl/1.0\")</p> <code>headers</code> <code>Headers</code> <p>Default headers to include with requests</p> <code>proxies</code> <code>HttpProxyConfig</code> <p>Proxy server configuration</p> Example <p>from ethicrawl.config import Config</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig--get-the-global-configuration","title":"Get the global configuration","text":"<p>config = Config()</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig--update-http-settings","title":"Update HTTP settings","text":"<p>config.http.timeout = 60.0 config.http.user_agent = \"MyCustomCrawler/2.0\" config.http.rate_limit = 1.0  # 1 request per second</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig--configure-proxy","title":"Configure proxy","text":"<p>config.http.proxies = {\"http\": \"http://proxy:8080\", \"https\": \"https://proxy:8443\"}</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.headers","title":"<code>headers</code>  <code>property</code> <code>writable</code>","text":"<p>Get request headers.</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.jitter","title":"<code>jitter</code>  <code>property</code> <code>writable</code>","text":"<p>Random variation factor for rate limiting.</p> <p>Adds randomness to the timing between requests to make crawling patterns less predictable. The random factor is calculated as: delay * (1 + random() * jitter)</p> <p>Valid range: 0.0-1.0 Default: 0.2</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a number</p> <code>ValueError</code> <p>If value outside allowed range</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.max_retries","title":"<code>max_retries</code>  <code>property</code> <code>writable</code>","text":"<p>Maximum number of retry attempts for failed requests.</p> <p>Controls how many times a failed request should be retried before giving up. Uses exponential backoff between attempts.</p> <p>Valid range: 0-10 (0 means no retries) Default: 3</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not an integer</p> <code>ValueError</code> <p>If value is negative or &gt; 10</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.proxies","title":"<code>proxies</code>  <code>property</code> <code>writable</code>","text":"<p>Proxy server configuration for HTTP requests.</p> <p>Configures HTTP and HTTPS proxy servers for requests.</p> Example <p>config.http.proxies = { ...    \"http\": \"http://proxy:8080\", ...    \"https\": \"https://proxy:8443\" ... }</p> <p>Returns:</p> Type Description <code>HttpProxyConfig</code> <p>HttpProxyConfig object with http and https properties</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not HttpProxyConfig or dict</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.rate_limit","title":"<code>rate_limit</code>  <code>property</code> <code>writable</code>","text":"<p>Maximum requests per second allowed.</p> <p>Controls request frequency to avoid overwhelming servers. Set to None to disable rate limiting (not recommended).</p> <p>Example: 0.5 means maximum of one request every 2 seconds</p> <p>Valid range: &gt; 0 Default: 0.5</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a number</p> <code>ValueError</code> <p>If value is &lt;= 0</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.retry_delay","title":"<code>retry_delay</code>  <code>property</code> <code>writable</code>","text":"<p>Base delay between retries in seconds</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.timeout","title":"<code>timeout</code>  <code>property</code> <code>writable</code>","text":"<p>Request timeout in seconds.</p> <p>Controls how long to wait for a response before abandoning the request.</p> <p>Valid range: 0 &lt; timeout &lt;= 300 Default: 30.0</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a number</p> <code>ValueError</code> <p>If value is &lt;= 0 or &gt; 300</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.user_agent","title":"<code>user_agent</code>  <code>property</code> <code>writable</code>","text":"<p>User agent string</p>"},{"location":"api/config/#ethicrawl.config.HttpConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary.</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig","title":"<code>HttpProxyConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>HTTP proxy configuration settings.</p> <p>Manages proxy server URLs for HTTP and HTTPS connections. Both proxy types can be configured independently and are validated to ensure they contain valid URLs.</p> <p>Attributes:</p> Name Type Description <code>http</code> <code>Url | None</code> <p>HTTP proxy server URL</p> <code>https</code> <code>Url | None</code> <p>HTTPS proxy server URL</p> Example <p>from ethicrawl.config import Config config = Config()</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig--configure-http-proxy","title":"Configure HTTP proxy","text":"<p>config.http.proxies.http = \"http://proxy.example.com:8080\"</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig--configure-https-proxy","title":"Configure HTTPS proxy","text":"<p>config.http.proxies.https = \"http://secure-proxy.example.com:8443\"</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig--clear-http-proxy","title":"Clear HTTP proxy","text":"<p>config.http.proxies.http = None</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig.http","title":"<code>http</code>  <code>property</code> <code>writable</code>","text":"<p>HTTP proxy server URL.</p> <p>Returns:</p> Type Description <code>Url | None</code> <p>Url object or None if not configured</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig.https","title":"<code>https</code>  <code>property</code> <code>writable</code>","text":"<p>HTTPS proxy server URL.</p> <p>Returns:</p> Type Description <code>Url | None</code> <p>Url object or None if not configured</p>"},{"location":"api/config/#ethicrawl.config.HttpProxyConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert proxy configuration to dictionary format.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with 'http' and 'https' keys mapping to URL strings or None</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig","title":"<code>LoggerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Logging configuration for Ethicrawl.</p> <p>Controls all aspects of Ethicrawl's logging system including output destinations, format, levels, and component-specific settings.</p> <p>The class provides validation for all settings and supports both numeric and string-based log levels (e.g., \"DEBUG\" or logging.DEBUG).</p> <p>Attributes:</p> Name Type Description <code>level</code> <code>int</code> <p>Default log level for all components (default: INFO)</p> <code>console_enabled</code> <code>bool</code> <p>Whether to log to console (default: True)</p> <code>file_enabled</code> <code>bool</code> <p>Whether to write logs to a file (default: False)</p> <code>file_path</code> <code>str | None</code> <p>Path where log file should be written (default: None)</p> <code>use_colors</code> <code>bool</code> <p>Whether to use colored console output (default: True)</p> <code>format</code> <code>str</code> <p>Log message format string</p> <code>component_levels</code> <code>dict[str, int]</code> <p>Dictionary of component-specific log levels</p> Example <p>from ethicrawl.config import Config config = Config()</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig--set-global-log-level","title":"Set global log level","text":"<p>config.logger.level = \"DEBUG\"</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig--enable-file-logging","title":"Enable file logging","text":"<p>config.logger.file_enabled = True config.logger.file_path = \"ethicrawl.log\"</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig--set-component-specific-level","title":"Set component-specific level","text":"<p>config.logger.set_component_level(\"robots\", \"DEBUG\") config.logger.set_component_level(\"http\", \"WARNING\")</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.component_levels","title":"<code>component_levels</code>  <code>property</code>","text":"<p>Special log levels for specific components.</p> <p>Returns a dictionary mapping component names to log levels. Note: Returns a copy to prevent direct mutation.</p> Example <p>config.logger.component_levels {'robots': 10, 'http': 30}</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.console_enabled","title":"<code>console_enabled</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to log to console/stdout.</p> <p>When True, log messages will be printed to the console. Default: True</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a boolean</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.file_enabled","title":"<code>file_enabled</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to log to a file.</p> <p>When True, log messages will be written to the file specified by file_path (which must also be set). Default: False</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a boolean</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.file_path","title":"<code>file_path</code>  <code>property</code> <code>writable</code>","text":"<p>Path to log file.</p> <p>Only used when file_enabled is True. Default: None</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a string or None</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.format","title":"<code>format</code>  <code>property</code> <code>writable</code>","text":"<p>Log message format string.</p> <p>Uses Python's logging format string syntax. Default: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a string</p> <code>ValueError</code> <p>If value is empty</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.level","title":"<code>level</code>  <code>property</code> <code>writable</code>","text":"<p>Default log level for all loggers.</p> <p>Can be set using either integer constants from the logging module or string names like \"DEBUG\", \"INFO\", etc.</p> <p>Default: logging.INFO (20)</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not an integer or string</p> <code>ValueError</code> <p>If value is an invalid level</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.use_colors","title":"<code>use_colors</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to use colorized output for console logging.</p> <p>When True, different log levels will be displayed in different colors in terminal output for better readability. Default: True</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a boolean</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.set_component_level","title":"<code>set_component_level(component_name, level)</code>","text":"<p>Set a specific log level for a component.</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <code>str</code> <p>The component name (e.g., \"robots\", \"sitemaps\")</p> required <code>level</code> <code>int | str</code> <p>The log level (can be int or level name string)</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If level is not an integer or string</p> <code>ValueError</code> <p>If string level name is not valid</p> Example <p>config.logger.set_component_level(\"robots\", \"DEBUG\") config.logger.set_component_level(\"http\", logging.WARNING)</p>"},{"location":"api/config/#ethicrawl.config.LoggerConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert logger configuration to a dictionary.</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig","title":"<code>SitemapConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for sitemap parsing and traversal.</p> <p>Controls behavior of sitemap parsing including recursion limits, error handling, and filtering options. This configuration affects how sitemaps are discovered, parsed, and which URLs are included in the final results.</p> <p>Attributes:</p> Name Type Description <code>max_depth</code> <code>int</code> <p>Maximum recursion depth for nested sitemaps (default: 5)</p> <code>follow_external</code> <code>bool</code> <p>Whether to follow sitemap links to external domains (default: False)</p> <code>validate_urls</code> <code>bool</code> <p>Whether to validate URLs before adding them to results (default: True)</p> Example <p>from ethicrawl.config import Config config = Config()</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig--increase-recursion-depth-for-complex-sites","title":"Increase recursion depth for complex sites","text":"<p>config.sitemap.max_depth = 10</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig--allow-following-external-domains","title":"Allow following external domains","text":"<p>config.sitemap.follow_external = True</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig.follow_external","title":"<code>follow_external</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to follow sitemap links to external domains.</p> <p>When True, sitemap references to other domains will be followed. When False, only sitemaps on the same domain will be processed.</p> <p>Default: False</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a boolean</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig.max_depth","title":"<code>max_depth</code>  <code>property</code> <code>writable</code>","text":"<p>Maximum recursion depth for nested sitemaps.</p> <p>Controls how many levels of sitemap references will be followed. Many sites use sitemap indexes that point to other sitemaps, and this setting limits how deep that recursion can go.</p> <p>Valid range: &gt;= 1 Default: 5</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not an integer</p> <code>ValueError</code> <p>If value is less than 1</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig.validate_urls","title":"<code>validate_urls</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to validate URLs before adding them to results.</p> <p>When True, each URL found in sitemaps will be validated for proper format before being included in results. This helps filter out malformed URLs but adds some processing overhead.</p> <p>Default: True</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If value is not a boolean</p>"},{"location":"api/config/#ethicrawl.config.SitemapConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert configuration to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with all sitemap configuration values</p>"},{"location":"api/context/","title":"Context Module","text":""},{"location":"api/context/#ethicrawl.context.Context","title":"<code>Context</code>","text":"<p>Dependency container for crawler operations providing resource and client access.</p> <p>Context serves as a dependency injection mechanism that bundles a resource (URL) with a client for making requests. It provides type validation for these dependencies and simplifies passing related objects throughout the system.</p> <p>The class enables components to operate with a consistent set of dependencies without having to pass multiple parameters or manage connections independently.</p> <p>Attributes:</p> Name Type Description <code>resource</code> <code>Resource</code> <p>The Resource object representing the current target</p> <code>client</code> <code>Client</code> <p>The Client used to make requests</p> Example <p>from ethicrawl.core import Resource from ethicrawl.client.http import HttpClient context = Context(Resource(\"https://example.com\"), HttpClient()) response = context.client.get(context.resource) logger = context.logger(\"robots\") logger.info(\"Processing robots.txt\")</p>"},{"location":"api/context/#ethicrawl.context.Context.client","title":"<code>client</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current client.</p> <p>Returns:</p> Type Description <code>Client</code> <p>The Client object for this context</p>"},{"location":"api/context/#ethicrawl.context.Context.resource","title":"<code>resource</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current resource.</p> <p>Returns:</p> Type Description <code>Resource</code> <p>The Resource object for this context</p>"},{"location":"api/context/#ethicrawl.context.Context.__init__","title":"<code>__init__(resource, client=None)</code>","text":"<p>Initialize a Context with a resource and optional client.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The Resource object representing the current URL</p> required <code>client</code> <code>Client | None</code> <p>Optional Client to use for requests. If None, a NoneClient will be used as a placeholder.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If resource is not a Resource instance</p> <code>TypeError</code> <p>If client is not a Client instance or None</p>"},{"location":"api/context/#ethicrawl.context.Context.__repr__","title":"<code>__repr__()</code>","text":"<p>Return an unambiguous string representation of the context.</p>"},{"location":"api/context/#ethicrawl.context.Context.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable string representation of the context.</p>"},{"location":"api/context/#ethicrawl.context.Context.logger","title":"<code>logger(component)</code>","text":"<p>Get a component-specific logger within this context.</p> <p>Creates a logger associated with the current resource and the specified component name.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>str</code> <p>Component name for the logger</p> required <p>Returns:</p> Type Description <p>A Logger instance for the specified component</p>"},{"location":"api/context/#ethicrawl.context.ContextManager","title":"<code>ContextManager</code>","text":"<p>Manages target contexts for different domain resources.</p> <p>This class handles the lifecycle of domain contexts, including binding resources to clients, managing robots.txt permissions, and providing access to domain-specific functionality like robots.txt handlers and sitemaps.</p>"},{"location":"api/context/#ethicrawl.context.ContextManager.bind","title":"<code>bind(resource, client=None)</code>","text":"<p>Bind a resource to a client in this context manager.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to bind</p> required <code>client</code> <code>Client | None</code> <p>The client to use for requests to this resource.    If None, uses the default client.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if binding was successful</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If client is not a Client instance or None</p>"},{"location":"api/context/#ethicrawl.context.ContextManager.client","title":"<code>client(resource)</code>","text":"<p>Get the client for a resource's domain.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to get the client for</p> required <p>Returns:</p> Name Type Description <code>Client</code> <code>Client | None</code> <p>The client instance for this domain, or None if not found</p>"},{"location":"api/context/#ethicrawl.context.ContextManager.get","title":"<code>get(resource, headers=None)</code>","text":"<p>Fetch a resource respecting robots.txt rules.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to fetch</p> required <code>headers</code> <code>Headers | None</code> <p>Optional headers for the request</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>The HTTP response from the resource</p> <p>Raises:</p> Type Description <code>RobotDisallowedError</code> <p>If the request is disallowed by robots.txt</p> <code>DomainWhitelistError</code> <p>If the domain is not bound to this context manager</p>"},{"location":"api/context/#ethicrawl.context.ContextManager.robot","title":"<code>robot(resource)</code>","text":"<p>Get the robot instance for a resource's domain.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to get the robot for</p> required <p>Returns:</p> Name Type Description <code>Robot</code> <code>Robot</code> <p>The robot instance for this domain</p> <p>Raises:</p> Type Description <code>DomainWhitelistError</code> <p>If the domain is not registered</p>"},{"location":"api/context/#ethicrawl.context.ContextManager.sitemap","title":"<code>sitemap(resource)</code>","text":"<p>Get the sitemap parser for a resource's domain.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to get the sitemap parser for</p> required <p>Returns:</p> Name Type Description <code>SitemapParser</code> <code>SitemapParser</code> <p>The sitemap parser for this domain</p> <p>Raises:</p> Type Description <code>DomainWhitelistError</code> <p>If the domain is not registered</p>"},{"location":"api/context/#ethicrawl.context.ContextManager.unbind","title":"<code>unbind(resource)</code>","text":"<p>Unbind a resource from this context manager.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to unbind</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if unbinding was successful</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the resource's domain is not bound</p>"},{"location":"api/core/","title":"Core Module","text":"<p>Core types and utilities for resource identification and handling.</p>"},{"location":"api/core/#ethicrawl.core.Headers","title":"<code>Headers</code>","text":"<p>               Bases: <code>dict</code></p> <p>HTTP headers container with case-insensitive key access.</p> <p>This class extends the standard dictionary to provide case-insensitive access to HTTP header keys, conforming to HTTP specifications. It ensures proper type handling and provides flexible initialization from various header sources.</p> <p>Inherits all dictionary attributes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; headers = Headers({\"Content-Type\": \"text/html\"})\n&gt;&gt;&gt; headers[\"content-type\"]\n'text/html'\n&gt;&gt;&gt; headers[\"CONTENT-TYPE\"] = \"application/json\"\n&gt;&gt;&gt; headers[\"content-type\"]\n'application/json'\n&gt;&gt;&gt; \"content-type\" in headers\nTrue\n&gt;&gt;&gt; \"CONTENT-TYPE\" in headers\nTrue\n</code></pre>"},{"location":"api/core/#ethicrawl.core.Headers.__contains__","title":"<code>__contains__(key)</code>","text":"<p>Check if header exists with case-insensitive comparison.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Header name to check (case-insensitive)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if header exists, False otherwise</p>"},{"location":"api/core/#ethicrawl.core.Headers.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get header value with case-insensitive key access.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Header name (case-insensitive)</p> required <p>Returns:</p> Type Description <p>The header value</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If key is not a string</p> <code>KeyError</code> <p>If header doesn't exist</p>"},{"location":"api/core/#ethicrawl.core.Headers.__init__","title":"<code>__init__(headers=None, **kwargs)</code>","text":"<p>Initialize headers from a dictionary, dict-like object, or keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>'Headers' | Mapping[str, Any] | None</code> <p>Optional dictionary, Headers instance, or any dict-like object</p> <code>None</code> <code>**kwargs</code> <p>Optional keyword arguments to add as headers</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If headers is not a dict-like object with an items() method</p>"},{"location":"api/core/#ethicrawl.core.Headers.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set header value with case-insensitive key storage.</p> <p>Setting a header to None will remove it from the collection. Non-string values are automatically converted to strings.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Header name (will be converted to lowercase)</p> required <code>value</code> <code>str | None</code> <p>Header value, or None to remove the header</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If key is not a string</p>"},{"location":"api/core/#ethicrawl.core.Headers.get","title":"<code>get(key, default=None)</code>","text":"<p>Get header value with optional default.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Header name (case-insensitive)</p> required <code>default</code> <p>Value to return if header doesn't exist</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Header value if it exists, otherwise the default value</p>"},{"location":"api/core/#ethicrawl.core.Resource","title":"<code>Resource</code>  <code>dataclass</code>","text":"<p>URL-identified entity within the crawler system.</p> <p>Resource is a generic representation of anything addressable by a URL within the Ethicrawl system. It serves as a common foundation for various components like requests, responses, robots.txt files, sitemap entries, etc.</p> <p>This class provides URL type safety, consistent equality comparison, and proper hashing behavior for all URL-addressable entities.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>The Url object identifying this resource. Can be initialized with either a string or Url object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When initialized with something other than a string or Url object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; resource = Resource(\"https://example.com/robots.txt\")\n&gt;&gt;&gt; resource.url.path\n'/robots.txt'\n&gt;&gt;&gt; resource2 = Resource(Url(\"https://example.com/robots.txt\"))\n&gt;&gt;&gt; resource == resource2\nTrue\n</code></pre>"},{"location":"api/core/#ethicrawl.core.Resource.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare resources for equality based on their URLs.</p> <p>Two resources are considered equal if they have the same URL.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>Another Resource object to compare with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if resources have the same URL, False otherwise</p>"},{"location":"api/core/#ethicrawl.core.Resource.__hash__","title":"<code>__hash__()</code>","text":"<p>Generate a hash based on the string representation of the URL.</p> <p>Returns:</p> Type Description <code>int</code> <p>Integer hash value</p>"},{"location":"api/core/#ethicrawl.core.Resource.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate and normalize the url attribute after initialization.</p> <p>Converts string URLs to Url objects and raises TypeError for invalid types.</p>"},{"location":"api/core/#ethicrawl.core.Resource.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a developer-friendly representation.</p>"},{"location":"api/core/#ethicrawl.core.Resource.__str__","title":"<code>__str__()</code>","text":"<p>Return the URL as a string for better readability.</p>"},{"location":"api/core/#ethicrawl.core.ResourceList","title":"<code>ResourceList</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Collection of Resource objects with filtering capabilities.</p> <p>ResourceList provides list-like functionality specialized for managing collections of Resources with additional filtering methods and type safety. The class is generic and can contain any subclass of Resource.</p> Note <p>This class has no public attributes as all storage is private.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ethicrawl.core import Resource, ResourceList\n&gt;&gt;&gt; resources = ResourceList()\n&gt;&gt;&gt; resources.append(Resource(\"https://example.com/page1\"))\n&gt;&gt;&gt; resources.append(Resource(\"https://example.com/page2\"))\n&gt;&gt;&gt; len(resources)\n2\n&gt;&gt;&gt; filtered = resources.filter(r\"page1\")\n&gt;&gt;&gt; len(filtered)\n1\n</code></pre>"},{"location":"api/core/#ethicrawl.core.ResourceList.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get items by index or slice.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | slice</code> <p>Integer index or slice object</p> required <p>Returns:</p> Type Description <code>T | ResourceList[T]</code> <p>Single Resource when indexed with integer, ResourceList when sliced</p>"},{"location":"api/core/#ethicrawl.core.ResourceList.__init__","title":"<code>__init__(items=None)</code>","text":"<p>Initialize a resource list with optional initial items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[T] | None</code> <p>Optional list of Resource objects to initialize with</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If items is not a list or contains non-Resource objects</p>"},{"location":"api/core/#ethicrawl.core.ResourceList.append","title":"<code>append(item)</code>","text":"<p>Add a resource to the list.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>T</code> <p>Resource object to add</p> required <p>Returns:</p> Type Description <code>ResourceList[T]</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If item is not a Resource object</p>"},{"location":"api/core/#ethicrawl.core.ResourceList.extend","title":"<code>extend(items)</code>","text":"<p>Add multiple resources to the list.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Iterable[T]</code> <p>Iterable of Resource objects to add</p> required <p>Returns:</p> Type Description <code>ResourceList[T]</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any item is not a Resource object</p>"},{"location":"api/core/#ethicrawl.core.ResourceList.filter","title":"<code>filter(pattern)</code>","text":"<p>Filter resources by URL pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str | Pattern</code> <p>String pattern or compiled regex Pattern to match against URLs</p> required <p>Returns:</p> Type Description <code>ResourceList[T]</code> <p>New ResourceList containing only matching resources of the same type as original</p>"},{"location":"api/core/#ethicrawl.core.ResourceList.to_list","title":"<code>to_list()</code>","text":"<p>Convert to a standard Python list.</p> <p>Returns:</p> Type Description <code>list[T]</code> <p>A copy of the internal list of resources</p>"},{"location":"api/core/#ethicrawl.core.Url","title":"<code>Url</code>","text":"<p>URL parser and manipulation class.</p> <p>This class provides methods for parsing, validating, and manipulating URLs. Supports HTTP, HTTPS, and file URL schemes with validation and component access. Path extension and query parameter manipulation are provided through the extend() method.</p> <p>Attributes:</p> Name Type Description <code>scheme</code> <code>str</code> <p>URL scheme (http, https, file)</p> <code>netloc</code> <code>str</code> <p>Network location/domain (HTTP/HTTPS only)</p> <code>hostname</code> <code>str</code> <p>Just the hostname portion of netloc (HTTP/HTTPS only)</p> <code>path</code> <code>str</code> <p>URL path component</p> <code>params</code> <code>str</code> <p>URL parameters (HTTP/HTTPS only)</p> <code>query</code> <code>str</code> <p>Raw query string (HTTP/HTTPS only)</p> <code>query_params</code> <code>dict[str, Any]</code> <p>Query string parsed into a dictionary (HTTP/HTTPS only)</p> <code>fragment</code> <code>str</code> <p>URL fragment identifier (HTTP/HTTPS only)</p> <code>base</code> <code>str</code> <p>Base URL (scheme + netloc)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When provided with invalid URLs or when performing invalid operations</p>"},{"location":"api/core/#ethicrawl.core.Url.base","title":"<code>base</code>  <code>property</code>","text":"<p>Get the base URL (scheme and netloc).</p> <p>Returns:</p> Type Description <code>str</code> <p>The base URL as a string (e.g., 'https://example.com')</p>"},{"location":"api/core/#ethicrawl.core.Url.fragment","title":"<code>fragment</code>  <code>property</code>","text":"<p>Get the fragment identifier from the URL.</p> <p>The fragment appears after # in a URL and typically references a section within a document.</p> <p>Returns:</p> Type Description <code>str</code> <p>Fragment string without the # character</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called on a non-HTTP(S) URL</p>"},{"location":"api/core/#ethicrawl.core.Url.hostname","title":"<code>hostname</code>  <code>property</code>","text":"<p>Get just the hostname part.</p>"},{"location":"api/core/#ethicrawl.core.Url.netloc","title":"<code>netloc</code>  <code>property</code>","text":"<p>Get the network location (domain).</p>"},{"location":"api/core/#ethicrawl.core.Url.params","title":"<code>params</code>  <code>property</code>","text":"<p>Get URL parameters.</p>"},{"location":"api/core/#ethicrawl.core.Url.path","title":"<code>path</code>  <code>property</code>","text":"<p>Get the path component.</p>"},{"location":"api/core/#ethicrawl.core.Url.query","title":"<code>query</code>  <code>property</code>","text":"<p>Get the query string.</p>"},{"location":"api/core/#ethicrawl.core.Url.query_params","title":"<code>query_params</code>  <code>property</code>","text":"<p>Get query parameters as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of query parameter keys and values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called on a non-HTTP(S) URL</p>"},{"location":"api/core/#ethicrawl.core.Url.scheme","title":"<code>scheme</code>  <code>property</code>","text":"<p>Get the URL scheme (file, http or https).</p>"},{"location":"api/core/#ethicrawl.core.Url.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare URLs for equality.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>Another Url object or string to compare with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URLs are equal, False otherwise</p>"},{"location":"api/core/#ethicrawl.core.Url.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hash of the URL.</p> <p>The hash is based on the string representation of the URL, ensuring URLs that are equal have the same hash.</p> <p>Returns:</p> Type Description <code>int</code> <p>Integer hash value</p>"},{"location":"api/core/#ethicrawl.core.Url.__init__","title":"<code>__init__(url, validate=False)</code>","text":"<p>Initialize a URL object with parsing and optional validation.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Union[str, Url]</code> <p>String or Url object to parse</p> required <code>validate</code> <code>bool</code> <p>If True, performs additional validation including DNS resolution</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the URL has an invalid scheme or missing required components</p> <code>ValueError</code> <p>When validate=True and the hostname cannot be resolved</p>"},{"location":"api/core/#ethicrawl.core.Url.__str__","title":"<code>__str__()</code>","text":"<p>Convert URL to string representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>Complete URL string</p>"},{"location":"api/core/#ethicrawl.core.Url.extend","title":"<code>extend(*args)</code>","text":"<p>Extend the URL with additional path components or query parameters.</p> <p>This method supports multiple extension patterns: 1. Path extension: extend(\"path/component\") 2. Single parameter: extend(\"param_name\", \"param_value\") 3. Multiple parameters: extend({\"param1\": \"value1\", \"param2\": \"value2\"})</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Either a path string, a parameter dict, or name/value parameter pair</p> <code>()</code> <p>Returns:</p> Type Description <code>Url</code> <p>A new Url object with the extended path or parameters</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If arguments don't match one of the supported patterns</p> <code>ValueError</code> <p>If trying to add query parameters to a file:// URL</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; url = Url(\"https://example.com/api\")\n&gt;&gt;&gt; url.extend(\"v1\").extend({\"format\": \"json\"})\nUrl(\"https://example.com/api/v1?format=json\")\n</code></pre>"},{"location":"api/error/","title":"Error Classes","text":""},{"location":"api/error/#ethicrawl.error.DomainResolutionError","title":"<code>DomainResolutionError</code>","text":"<p>               Bases: <code>EthicrawlError</code></p> <p>Raised when a domain cannot be resolved through DNS.</p> <p>This error occurs when a hostname in a URL cannot be resolved to an IP address, typically indicating network connectivity issues or a non-existent domain.</p> <p>Attributes:</p> Name Type Description <code>url</code> <p>The URL that was attempted to be accessed</p> <code>hostname</code> <p>The specific hostname that could not be resolved</p>"},{"location":"api/error/#ethicrawl.error.DomainWhitelistError","title":"<code>DomainWhitelistError</code>","text":"<p>               Bases: <code>EthicrawlError</code></p> <p>Raised when attempting to access a non-whitelisted domain.</p> <p>This error occurs when a request is made to a domain that differs from the primary bound domain and hasn't been explicitly whitelisted.</p> <p>Attributes:</p> Name Type Description <code>url</code> <p>URL that was attempted to be accessed</p> <code>bound_domain</code> <p>The domain the ethicrawl is bound to</p>"},{"location":"api/error/#ethicrawl.error.EthicrawlError","title":"<code>EthicrawlError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for all Ethicrawl-specific errors.</p> <p>This class serves as the parent for all custom exceptions raised by the Ethicrawl library. By catching this exception type, client code can handle all Ethicrawl-specific errors while allowing other exceptions to propagate normally.</p> Example <p>try: ...     crawler.get(\"https://example.com/disallowed\") ... except EthicrawlError as e: ...     print(f\"Ethicrawl error: {e}\")</p>"},{"location":"api/error/#ethicrawl.error.RobotDisallowedError","title":"<code>RobotDisallowedError</code>","text":"<p>               Bases: <code>EthicrawlError</code></p> <p>Raised when a URL is disallowed by robots.txt rules.</p> <p>This exception indicates that the requested URL cannot be accessed because it is explicitly disallowed by the site's robots.txt file.</p> <p>Attributes:</p> Name Type Description <code>url</code> <p>The URL that was disallowed</p> <code>robot_url</code> <p>The URL of the robots.txt file that disallowed access</p>"},{"location":"api/error/#ethicrawl.error.SitemapError","title":"<code>SitemapError</code>","text":"<p>               Bases: <code>EthicrawlError</code></p> <p>Raised when a sitemap cannot be parsed or processed.</p> <p>This exception indicates problems with sitemap fetching, parsing, or validation, such as invalid XML or missing required elements.</p>"},{"location":"api/ethicrawl/","title":"Ethicrawl API","text":"<p>The <code>Ethicrawl</code> class provides the main interface for web crawling operations.</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl","title":"<code>Ethicrawl</code>","text":"<p>Main entry point for ethical web crawling operations.</p> <p>This class provides a simplified interface for crawling websites while respecting robots.txt rules, rate limits, and domain boundaries. It manages the lifecycle of crawling operations through binding to domains and provides access to robots.txt and sitemap functionality.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Config</code> <p>Configuration settings for crawling behavior</p> <code>robots</code> <code>Robot</code> <p>Handler for robots.txt rules (available after binding)</p> <code>sitemaps</code> <code>SitemapParser</code> <p>Parser for XML sitemaps (available after binding)</p> <code>logger</code> <code>Logger</code> <p>Logger instance for this ethicrawl (available after binding)</p> <code>bound</code> <code>bool</code> <p>Whether the ethicrawl is currently bound to a site</p> Example <p>from ethicrawl import Ethicrawl ethicrawl = Ethicrawl() ethicrawl.bind(\"https://example.com\") response = ethicrawl.get(\"https://example.com/about\") print(response.status_code) 200</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl--find-urls-in-sitemap","title":"Find URLs in sitemap","text":"<p>urls = ethicrawl.sitemaps.parse() ethicrawl.unbind()  # Clean up when done</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.bound","title":"<code>bound</code>  <code>property</code>","text":"<p>Check if currently bound to a site.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the ethicrawl is bound to a domain, False otherwise</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.config","title":"<code>config</code>  <code>property</code>","text":"<p>Access the configuration settings for this ethicrawl.</p> <p>Returns:</p> Name Type Description <code>Config</code> <code>Config</code> <p>The configuration object with settings for all ethicrawl components</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.logger","title":"<code>logger</code>  <code>property</code>","text":"<p>Get the logger for the current bound domain.</p> <p>This logger is configured according to the settings in Config.logger.</p> <p>Returns:</p> Name Type Description <code>Logger</code> <code>Logger</code> <p>Configured logger instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not bound to a site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.robots","title":"<code>robots</code>  <code>property</code>","text":""},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.sitemaps","title":"<code>sitemaps</code>  <code>property</code>","text":"<p>Access the sitemap parser for the primary bound domain.</p> <p>The parser is created on first access and cached for subsequent calls. It provides methods to extract URLs from XML sitemaps.</p> <p>Returns:</p> Name Type Description <code>SitemapParser</code> <code>SitemapParser</code> <p>Parser for handling XML sitemaps</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not bound to a site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.bind","title":"<code>bind(url, client=None)</code>","text":"<p>Bind the ethicrawl to a specific website domain.</p> <p>Binding establishes the primary domain context with its robots.txt handler, client configuration, and sets up logging for operations on this domain.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | Url | Resource</code> <p>The base URL of the site to crawl (string, Url, or Resource)</p> required <code>client</code> <code>Client | None</code> <p>HTTP client to use for requests. Defaults to a standard Client</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if binding was successful</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL is invalid</p> <code>RuntimeError</code> <p>If already bound to a different site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.get","title":"<code>get(url, headers=None)</code>","text":"<p>Make an HTTP GET request to the specified URL, respecting robots.txt rules and domain whitelisting.</p> <p>This method enforces ethical crawling by: - Checking that the domain is allowed (primary or whitelisted) - Verifying the URL is permitted by robots.txt rules - Using the appropriate client for the domain</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | Url | Resource</code> <p>URL to fetch (string, Url, or Resource)</p> required <code>headers</code> <code>Headers | dict | None</code> <p>Additional headers for this request</p> <code>None</code> <p>Returns:</p> Type Description <code>Response | HttpResponse</code> <p>Response or HttpResponse: The response from the server</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL is from a non-whitelisted domain or disallowed by robots.txt</p> <code>RuntimeError</code> <p>If not bound to a site</p> <code>TypeError</code> <p>If url parameter is not a string, Url, or Resource</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.unbind","title":"<code>unbind()</code>","text":"<p>Unbind the ethicrawl from its current site.</p> <p>This releases resources and allows the ethicrawl to be bound to a different site. It removes all domain contexts, cached resources, and resets the ethicrawl state.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if unbinding was successful</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.whitelist","title":"<code>whitelist(url, client=None)</code>","text":"<p>Add a domain to the whitelist.</p> Deprecated <p>This method is deprecated and will be removed in a future version. Use bind() instead, which now serves the same purpose.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | Url</code> <p>The base URL to whitelist</p> required <code>client</code> <code>HttpClient | None</code> <p>HTTP client to use for this domain</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if whitelisting was successful</p>"},{"location":"api/logger/","title":"Logger Module","text":"<p>Provides structured, configurable logging across the application.</p>"},{"location":"api/logger/#ethicrawl.logger.Logger","title":"<code>Logger</code>","text":"<p>Factory class for creating and managing loggers throughout Ethicrawl.</p> <p>This class provides a centralized way to create and configure loggers based on the application's configuration settings. It supports:</p> <ul> <li>Resource-specific loggers with hierarchical naming</li> <li>Component-specific log levels</li> <li>Console output with optional color formatting</li> <li>File output with configurable paths</li> <li>Initialization management to prevent duplicate configuration</li> </ul> <p>The Logger class is designed as a static utility class rather than being instantiated. All methods are static and operate on the global logging configuration.</p> Example <p>from ethicrawl.logger import Logger from ethicrawl.core import Resource, Url</p>"},{"location":"api/logger/#ethicrawl.logger.Logger--setup-logging-happens-automatically-when-first-logger-is-created","title":"Setup logging (happens automatically when first logger is created)","text":"<p>Logger.setup_logging()</p>"},{"location":"api/logger/#ethicrawl.logger.Logger--get-a-logger-for-a-specific-resource","title":"Get a logger for a specific resource","text":"<p>resource = Resource(Url(\"https://example.com\")) logger = Logger.logger(resource, \"http\") logger.info(\"Making request to %s\", resource.url)</p>"},{"location":"api/logger/#ethicrawl.logger.Logger.logger","title":"<code>logger(resource, component=None)</code>  <code>staticmethod</code>","text":"<p>Get a logger for the specified resource, optionally with a component name.</p> <p>Creates or retrieves a logger with a hierarchical name based on the resource URL and optional component. Automatically initializes logging if not already done.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to create a logger for</p> required <code>component</code> <code>str | None</code> <p>Optional component name (e.g., \"robots\", \"sitemaps\")</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>A logger instance configured according to application settings</p> Example <p>from ethicrawl.core import Resource resource = Resource(\"https://example.com\") logger = Logger.logger(resource, \"http\") logger.debug(\"Processing %s\", resource.url)</p>"},{"location":"api/logger/#ethicrawl.logger.Logger.reset","title":"<code>reset()</code>  <code>staticmethod</code>","text":"<p>Reset logging configuration to initial state.</p> <p>Removes all handlers and resets the initialization flag, allowing logging to be reconfigured. Primarily used for testing.</p> Example"},{"location":"api/logger/#ethicrawl.logger.Logger.reset--in-a-test-setup","title":"In a test setup","text":"<p>def setUp(self):     Logger.reset()  # Ensure clean logging state</p>"},{"location":"api/logger/#ethicrawl.logger.Logger.setup_logging","title":"<code>setup_logging()</code>  <code>staticmethod</code>","text":"<p>Configure the logging system based on current configuration.</p> <p>This method reads the logger configuration from the global Config singleton and sets up handlers, formatters, and log levels accordingly. It should be called once at application startup, but is also called automatically by logger() if needed.</p> <p>The method configures: - Root logger with WARNING level - Main application logger with configured level - Console output (if enabled) - File output (if enabled) - Component-specific log levels</p> <p>This method is idempotent - calling it multiple times has no effect after the initial setup.</p>"},{"location":"api/overview/","title":"Overview","text":"<p>Ethicrawl is organized into several key modules:</p> <ul> <li> <p>Ethicrawl: Provides the main interface for operating with the library.</p> </li> <li> <p>Core: Fundamental components including URL handling, resource management, and standardized data structures. Provides the <code>Resource</code>, <code>ResourceList</code>, and <code>Url</code> classes that are the building blocks of crawling operations.</p> </li> <li> <p>Client: HTTP clients for various access methods:</p> </li> <li>HttpClient: Standard requests-based client for most web resources</li> <li> <p>ChromeClient: Browser automation for JavaScript-heavy sites using Selenium</p> </li> <li> <p>Context: Manages the crawling environment including domain boundaries, allowed paths, and session state. Enforces ethical boundaries during execution.</p> </li> <li> <p>Sitemaps: Tools for parsing and utilizing XML sitemaps. Provides classes for handling <code>IndexEntry</code>, <code>UrlsetEntry</code>, and other sitemap standard formats.</p> </li> <li> <p>Robots: Robots.txt parsing and rule enforcement. Ensures crawlers respect website policies regarding crawlable content.</p> </li> <li> <p>Config: Configuration management for controlling crawler behavior. Provides a singleton <code>Config</code> class for global settings and per-instance overrides.</p> </li> <li> <p>Logger: Customizable logging functionality with sensible defaults for tracking crawler operations.</p> </li> <li> <p>Error: Specialized exception classes for error handling, including <code>DomainWhitelistError</code>, <code>RobotDisallowedError</code>, and other ethical boundary violations.</p> </li> </ul>"},{"location":"api/overview/#getting-started","title":"Getting Started","text":"<p>The typical usage flow follows this pattern:</p> <ol> <li>Create an <code>Ethicrawl</code> instance</li> <li>Bind to a primary domain</li> <li>Access content with the built-in HTTP methods</li> </ol> <pre><code>from ethicrawl import Ethicrawl\nfrom ethicrawl.error import RobotDisallowedError\n\n# Create and bind to a domain\nethicrawl = Ethicrawl()\nethicrawl.bind(\"https://example.com\")\n\n# Get a page - robots.txt rules automatically respected\ntry:\n    response = ethicrawl.get(\"https://example.com/page.html\")\nexcept RobotDisallowedError:\n    print(\"The site prohibits fetching the page\")\n\n# Release resources when done\nethicrawl.unbind()\n</code></pre> <p>For detailed usage examples, see the examples directory in the repository.</p>"},{"location":"api/robots/","title":"Robots Module","text":"<p>Robots.txt parsing and permission checking</p>"},{"location":"api/robots/#ethicrawl.robots.Robot","title":"<code>Robot</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Representation of a site's robots.txt file with permission checking.</p> <p>This class handles fetching, parsing, and enforcing robots.txt rules according to the Robots Exclusion Protocol. It follows standard robots.txt behavior: - 404 response: allow all URLs (fail open) - 200 response: parse and enforce rules in the robots.txt file - Other responses (5xx, etc.): deny all URLs (fail closed)</p> <p>As a Resource subclass, Robot maintains the URL identity of the robots.txt file while providing methods to check permissions and access sitemap references.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the robots.txt file (inherited from Resource)</p> <code>context</code> <code>Context</code> <p>Context with client for making requests</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource, Url from ethicrawl.robots import Robot from ethicrawl.client.http import HttpClient client = HttpClient() context = Context(Resource(\"https://example.com\"), client) robot = Robot(Url(\"https://example.com/robots.txt\"), context) robot.can_fetch(\"https://example.com/allowed\") True</p>"},{"location":"api/robots/#ethicrawl.robots.Robot.context","title":"<code>context</code>  <code>property</code>","text":"<p>Get the context associated with this robot.</p> <p>Returns:</p> Type Description <code>Context</code> <p>The context object containing client and other settings</p>"},{"location":"api/robots/#ethicrawl.robots.Robot.sitemaps","title":"<code>sitemaps</code>  <code>property</code>","text":"<p>Get sitemap URLs referenced in robots.txt.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList containing IndexEntry objects for each sitemap URL</p> Example <p>for sitemap in robot.sitemaps: ...     print(f\"Found sitemap: {sitemap.url}\")</p>"},{"location":"api/robots/#ethicrawl.robots.Robot.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize the robot instance and fetch robots.txt.</p> <p>Fetches the robots.txt file using the provided context's client, then parses it according to response status: - 404: Create empty ruleset (allow all) - 200: Parse actual robots.txt content - Other: Create restrictive ruleset (deny all)</p>"},{"location":"api/robots/#ethicrawl.robots.Robot.can_fetch","title":"<code>can_fetch(resource, user_agent=None)</code>","text":"<p>Check if a URL can be fetched according to robots.txt rules.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource | Url | str</code> <p>The URL to check against robots.txt rules</p> required <code>user_agent</code> <code>str | None</code> <p>Optional user agent string to use for checking. If not provided, uses client's user_agent or config default.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the URL is allowed by robots.txt</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If resource is not a string, Url, or Resource</p> <code>RobotDisallowedError</code> <p>If the URL is disallowed by robots.txt</p> Example <p>if robot.can_fetch(\"https://example.com/page\"): ...     response = client.get(Resource(\"https://example.com/page\"))</p>"},{"location":"api/robots/#ethicrawl.robots.RobotFactory","title":"<code>RobotFactory</code>","text":"<p>Factory for creating Robot instances and robots.txt URLs.</p> <p>This utility class provides methods for: - Converting site URLs to robots.txt URLs - Creating properly initialized Robot instances from a Context</p> <p>Using this factory ensures consistent Robot creation throughout the application and handles the necessary URL transformations.</p>"},{"location":"api/robots/#ethicrawl.robots.RobotFactory.robot","title":"<code>robot(context)</code>  <code>staticmethod</code>","text":"<p>Create a Robot instance from a Context.</p> <p>Extracts the URL from the context's resource, converts it to a robots.txt URL, and creates a new Robot instance bound to that URL and the provided context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>The context to use for the Robot</p> required <p>Returns:</p> Type Description <code>Robot</code> <p>A fully initialized Robot instance</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If context is not a Context instance</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.robots import RobotFactory ctx = Context(Resource(\"https://example.com\")) robot = RobotFactory.robot(ctx) robot.can_fetch(\"https://example.com/page\") True</p>"},{"location":"api/robots/#ethicrawl.robots.RobotFactory.robotify","title":"<code>robotify(url)</code>  <code>staticmethod</code>","text":"<p>Convert a site URL to its corresponding robots.txt URL.</p> <p>Takes any URL and transforms it to the canonical robots.txt URL by extracting the base URL and appending \"robots.txt\".</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Url</code> <p>The site URL to convert</p> required <p>Returns:</p> Type Description <code>Url</code> <p>A new Url pointing to the site's robots.txt file</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If url is not a Url instance</p> Example <p>from ethicrawl.core import Url from ethicrawl.robots import RobotFactory site = Url(\"https://example.com/some/page\") robots_url = RobotFactory.robotify(site) str(robots_url) 'https://example.com/robots.txt'</p>"},{"location":"api/sitemaps/","title":"Sitemaps Module","text":"<p>XML sitemap parsing and traversal for discovering website structure.</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexDocument","title":"<code>IndexDocument</code>","text":"<p>               Bases: <code>SitemapDocument</code></p> <p>Specialized parser for sitemap index documents.</p> <p>This class extends the SitemapDocument class to handle sitemap indexes, which are XML documents containing references to other sitemap files. It validates that the document is a proper sitemap index and extracts all sitemap references as IndexEntry objects.</p> <p>IndexDocument enforces type safety for its entries collection, ensuring that only IndexEntry objects can be added.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>ResourceList</code> <p>ResourceList of IndexEntry objects representing sitemap references</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.sitemaps import IndexDocument context = Context(Resource(\"https://example.com\")) sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; ...  ...    ...     https://example.com/sitemap1.xml ...     2023-06-15T14:30:00Z ...    ... ''' index = IndexDocument(context, sitemap_xml) len(index.entries) 1 str(index.entries[0]) 'https://example.com/sitemap1.xml (last modified: 2023-06-15T14:30:00Z)'</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexDocument.entries","title":"<code>entries</code>  <code>property</code> <code>writable</code>","text":"<p>Get the sitemaps in this index.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList of IndexEntry objects representing sitemap references</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexDocument.__init__","title":"<code>__init__(context, document=None)</code>","text":"<p>Initialize a sitemap index document parser.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Context for logging and resource resolution</p> required <code>document</code> <code>str | None</code> <p>Optional XML sitemap index content to parse</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the document is not a valid sitemap index</p> <code>SitemapError</code> <p>If the document cannot be parsed</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexEntry","title":"<code>IndexEntry</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SitemapEntry</code></p> <p>Represents an entry in a sitemap index file.</p> <p>IndexEntry specializes SitemapEntry for use in sitemap index files. Sitemap indexes are XML files that contain references to other sitemap files, allowing websites to organize their sitemaps hierarchically.</p> <p>IndexEntry maintains the same attributes as SitemapEntry (url and lastmod) but provides specialized string representation appropriate for index entries.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the sitemap file (inherited from Resource)</p> <code>lastmod</code> <code>str | None</code> <p>Last modification date of the sitemap (inherited from SitemapEntry)</p> Example <p>from ethicrawl.core import Url from ethicrawl.sitemaps import IndexEntry index = IndexEntry( ...     Url(\"https://example.com/sitemap-products.xml\"), ...     lastmod=\"2023-06-15T14:30:00Z\" ... ) str(index) 'https://example.com/sitemap-products.xml (last modified: 2023-06-15T14:30:00Z)' repr(index) \"SitemapIndexEntry(url='https://example.com/sitemap-products.xml', lastmod='2023-06-15T14:30:00Z')\"</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexEntry.__repr__","title":"<code>__repr__()</code>","text":"<p>Detailed representation for debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation showing class name and field values</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument","title":"<code>SitemapDocument</code>","text":"<p>Parser and representation of XML sitemap documents.</p> <p>This class handles the parsing, validation, and extraction of entries from XML sitemap documents, supporting both sitemap index files and urlset files. It implements security best practices for XML parsing to prevent common vulnerabilities like XXE attacks.</p> <p>SitemapDocument distinguishes between sitemap indexes (which contain references to other sitemaps) and urlsets (which contain actual page URLs), extracting the appropriate entries in each case.</p> <p>Attributes:</p> Name Type Description <code>SITEMAP_NS</code> <p>The official sitemap namespace URI</p> <code>entries</code> <code>ResourceList</code> <p>ResourceList containing the parsed sitemap entries</p> <code>type</code> <code>str</code> <p>The type of sitemap (sitemapindex, urlset, or unsupported)</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.sitemaps import SitemapDocument context = Context(Resource(\"https://example.com\")) sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; ...  ...    ...     https://example.com/page1 ...     2023-06-15T14:30:00Z ...    ... ''' sitemap = SitemapDocument(context, sitemap_xml) sitemap.type 'urlset' len(sitemap.entries) 1</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument.entries","title":"<code>entries</code>  <code>property</code>","text":"<p>Get the list of entries extracted from the sitemap.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList containing SitemapEntry objects (either IndexEntry</p> <code>ResourceList</code> <p>or UrlsetEntry depending on the sitemap type)</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument.type","title":"<code>type</code>  <code>property</code>","text":"<p>Get the type of sitemap document.</p> <p>Determines the type based on the root element's local name.</p> <p>Returns:</p> Type Description <code>str</code> <p>String indicating the sitemap type:</p> <code>str</code> <ul> <li>'sitemapindex': A sitemap index containing references to other sitemaps</li> </ul> <code>str</code> <ul> <li>'urlset': A sitemap containing page URLs</li> </ul> <code>str</code> <ul> <li>'unsupported': Any other type of document</li> </ul> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If no document has been loaded</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument.__init__","title":"<code>__init__(context, document=None)</code>","text":"<p>Initialize a sitemap document parser with security protections.</p> <p>Sets up the XML parser with security features to prevent common XML vulnerabilities and optionally parses a provided document.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Context for logging and resource resolution</p> required <code>document</code> <code>str | None</code> <p>Optional XML sitemap content to parse immediately</p> <code>None</code> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If the provided document cannot be parsed</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapEntry","title":"<code>SitemapEntry</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Base class for entries found in XML sitemaps.</p> <p>SitemapEntry extends Resource to represent entries from XML sitemaps with their additional metadata. It maintains the URL identity pattern while adding sitemap-specific attributes like the last modification date.</p> <p>This class handles validation of W3C datetime formats and provides appropriate string representation of sitemap entries.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the sitemap entry (inherited from Resource)</p> <code>lastmod</code> <code>str | None</code> <p>Last modification date string in W3C format (optional)</p> Example <p>from ethicrawl.core import Url from ethicrawl.sitemaps import SitemapEntry entry = SitemapEntry( ...     Url(\"https://example.com/page1\"), ...     lastmod=\"2023-06-15T14:30:00Z\" ... ) str(entry) 'https://example.com/page1 (last modified: 2023-06-15T14:30:00Z)'</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapEntry.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate fields after initialization.</p> <p>Validates the lastmod date format if provided, ensuring it conforms to one of the accepted W3C datetime formats.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lastmod format is invalid</p> <code>TypeError</code> <p>If lastmod is not a string</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapEntry.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation of the sitemap entry.</p> <p>Returns:</p> Type Description <code>str</code> <p>URL string with last modification date if available</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser","title":"<code>SitemapParser</code>","text":"<p>Recursive parser for extracting URLs from sitemap documents.</p> <p>This class handles the traversal of sitemap structures, including nested sitemap indexes, to extract all page URLs. It implements:</p> <ul> <li>Recursive traversal of sitemap indexes</li> <li>Depth limiting to prevent excessive recursion</li> <li>Cycle detection to prevent infinite loops</li> <li>URL deduplication</li> <li>Multiple input formats (IndexDocument, ResourceList, etc.)</li> </ul> <p>Attributes:</p> Name Type Description <code>context</code> <p>Context with client for fetching sitemaps and logging</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource, Url from ethicrawl.sitemaps import SitemapParser context = Context(Resource(Url(\"https://example.com\"))) parser = SitemapParser(context)</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser--parse-from-a-single-sitemap-url","title":"Parse from a single sitemap URL","text":"<p>urls = parser.parse([Resource(Url(\"https://example.com/sitemap.xml\"))]) print(f\"Found {len(urls)} URLs in sitemap\")</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser.__init__","title":"<code>__init__(context)</code>","text":"<p>Initialize the sitemap parser.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Context with client for fetching sitemaps and logging</p> required"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser.parse","title":"<code>parse(root=None)</code>","text":"<p>Parse sitemap(s) and extract all contained URLs.</p> <p>This is the main entry point for sitemap parsing. It accepts various input formats and recursively extracts all URLs from the sitemap(s).</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>IndexDocument | ResourceList | list[Resource] | None</code> <p>Source to parse, which can be: - IndexDocument: Pre-parsed sitemap index - ResourceList: List of resources to fetch as sitemaps - list[Resource]: List of resources to fetch as sitemaps - None: Use the context's base URL for robots.txt discovery</p> <code>None</code> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList containing all page URLs found in the sitemap(s)</p> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If a sitemap cannot be fetched or parsed</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetDocument","title":"<code>UrlsetDocument</code>","text":"<p>               Bases: <code>SitemapDocument</code></p> <p>Specialized parser for sitemap urlset documents.</p> <p>This class extends SitemapDocument to handle urlset sitemaps, which contain page URLs with metadata like change frequency and priority. It validates that the document is a proper urlset and extracts all URL references as UrlsetEntry objects.</p> <p>UrlsetDocument supports only the core sitemap protocol specification elements (loc, lastmod, changefreq, priority) and does not handle any sitemap protocol extensions.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>ResourceList</code> <p>ResourceList of UrlsetEntry objects representing page URLs</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.sitemaps import UrlsetDocument context = Context(Resource(\"https://example.com\")) sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; ...  ...    ...     https://example.com/page1 ...     2023-06-15T14:30:00Z ...     weekly ...     0.8 ...    ... ''' urlset = UrlsetDocument(context, sitemap_xml) len(urlset.entries) 1 entry = urlset.entries[0] entry.priority '0.8'</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetDocument.entries","title":"<code>entries</code>  <code>property</code> <code>writable</code>","text":"<p>Get the URLs in this urlset.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList of UrlsetEntry objects representing page URLs</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetDocument.__init__","title":"<code>__init__(context, document=None)</code>","text":"<p>Initialize a urlset sitemap document parser.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Context for logging and resource resolution</p> required <code>document</code> <code>str | None</code> <p>Optional XML urlset content to parse</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the document is not a valid urlset</p> <code>SitemapError</code> <p>If the document cannot be parsed</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetEntry","title":"<code>UrlsetEntry</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SitemapEntry</code></p> <p>Represents an entry in a sitemap urlset file.</p> <p>UrlsetEntry specializes SitemapEntry for standard sitemap URL entries that contain page URLs with metadata. These entries represent actual content pages on a website, as opposed to index entries that point to other sitemap files.</p> <p>In addition to the URL and lastmod attributes inherited from SitemapEntry, UrlsetEntry adds support for: - changefreq: How frequently the page is likely to change - priority: Relative importance of this URL (0.0-1.0)</p> <p>All attributes are validated during initialization to ensure they conform to the sitemap protocol specification.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the page (inherited from Resource)</p> <code>lastmod</code> <code>str | None</code> <p>Last modification date (inherited from SitemapEntry)</p> <code>changefreq</code> <code>str | None</code> <p>Update frequency (always, hourly, daily, weekly, etc.)</p> <code>priority</code> <code>float | str | None</code> <p>Relative importance value from 0.0 to 1.0</p> Example <p>from ethicrawl.core import Url from ethicrawl.sitemaps import UrlsetEntry entry = UrlsetEntry( ...     Url(\"https://example.com/page1\"), ...     lastmod=\"2023-06-15T14:30:00Z\", ...     changefreq=\"weekly\", ...     priority=0.8 ... ) str(entry) 'https://example.com/page1 | last modified: 2023-06-15T14:30:00Z | frequency: weekly | priority: 0.8'</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetEntry.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate fields after initialization.</p> <p>Calls the parent class validation, then validates and normalizes the changefreq and priority attributes if they are provided.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any field contains invalid values</p> <code>TypeError</code> <p>If any field has an incorrect type</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetEntry.__repr__","title":"<code>__repr__()</code>","text":"<p>Detailed representation for debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation showing class name and all field values</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetEntry.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> <p>Creates a pipe-separated string containing the URL and any available metadata (lastmod, changefreq, priority).</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with URL and metadata</p>"}]}