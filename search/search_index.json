{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ethicrawl","text":"<p>A Python library for ethical web crawling that respects robots.txt rules, maintains proper rate limits, and provides powerful tools for web scraping.</p>"},{"location":"#project-goals","title":"Project Goals","text":"<p>Ethicrawl is built on the principle that web crawling should be:</p> <ul> <li>Ethical: Respect website owners' rights and server resources</li> <li>Safe: Prevent accidental overloading of servers or violation of policies</li> <li>Powerful: Provide a complete toolkit for professional web crawling</li> <li>Extensible: Support customization for diverse crawling needs</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Robots.txt Compliance: Automatic parsing and enforcement of robots.txt rules</li> <li>Rate Limiting: Built-in, configurable request rate management</li> <li>Sitemap Support: Parse and filter XML sitemaps to discover content</li> <li>Domain Control: Explicit whitelisting for cross-domain access</li> <li>Flexible Configuration: Easily configure all aspects of crawling behavior</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version from PyPI:</p> <pre><code>pip install ethicrawl\n</code></pre> <p>For development:</p> <pre><code># Clone the repository\ngit clone https://github.com/ethicrawl/ethicrawl.git\n\n# Navigate to the directory\ncd ethicrawl\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from ethicrawl import Ethicrawl\nfrom ethicrawl.error import RobotDisallowedError\n\n# Create and bind to a domain\nethicrawl = Ethicrawl()\nethicrawl.bind(\"https://example.com\")\n\n# Get a page - robots.txt rules automatically respected\ntry:\n    response = ethicrawl.get(\"/page.html\")\nexcept RobotDisallowedError:\n    print(\"The site prohibits fetching the page\")\n\n# Release resources when done\nethicrawl.unbind()\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 License - See LICENSE file for details.</p>"},{"location":"standards/","title":"Coding Standards","text":""},{"location":"standards/#python-code-standards","title":"Python Code Standards","text":""},{"location":"standards/#general-requirements","title":"General Requirements","text":"<ul> <li>Python Version: We target Python 3.10 and above</li> <li>Code may work on earlier versions but is not tested or supported</li> <li>Line Endings: Use UNIX-style line endings (LF, <code>\\n</code>)</li> <li>Indentation: Use 4 spaces for indentation (no tabs)</li> <li>Maximum Line Length: 88 characters (Black default)</li> </ul>"},{"location":"standards/#code-formatting","title":"Code Formatting","text":"<p>We use Black as our code formatter with default settings:</p> <pre><code># Install Black\npip install black\n\n# Format a single file\nblack path/to/file.py\n\n# Format all Python files in a directory\nblack directory_name/\n</code></pre>"},{"location":"standards/#import-organization","title":"Import Organization","text":"<p>Imports should be grouped in the following order, with a blank line between each group:</p> <ul> <li>Standard library imports</li> <li>Third-party imports</li> <li>Local application imports</li> </ul> <pre><code># Standard library\nimport os\nfrom datetime import datetime\n\n# Third-party\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Local\nfrom ethicrawl.core import Resource\nfrom ethicrawl.config import Config\n</code></pre>"},{"location":"standards/#type-hints","title":"Type Hints","text":"<p>Use type hints for all public methods and functions:</p> <pre><code>def process_url(url: str, timeout: Optional[int] = None) -&gt; Dict[str, Any]:\n    \"\"\"Process the given URL and return results.\"\"\"\n</code></pre>"},{"location":"standards/#methodfunction-docstrings","title":"Method/Function Docstrings","text":"<pre><code>def can_fetch(self, url: str, user_agent: str = None) -&gt; bool:\n    \"\"\"Check if a URL can be fetched according to robots.txt rules.\n\n    Args:\n        url: The URL to check against robots.txt rules\n        user_agent: Optional user agent string to use for checking.\n            Defaults to the client's configured user agent.\n\n    Returns:\n        True if the URL is allowed, False otherwise.\n\n    Raises:\n        RobotDisallowedError: If access is denied and raise_on_disallow=True\n        ValueError: If the URL is malformed\n    \"\"\"\n</code></pre>"},{"location":"standards/#error-handling-style-guide-for-ethicrawl","title":"Error Handling Style Guide for Ethicrawl","text":"<p>A consistent approach to error handling improves code readability and helps developers understand and handle errors effectively. This document outlines our standards for raising exceptions in the Ethicrawl codebase.</p>"},{"location":"standards/#general-principles","title":"General Principles","text":"<ol> <li>Be specific - Use the most specific exception type appropriate for the error</li> <li>Be descriptive - Error messages should help users identify and fix problems</li> <li>Be consistent - Follow the same patterns throughout the codebase</li> </ol>"},{"location":"standards/#standard-exception-types","title":"Standard Exception Types","text":""},{"location":"standards/#typeerror","title":"TypeError","text":"<p>Use for incorrect argument types or invalid operations on types.</p>"},{"location":"standards/#format","title":"Format:","text":"<pre><code>raise TypeError(f\"Expected {expected_type}, got {type(actual).__name__}\")\n</code></pre>"},{"location":"standards/#examples","title":"Examples:","text":"<pre><code>raise TypeError(f\"Expected string, Url, or Resource, got {type(resource).__name__}\")\nraise TypeError(\"headers must be a Headers instance or dictionary\")\n</code></pre>"},{"location":"standards/#valueerror","title":"ValueError","text":"<p>Use for arguments that have the right type but an invalid value.</p>"},{"location":"standards/#format_1","title":"Format:","text":"<pre><code>raise ValueError(f\"{parameter_name} must be {constraint}\")\n</code></pre>"},{"location":"standards/#examples_1","title":"Examples:","text":"<pre><code>raise ValueError(\"jitter must be between 0.0 and 1.0\")\nraise ValueError(\"max_retries cannot be negative\")\n</code></pre>"},{"location":"standards/#domain-specific-exceptions","title":"Domain-Specific Exceptions","text":"<p>Create custom exceptions for domain-specific errors. All custom exceptions should:</p> <ol> <li>Inherit from appropriate base exceptions</li> <li>Include \"Error\" in the class name</li> <li>Be placed in a relevant _error.py module</li> </ol>"},{"location":"standards/#example","title":"Example:","text":"<pre><code>class RobotDisallowedError(ValueError):\n    \"\"\"Raised when access to a URL is disallowed by robots.txt rules.\"\"\"\n</code></pre>"},{"location":"standards/#error-message-guidelines","title":"Error Message Guidelines","text":"<ol> <li>Be specific about what went wrong</li> <li>Provide the invalid value when helpful</li> <li>Suggest a fix when possible</li> <li>Use consistent terminology across similar errors</li> </ol>"},{"location":"standards/#documenting-exceptions","title":"Documenting Exceptions","text":"<pre><code>def function_name():\n    \"\"\"Function description.\n\n    Args:\n        param_name: Parameter description.\n\n    Returns:\n        Return description.\n\n    Raises:\n        ExceptionType: Condition when raised.\n        AnotherException: Another condition.\n    \"\"\"\n</code></pre> <p>Always document exceptions in docstrings:</p>"},{"location":"standards/#error-assertions","title":"Error Assertions","text":"<p>For internal logic verification, use assertions with descriptive messages:</p> <pre><code>assert isinstance(item, Resource), f\"Expected Resource, got {type(item).__name__}\"\n</code></pre>"},{"location":"standards/#documentation","title":"Documentation","text":"<p>We follow Google-style docstrings for all code documentation.</p>"},{"location":"standards/#documentation-focus-areas","title":"Documentation Focus Areas","text":"<ul> <li>All public APIs (methods, classes, and modules) must have comprehensive docstrings</li> <li>Private methods with complex logic should have docstrings</li> <li>Simple private methods or properties may omit docstrings</li> <li>Focus on documentation that enhances IDE tooltips and developer experience</li> </ul>"},{"location":"standards/#docstring-format","title":"Docstring Format","text":""},{"location":"standards/#module-docstrings","title":"Module Docstrings","text":"<pre><code>\"\"\"Module for handling robots.txt parsing and permission checking.\n\nThis module provides functionality for fetching, parsing and checking\npermissions against robots.txt files according to the Robots Exclusion\nProtocol.\n\"\"\"\n</code></pre>"},{"location":"standards/#class-docstrings","title":"Class Docstrings","text":"<pre><code>class Robot:\n    \"\"\"Representation of a robots.txt file with permission checking.\n\n    This class handles fetching and parsing robots.txt files and provides\n    methods to check if URLs can be accessed according to the rules.\n\n    Attributes:\n        url: The URL of the robots.txt file\n        sitemaps: List of sitemap URLs found in robots.txt\n    \"\"\"\n</code></pre>"},{"location":"standards/#methodfunction-docstrings_1","title":"Method/Function Docstrings","text":"<pre><code>def can_fetch(self, url: str, user_agent: str = None) -&gt; bool:\n    \"\"\"Check if a URL can be fetched according to robots.txt rules.\n\n    Args:\n        url: The URL to check against robots.txt rules\n        user_agent: Optional user agent string to use for checking.\n            Defaults to the client's configured user agent.\n\n    Returns:\n        True if the URL is allowed, False otherwise.\n\n    Raises:\n        RobotDisallowedError: If access is denied and raise_on_disallow=True\n        ValueError: If the URL is malformed\n\n    Example:\n        &gt;&gt;&gt; robot = Robot(\"https://example.com/robots.txt\")\n        &gt;&gt;&gt; robot.can_fetch(\"https://example.com/allowed\")\n        True\n        &gt;&gt;&gt; robot.can_fetch(\"https://example.com/disallowed\")\n        False\n</code></pre>"},{"location":"standards/#property-docstrings","title":"Property Docstrings","text":"<pre><code>@property\ndef sitemaps(self) -&gt; List[str]:\n    \"\"\"List of sitemap URLs found in robots.txt.\n\n    Returns:\n        List of sitemap URLs as strings.\n    \"\"\"\n</code></pre>"},{"location":"standards/#constructor-docstrings","title":"Constructor Docstrings","text":"<pre><code>def __init__(self, url: str, context: Context = None):\n    \"\"\"Initialize a Robot instance.\n\n    Args:\n        url: URL to the robots.txt file\n        context: Optional context for the request.\n            If not provided, a default context will be created.\n    \"\"\"\n</code></pre>"},{"location":"standards/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>All public methods, classes, and modules must have docstrings (100% coverage)</li> <li>Private methods with complex logic should have docstrings</li> <li>Simple private methods or properties may omit docstrings</li> <li>The overall project requires a minimum of 95% docstring coverage as measured by interrogate</li> </ul>"},{"location":"standards/#special-cases","title":"Special Cases","text":""},{"location":"standards/#private-methods","title":"Private Methods","text":"<p>Private methods (starting with underscore) should have docstrings if they: - Contain complex logic - Are called from multiple places - Would benefit from documentation for maintainers</p>"},{"location":"standards/#one-line-docstrings","title":"One-line Docstrings","text":"<p>For simple methods with obvious behavior, a one-line docstring is acceptable:</p> <pre><code>def is_allowed(self, url: str) -&gt; bool:\n    \"\"\"Return True if the URL is allowed by robots.txt.\"\"\"\n</code></pre>"},{"location":"standards/#verification-approach","title":"Verification Approach","text":"<p>Documentation quality is verified through code review rather than automated metrics. Reviewers should confirm that:</p> <ul> <li>Public APIs have clear, complete docstrings</li> <li>Examples are provided for non-obvious usage</li> <li>Type information is present in docstrings</li> <li>Error cases and exceptions are documented</li> </ul>"},{"location":"standards/#documentation-language-guidelines","title":"Documentation Language Guidelines","text":"<p>When writing documentation, follow these language principles:</p> <ol> <li>Be objective - Avoid subjective descriptors like \"elegant\", \"beautiful\", or \"clever\"</li> <li>Be precise - Focus on what code does, not subjective quality judgments</li> <li>Be technical - Use concrete technical terms rather than metaphorical language</li> <li>Be consistent - Maintain a neutral, professional tone throughout documentation</li> </ol>"},{"location":"standards/#examples_2","title":"Examples:","text":"<p>Avoid: <pre><code>\"\"\"This elegant pattern enables seamless chaining of operations.\"\"\"\n</code></pre></p> <p>Better: <pre><code>\"\"\"This design allows operation outputs to serve as inputs to other operations.\"\"\"\n</code></pre></p> <p>Avoid <pre><code>\"\"\"Beautiful integration between components creates a powerful system.\"\"\"\n</code></pre></p> <p>Better <code>Components communicate through well-defined interfaces that enable extensibility.</code></p>"},{"location":"standards/#logging-standards-for-ethicrawl","title":"Logging Standards for Ethicrawl","text":"<p>Good logging practices are essential for troubleshooting, monitoring, and understanding application behavior.</p>"},{"location":"standards/#log-levels-and-their-uses","title":"Log Levels and Their Uses","text":""},{"location":"standards/#critical-loggingcritical-50","title":"CRITICAL (logging.CRITICAL, 50)","text":"<p>Use for severe errors that prevent core functionality from working.</p>"},{"location":"standards/#when-to-use","title":"When to use:","text":"<ul> <li>Application cannot continue functioning</li> <li>Data corruption or loss has occurred</li> <li>Security breaches or compromises</li> <li>Resource exhaustion that threatens system stability</li> </ul>"},{"location":"standards/#examples_3","title":"Examples:","text":"<pre><code>logger.critical(f\"Failed to initialize client: {error}\")\nlogger.critical(f\"Persistent storage corruption detected in {file_path}\")\n</code></pre>"},{"location":"standards/#error-loggingerror-40","title":"ERROR (logging.ERROR, 40)","text":"<p>Use for runtime errors that prevent specific operations from completing but don't crash the application.</p>"},{"location":"standards/#when-to-use_1","title":"When to use:","text":"<ul> <li>Failed HTTP requests</li> <li>Failed data processing operations</li> <li>Configuration errors</li> <li>External service unavailability</li> <li>Unexpected exceptions in non-critical paths</li> </ul>"},{"location":"standards/#examples_4","title":"Examples:","text":"<pre><code>logger.error(f\"HTTP request failed: {status_code} {reason}\")\nlogger.error(f\"Failed to parse sitemap at {url}: {error_message}\")\n</code></pre>"},{"location":"standards/#warning-loggingwarning-30","title":"WARNING (logging.WARNING, 30)","text":"<p>Use for conditions that might cause problems but allow operations to continue.</p>"},{"location":"standards/#when-to-use_2","title":"When to use:","text":"<ul> <li>Deprecated feature usage</li> <li>Slow response times</li> <li>Retrying operations after recoverable failures</li> <li>Access denied for certain operations</li> <li>Unexpected data formats that can be handled</li> <li>Rate limiting being applied</li> </ul>"},{"location":"standards/#examples_5","title":"Examples:","text":"<pre><code>logger.warning(f\"URL disallowed by robots.txt: {url}\")\nlogger.warning(f\"Slow response from {domain}: {response_time}s\")\nlogger.warning(f\"Retrying request ({retry_count}/{max_retries})\")\n</code></pre>"},{"location":"standards/#info-logginginfo-20","title":"INFO (logging.INFO, 20)","text":"<p>Use for normal operational events and milestones.</p>"},{"location":"standards/#when-to-use_3","title":"When to use:","text":"<ul> <li>Application startup and shutdown</li> <li>Configuration settings</li> <li>Successful site binding and crawling</li> <li>Processing milestones</li> <li>Summary information about operations</li> <li>Changes to application state</li> </ul>"},{"location":"standards/#examples_6","title":"Examples:","text":"<pre><code>logger.info(f\"Bound to site: {url}\")\nlogger.info(f\"Robots.txt processed: {allowed_count} allowed paths, {disallowed_count} disallowed\")\nlogger.info(f\"Processed {page_count} pages in {duration}s\")\n</code></pre>"},{"location":"standards/#debug-loggingdebug-10","title":"DEBUG (logging.DEBUG, 10)","text":"<p>Use for detailed information useful during development and debugging.</p>"},{"location":"standards/#when-to-use_4","title":"When to use:","text":"<ul> <li>Function entry/exit points</li> <li>Variable values and state changes</li> <li>Decision logic paths</li> <li>Low-level HTTP details</li> <li>Parsing steps</li> <li>Rate limiting details</li> </ul>"},{"location":"standards/#examples_7","title":"Examples:","text":"<pre><code>logger.debug(f\"Processing URL: {url}\")\nlogger.debug(f\"Page found in cache, age: {cache_age}s\")\nlogger.debug(f\"Parser state: {current_state}\")\n</code></pre>"},{"location":"standards/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li>Be Concise and Specific</li> <li>Include exactly what happened and where</li> <li> <p>Use active voice (e.g., \"Failed to connect\" instead of \"Connection failure occurred\")</p> </li> <li> <p>Include Context</p> </li> <li>Always include relevant identifiers (URLs, IDs, component names)</li> <li>Include relevant variable values</li> <li> <p>For errors, include exception messages and/or stack traces</p> </li> <li> <p>Be Consistent</p> </li> <li>Use consistent terminology across similar log messages</li> <li>Use consistent formatting for similar events</li> <li> <p>Use sentence case for log messages (capitalize first word)</p> </li> <li> <p>Avoid Sensitive Information</p> </li> <li>No authentication credentials</li> <li>No personal data</li> <li> <p>No sensitive headers or tokens</p> </li> <li> <p>Use Structured Fields for Machine Parsing</p> </li> <li>Place structured data at the end of the message</li> <li>Use consistent key-value format: <code>key=value</code></li> </ol>"},{"location":"standards/#component-specific-guidelines","title":"Component-Specific Guidelines","text":"<p>Each component should have a consistent logging identity:</p> <ol> <li>Robot/Robots.txt</li> <li>INFO: Robots.txt fetching and parsing results</li> <li>WARNING: Disallowed access attempts</li> <li> <p>ERROR: Failed to fetch/parse robots.txt</p> </li> <li> <p>HTTP Client</p> </li> <li>DEBUG: Request details</li> <li>INFO: Rate limiting information</li> <li>WARNING: Retries and slow responses</li> <li> <p>ERROR: Failed requests</p> </li> <li> <p>Sitemap</p> </li> <li>INFO: Sitemap discovery and parsing</li> <li>WARNING: Malformed but recoverable sitemap content</li> <li>ERROR: Failed sitemap fetching/parsing</li> </ol>"},{"location":"api/ethicrawl/","title":"Ethicrawl API","text":"<p>Ethicrawl - An ethical web crawler that respects robots.txt and rate limits.</p> <p>This package provides tools for crawling websites in a respectful manner, following robots.txt rules and maintaining reasonable request rates.</p>"},{"location":"api/ethicrawl/#ethicrawl.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>Global configuration singleton for Ethicrawl.</p> <p>This class provides a centralized, thread-safe configuration system for all components of Ethicrawl. It implements the Singleton pattern to ensure consistent settings throughout the application.</p> <p>The configuration is organized into sections (http, logger, sitemap) with each section containing component-specific settings.</p> Thread Safety <p>All configuration updates are protected by a reentrant lock, ensuring thread-safe operation in multi-threaded crawling scenarios.</p> Integration Features <ul> <li>Convert to/from dictionaries for integration with external config systems</li> <li>JSON serialization for storage or transmission</li> <li>Hierarchical structure matches common config formats</li> </ul> <p>Attributes:</p> Name Type Description <code>http</code> <code>HttpConfig</code> <p>HTTP-specific configuration (user agent, headers, timeout)</p> <code>logger</code> <code>LoggerConfig</code> <p>Logging configuration (levels, format, output)</p> <code>sitemap</code> <code>SitemapConfig</code> <p>Sitemap parsing configuration (limits, defaults)</p> Example <p>from ethicrawl.config import Config config = Config()  # Get the global instance config.http.user_agent = \"MyCustomBot/1.0\" config.logger.level = \"DEBUG\"</p>"},{"location":"api/ethicrawl/#ethicrawl.Config--thread-safe-update-of-multiple-settings-at-once","title":"Thread-safe update of multiple settings at once","text":"<p>config.update({ ...     \"http\": {\"timeout\": 30}, ...     \"logger\": {\"component_levels\": {\"robots\": \"DEBUG\"}} ... })</p>"},{"location":"api/ethicrawl/#ethicrawl.Config--get-a-snapshot-for-thread-safe-reading","title":"Get a snapshot for thread-safe reading","text":"<p>snapshot = config.get_snapshot() print(snapshot.http.timeout) 30</p>"},{"location":"api/ethicrawl/#ethicrawl.Config--export-config-for-integration-with-external-systems","title":"Export config for integration with external systems","text":"<p>config_dict = config.to_dict() config_json = str(config)</p>"},{"location":"api/ethicrawl/#ethicrawl.Config.__str__","title":"<code>__str__()</code>","text":"<p>Format the configuration as a JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted JSON representation of the configuration</p>"},{"location":"api/ethicrawl/#ethicrawl.Config.get_snapshot","title":"<code>get_snapshot()</code>","text":"<p>Create a thread-safe deep copy of the current configuration.</p> <p>Returns:</p> Type Description <code>Config</code> <p>A deep copy of the current Config object</p>"},{"location":"api/ethicrawl/#ethicrawl.Config.reset","title":"<code>reset()</code>  <code>classmethod</code>","text":"<p>Reset the singleton instance to default values.</p> <p>Removes the existing instance from the singleton registry, causing a new instance to be created on next access.</p> Example <p>Config.reset()  # Reset to defaults config = Config()  # Get fresh instance</p>"},{"location":"api/ethicrawl/#ethicrawl.Config.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the configuration to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing all configuration sections</p>"},{"location":"api/ethicrawl/#ethicrawl.Config.update","title":"<code>update(config_dict)</code>","text":"<p>Update configuration from a dictionary.</p> <p>Updates configuration sections based on a nested dictionary structure. The dictionary should have section names as top-level keys and property-value pairs as nested dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict[str, Any]</code> <p>Dictionary with configuration settings</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If trying to set a property that doesn't exist</p> Example <p>config.update({ ...     \"http\": { ...         \"user_agent\": \"CustomBot/1.0\", ...         \"timeout\": 30 ...     }, ...     \"logger\": { ...         \"level\": \"DEBUG\" ...     } ... })</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl","title":"<code>Ethicrawl</code>","text":"<p>Main entry point for ethical web crawling operations.</p> <p>This class provides a simplified interface for crawling websites while respecting robots.txt rules, rate limits, and domain boundaries. It manages the lifecycle of crawling operations through binding to domains and provides access to robots.txt and sitemap functionality.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Config</code> <p>Configuration settings for crawling behavior</p> <code>robots</code> <code>Robot</code> <p>Handler for robots.txt rules (available after binding)</p> <code>sitemaps</code> <code>SitemapParser</code> <p>Parser for XML sitemaps (available after binding)</p> <code>logger</code> <code>Logger</code> <p>Logger instance for this ethicrawl (available after binding)</p> <code>bound</code> <code>bool</code> <p>Whether the ethicrawl is currently bound to a site</p> Example <p>from ethicrawl import Ethicrawl ethicrawl = Ethicrawl() ethicrawl.bind(\"https://example.com\") response = ethicrawl.get(\"https://example.com/about\") print(response.status_code) 200</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl--find-urls-in-sitemap","title":"Find URLs in sitemap","text":"<p>urls = ethicrawl.sitemaps.parse() ethicrawl.unbind()  # Clean up when done</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.bound","title":"<code>bound</code>  <code>property</code>","text":"<p>Check if currently bound to a site.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the ethicrawl is bound to a domain, False otherwise</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.config","title":"<code>config</code>  <code>property</code>","text":"<p>Access the configuration settings for this ethicrawl.</p> <p>Returns:</p> Name Type Description <code>Config</code> <code>Config</code> <p>The configuration object with settings for all ethicrawl components</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.logger","title":"<code>logger</code>  <code>property</code>","text":"<p>Get the logger for the current bound domain.</p> <p>This logger is configured according to the settings in Config.logger.</p> <p>Returns:</p> Name Type Description <code>Logger</code> <code>Logger</code> <p>Configured logger instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not bound to a site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.robots","title":"<code>robots</code>  <code>property</code>","text":"<p>Access the robots.txt handler for the bound domain.</p> <p>The Robot instance manages fetching, parsing, and enforcing robots.txt rules for the current domain.</p> <p>Returns:</p> Name Type Description <code>Robot</code> <code>Robot</code> <p>The robots.txt handler for this domain</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not bound to a site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.sitemaps","title":"<code>sitemaps</code>  <code>property</code>","text":"<p>Access the sitemap parser for the bound domain.</p> <p>The parser is created on first access and cached for subsequent calls. It provides methods to extract URLs from XML sitemaps.</p> <p>Returns:</p> Name Type Description <code>SitemapParser</code> <code>SitemapParser</code> <p>Parser for handling XML sitemaps</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not bound to a site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.bind","title":"<code>bind(url, client=None)</code>","text":"<p>Bind the ethicrawl to a specific website domain.</p> <p>Binding establishes the primary domain context with its robots.txt handler, client configuration, and sets up logging for operations on this domain.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | Url | Resource</code> <p>The base URL of the site to crawl (string, Url, or Resource)</p> required <code>client</code> <code>HttpClient | None</code> <p>HTTP client to use for requests. Defaults to a standard HttpClient</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if binding was successful</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL is invalid</p> <code>RuntimeError</code> <p>If already bound to a different site</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.get","title":"<code>get(url, headers=None)</code>","text":"<p>Make an HTTP GET request to the specified URL, respecting robots.txt rules and domain whitelisting.</p> <p>This method enforces ethical crawling by: - Checking that the domain is allowed (primary or whitelisted) - Verifying the URL is permitted by robots.txt rules - Using the appropriate client for the domain</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | Url | Resource</code> <p>URL to fetch (string, Url, or Resource)</p> required <code>headers</code> <code>Headers | dict | None</code> <p>Additional headers for this request</p> <code>None</code> <p>Returns:</p> Type Description <code>Response | HttpResponse</code> <p>Response or HttpResponse: The response from the server</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL is from a non-whitelisted domain or disallowed by robots.txt</p> <code>RuntimeError</code> <p>If not bound to a site</p> <code>TypeError</code> <p>If url parameter is not a string, Url, or Resource</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.unbind","title":"<code>unbind()</code>","text":"<p>Unbind the ethicrawl from its current site.</p> <p>This releases resources and allows the ethicrawl to be bound to a different site. It removes all domain contexts, cached resources, and resets the ethicrawl state.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if unbinding was successful</p>"},{"location":"api/ethicrawl/#ethicrawl.Ethicrawl.whitelist","title":"<code>whitelist(url, client=None)</code>","text":"<p>Whitelist an additional domain for crawling.</p> <p>By default, Ethicrawl will only request URLs from the bound domain. Whitelisting allows accessing resources from other domains (like CDNs).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str or Url</code> <p>URL from the domain to whitelist</p> required <code>client</code> <code>HttpClient</code> <p>Client to use for this domain</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if whitelisting was successful</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not bound to a primary site</p>"},{"location":"api/ethicrawl/#ethicrawl.HttpClient","title":"<code>HttpClient</code>","text":"<p>               Bases: <code>Client</code></p> <p>HTTP client implementation with configurable transports and rate limiting.</p> <p>This client provides a flexible HTTP interface with the following features: - Configurable backend transport (Requests or Selenium Chrome) - Built-in rate limiting with jitter to avoid detection - Header management with User-Agent control - Automatic retry with exponential backoff - Detailed logging of request/response cycles</p> <p>The client can use either a simple RequestsTransport for basic HTTP operations or a ChromeTransport for JavaScript-rendered content.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>min_interval</code> <code>float</code> <p>Minimum time between requests in seconds</p> <code>jitter</code> <code>float</code> <p>Random time variation added to rate limiting</p> <code>headers</code> <code>Headers</code> <p>Default headers to send with each request</p> <code>last_request_time</code> <code>float</code> <p>Timestamp of the last request</p> <code>user_agent</code> <code>str</code> <p>User agent string used for requests</p> Example <p>from ethicrawl.client.http import HttpClient from ethicrawl.core import Resource client = HttpClient(rate_limit=1.0)  # 1 request per second response = client.get(Resource(\"https://example.com\")) print(response.status_code) 200</p>"},{"location":"api/ethicrawl/#ethicrawl.HttpClient--switch-to-chrome-for-javascript-heavy-sites","title":"Switch to Chrome for JavaScript-heavy sites","text":"<p>chrome_client = client.with_chrome(headless=True) js_response = chrome_client.get(Resource(\"https://spa-example.com\"))</p>"},{"location":"api/ethicrawl/#ethicrawl.HttpClient.__init__","title":"<code>__init__(context=None, transport=None, timeout=10, rate_limit=1.0, jitter=0.5, headers=None, chrome_params=None)</code>","text":"<p>Initialize an HTTP client with configurable transport and rate limiting.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Context for the client. If None, a default context with a dummy URL will be created.</p> <code>None</code> <code>transport</code> <code>Transport</code> <p>Custom transport implementation. If None, either ChromeTransport or RequestsTransport will be used.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>10</code> <code>rate_limit</code> <code>float</code> <p>Maximum requests per second. Set to 0 for no limit.</p> <code>1.0</code> <code>jitter</code> <code>float</code> <p>Random variation (0-1) to add to rate limiting</p> <code>0.5</code> <code>headers</code> <code>dict</code> <p>Default headers to send with each request</p> <code>None</code> <code>chrome_params</code> <code>dict</code> <p>Parameters for ChromeTransport if used</p> <code>None</code>"},{"location":"api/ethicrawl/#ethicrawl.HttpClient.get","title":"<code>get(resource, timeout=None, headers=None)</code>","text":"<p>Make a GET request to the specified resource.</p> <p>This method applies rate limiting, handles headers, and logs the result. For JavaScript-heavy sites, use with_chrome() first to switch to a Chrome-based transport.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to request</p> required <code>timeout</code> <code>int</code> <p>Request-specific timeout that overrides the client's default timeout</p> <code>None</code> <code>headers</code> <code>dict</code> <p>Additional headers for this request</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HttpResponse</code> <code>HttpResponse</code> <p>Response object with status, headers and content</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If resource is not a Resource instance</p> <code>IOError</code> <p>If the HTTP request fails for any reason</p> Example <p>client = HttpClient() response = client.get(Resource(\"https://example.com\")) if response.status_code == 200: ...     print(f\"Got {len(response.content)} bytes\")</p>"},{"location":"api/ethicrawl/#ethicrawl.HttpClient.with_chrome","title":"<code>with_chrome(headless=True, wait_time=3, timeout=30, rate_limit=0.5, jitter=0.3)</code>","text":"<p>Create a new HttpClient instance using Chrome/Selenium transport.</p> <p>This creates a new client that can render JavaScript and interact with dynamic web applications.</p> <p>Parameters:</p> Name Type Description Default <code>headless</code> <code>bool</code> <p>Whether to run Chrome in headless mode</p> <code>True</code> <code>wait_time</code> <code>int</code> <p>Default time to wait for page elements in seconds</p> <code>3</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <code>rate_limit</code> <code>float</code> <p>Maximum requests per second</p> <code>0.5</code> <code>jitter</code> <code>float</code> <p>Random variation factor for rate limiting</p> <code>0.3</code> <p>Returns:</p> Name Type Description <code>HttpClient</code> <code>HttpClient</code> <p>A new client instance configured to use Chrome</p> Example <p>client = HttpClient() chrome = client.with_chrome(headless=True) response = chrome.get(Resource(\"https://single-page-app.com\"))</p>"},{"location":"api/ethicrawl/#ethicrawl.Resource","title":"<code>Resource</code>  <code>dataclass</code>","text":"<p>URL-identified entity within the crawler system.</p> <p>Resource is a generic representation of anything addressable by a URL within the Ethicrawl system. It serves as a common foundation for various components like requests, responses, robots.txt files, sitemap entries, etc.</p> <p>This class provides URL type safety, consistent equality comparison, and proper hashing behavior for all URL-addressable entities.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>The Url object identifying this resource. Can be initialized with either a string or Url object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When initialized with something other than a string or Url object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; resource = Resource(\"https://example.com/robots.txt\")\n&gt;&gt;&gt; resource.url.path\n'/robots.txt'\n&gt;&gt;&gt; resource2 = Resource(Url(\"https://example.com/robots.txt\"))\n&gt;&gt;&gt; resource == resource2\nTrue\n</code></pre>"},{"location":"api/ethicrawl/#ethicrawl.Resource.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare resources for equality based on their URLs.</p> <p>Two resources are considered equal if they have the same URL.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>Another Resource object to compare with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if resources have the same URL, False otherwise</p>"},{"location":"api/ethicrawl/#ethicrawl.Resource.__hash__","title":"<code>__hash__()</code>","text":"<p>Generate a hash based on the string representation of the URL.</p> <p>Returns:</p> Type Description <code>int</code> <p>Integer hash value</p>"},{"location":"api/ethicrawl/#ethicrawl.Resource.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate and normalize the url attribute after initialization.</p> <p>Converts string URLs to Url objects and raises TypeError for invalid types.</p>"},{"location":"api/ethicrawl/#ethicrawl.Resource.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a developer-friendly representation.</p>"},{"location":"api/ethicrawl/#ethicrawl.Resource.__str__","title":"<code>__str__()</code>","text":"<p>Return the URL as a string for better readability.</p>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList","title":"<code>ResourceList</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Collection of Resource objects with filtering capabilities.</p> <p>ResourceList provides list-like functionality specialized for managing collections of Resources with additional filtering methods and type safety. The class is generic and can contain any subclass of Resource.</p> Note <p>This class has no public attributes as all storage is private.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ethicrawl.core import Resource, ResourceList\n&gt;&gt;&gt; resources = ResourceList()\n&gt;&gt;&gt; resources.append(Resource(\"https://example.com/page1\"))\n&gt;&gt;&gt; resources.append(Resource(\"https://example.com/page2\"))\n&gt;&gt;&gt; len(resources)\n2\n&gt;&gt;&gt; filtered = resources.filter(r\"page1\")\n&gt;&gt;&gt; len(filtered)\n1\n</code></pre>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get items by index or slice.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | slice</code> <p>Integer index or slice object</p> required <p>Returns:</p> Type Description <code>T | ResourceList[T]</code> <p>Single Resource when indexed with integer, ResourceList when sliced</p>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList.__init__","title":"<code>__init__(items=None)</code>","text":"<p>Initialize a resource list with optional initial items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[T] | None</code> <p>Optional list of Resource objects to initialize with</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If items is not a list or contains non-Resource objects</p>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList.append","title":"<code>append(item)</code>","text":"<p>Add a resource to the list.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>T</code> <p>Resource object to add</p> required <p>Returns:</p> Type Description <code>ResourceList[T]</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If item is not a Resource object</p>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList.extend","title":"<code>extend(items)</code>","text":"<p>Add multiple resources to the list.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Iterable[T]</code> <p>Iterable of Resource objects to add</p> required <p>Returns:</p> Type Description <code>ResourceList[T]</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any item is not a Resource object</p>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList.filter","title":"<code>filter(pattern)</code>","text":"<p>Filter resources by URL pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str | Pattern</code> <p>String pattern or compiled regex Pattern to match against URLs</p> required <p>Returns:</p> Type Description <code>ResourceList[T]</code> <p>New ResourceList containing only matching resources of the same type as original</p>"},{"location":"api/ethicrawl/#ethicrawl.ResourceList.to_list","title":"<code>to_list()</code>","text":"<p>Convert to a standard Python list.</p> <p>Returns:</p> Type Description <code>list[T]</code> <p>A copy of the internal list of resources</p>"},{"location":"api/ethicrawl/#ethicrawl.Url","title":"<code>Url</code>","text":"<p>URL parser and manipulation class.</p> <p>This class provides methods for parsing, validating, and manipulating URLs. Supports HTTP, HTTPS, and file URL schemes with validation and component access. Path extension and query parameter manipulation are provided through the extend() method.</p> <p>Attributes:</p> Name Type Description <code>scheme</code> <code>str</code> <p>URL scheme (http, https, file)</p> <code>netloc</code> <code>str</code> <p>Network location/domain (HTTP/HTTPS only)</p> <code>hostname</code> <code>str</code> <p>Just the hostname portion of netloc (HTTP/HTTPS only)</p> <code>path</code> <code>str</code> <p>URL path component</p> <code>params</code> <code>str</code> <p>URL parameters (HTTP/HTTPS only)</p> <code>query</code> <code>str</code> <p>Raw query string (HTTP/HTTPS only)</p> <code>query_params</code> <code>dict[str, Any]</code> <p>Query string parsed into a dictionary (HTTP/HTTPS only)</p> <code>fragment</code> <code>str</code> <p>URL fragment identifier (HTTP/HTTPS only)</p> <code>base</code> <code>str</code> <p>Base URL (scheme + netloc)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When provided with invalid URLs or when performing invalid operations</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.base","title":"<code>base</code>  <code>property</code>","text":"<p>Get the base URL (scheme and netloc).</p> <p>Returns:</p> Type Description <code>str</code> <p>The base URL as a string (e.g., 'https://example.com')</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.fragment","title":"<code>fragment</code>  <code>property</code>","text":"<p>Get the fragment identifier from the URL.</p> <p>The fragment appears after # in a URL and typically references a section within a document.</p> <p>Returns:</p> Type Description <code>str</code> <p>Fragment string without the # character</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called on a non-HTTP(S) URL</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.hostname","title":"<code>hostname</code>  <code>property</code>","text":"<p>Get just the hostname part.</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.netloc","title":"<code>netloc</code>  <code>property</code>","text":"<p>Get the network location (domain).</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.params","title":"<code>params</code>  <code>property</code>","text":"<p>Get URL parameters.</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.path","title":"<code>path</code>  <code>property</code>","text":"<p>Get the path component.</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.query","title":"<code>query</code>  <code>property</code>","text":"<p>Get the query string.</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.query_params","title":"<code>query_params</code>  <code>property</code>","text":"<p>Get query parameters as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of query parameter keys and values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called on a non-HTTP(S) URL</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.scheme","title":"<code>scheme</code>  <code>property</code>","text":"<p>Get the URL scheme (file, http or https).</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare URLs for equality.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>Another Url object or string to compare with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URLs are equal, False otherwise</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hash of the URL.</p> <p>The hash is based on the string representation of the URL, ensuring URLs that are equal have the same hash.</p> <p>Returns:</p> Type Description <code>int</code> <p>Integer hash value</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.__init__","title":"<code>__init__(url, validate=False)</code>","text":"<p>Initialize a URL object with parsing and optional validation.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Union[str, Url]</code> <p>String or Url object to parse</p> required <code>validate</code> <code>bool</code> <p>If True, performs additional validation including DNS resolution</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the URL has an invalid scheme or missing required components</p> <code>ValueError</code> <p>When validate=True and the hostname cannot be resolved</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.__str__","title":"<code>__str__()</code>","text":"<p>Convert URL to string representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>Complete URL string</p>"},{"location":"api/ethicrawl/#ethicrawl.Url.extend","title":"<code>extend(*args)</code>","text":"<p>Extend the URL with additional path components or query parameters.</p> <p>This method supports multiple extension patterns: 1. Path extension: extend(\"path/component\") 2. Single parameter: extend(\"param_name\", \"param_value\") 3. Multiple parameters: extend({\"param1\": \"value1\", \"param2\": \"value2\"})</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Either a path string, a parameter dict, or name/value parameter pair</p> <code>()</code> <p>Returns:</p> Type Description <code>Url</code> <p>A new Url object with the extended path or parameters</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If arguments don't match one of the supported patterns</p> <code>ValueError</code> <p>If trying to add query parameters to a file:// URL</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; url = Url(\"https://example.com/api\")\n&gt;&gt;&gt; url.extend(\"v1\").extend({\"format\": \"json\"})\nUrl(\"https://example.com/api/v1?format=json\")\n</code></pre>"}]}