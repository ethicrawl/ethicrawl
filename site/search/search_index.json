{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ethicrawl Documentation","text":"<p>Welcome to Ethicrawl, a Python library designed for ethical and respectful web crawling. Ethicrawl provides tools and abstractions to help developers build crawlers that follow best practices, respect site policies, and minimize server impact.</p>"},{"location":"#overview","title":"Overview","text":"<p>Ethicrawl focuses on:</p> <ul> <li>Ethical Crawling: Respectful of robots.txt, rate limits, and server resources</li> <li>Resource Abstractions: Type-safe handling of web resources and collections</li> <li>Policy Enforcement: Automatic compliance with web standards</li> <li>Comprehensive Logging: Detailed visibility into crawler operations</li> <li>Configurability: Fine-grained control over crawler behavior</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ethicrawl\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from ethicrawl import Ethicrawl\nfrom ethicrawl.error import RobotDisallowedError\n\n# Create and bind to a domain\nethicrawl = Ethicrawl()\nethicrawl.bind(\"https://example.com\")\n\n# Get a page - robots.txt rules automatically respected\ntry:\n    response = ethicrawl.get(\"/page.html\")\nexcept RobotDisallowedError:\n    print(\"The site prohibits fetching the page\")\n\n# Release resources when done\nethicrawl.unbind()\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>Our examples directory contains practical demonstrations of Ethicrawl's features:</p> <ol> <li>Basic Usage: Simple crawling with automatic robots.txt compliance</li> <li>Domain Whitelisting: Allowing access to secondary domains</li> <li>Chrome Client: Using Chrome browser for JavaScript rendering</li> <li>Sitemap Support: Parsing and using sitemaps</li> <li>Resources and ResourceLists: Working with web resources</li> <li>Proxies and Caching: Using proxy servers</li> <li>Configuration: Configuring Ethicrawl</li> <li>Logging: Using the logging system</li> </ol>"},{"location":"#documentation-guides","title":"Documentation Guides","text":"<ul> <li>Standards: Ethical crawling standards enforced by Ethicrawl</li> <li>Logging: Detailed guide to Ethicrawl's logging system</li> <li>Documentation: Guidelines for contributing to these docs</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>The API Reference provides comprehensive documentation of all Ethicrawl's classes, methods, and functions:</p> <ul> <li>Core: Base classes like Resource, ResourceList, and Url</li> <li>Client: HTTP client implementation</li> <li>Config: Configuration system</li> <li>Robots: Robots.txt parsing and enforcement</li> <li>Sitemaps: Sitemap processing</li> <li>Logger: Logging system</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>ethicrawl/\n\u251c\u2500\u2500 core/         # Core abstractions (Resource, Url, etc.)\n\u251c\u2500\u2500 client/       # HTTP client implementation\n\u251c\u2500\u2500 config/       # Configuration system\n\u251c\u2500\u2500 robots/       # Robots.txt parser and enforcer\n\u251c\u2500\u2500 sitemaps/     # Sitemap parser\n\u2514\u2500\u2500 logger/       # Logging system\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Ethicrawl welcomes contributions! Please see our Contribution Guidelines for more information on how to get involved.</p>"},{"location":"documentation/","title":"Documentation","text":"<p>We follow Google-style docstrings for all code documentation.</p>"},{"location":"documentation/#documentation-focus-areas","title":"Documentation Focus Areas","text":"<ul> <li>All public APIs (methods, classes, and modules) must have comprehensive docstrings</li> <li>Private methods with complex logic should have docstrings</li> <li>Simple private methods or properties may omit docstrings</li> <li>Focus on documentation that enhances IDE tooltips and developer experience</li> </ul>"},{"location":"documentation/#docstring-format","title":"Docstring Format","text":""},{"location":"documentation/#module-docstrings","title":"Module Docstrings","text":"<pre><code>\"\"\"Module for handling robots.txt parsing and permission checking.\n\nThis module provides functionality for fetching, parsing and checking\npermissions against robots.txt files according to the Robots Exclusion\nProtocol.\n\"\"\"\n</code></pre>"},{"location":"documentation/#class-docstrings","title":"Class Docstrings","text":"<pre><code>class Robot:\n    \"\"\"Representation of a robots.txt file with permission checking.\n\n    This class handles fetching and parsing robots.txt files and provides\n    methods to check if URLs can be accessed according to the rules.\n\n    Attributes:\n        url: The URL of the robots.txt file\n        sitemaps: List of sitemap URLs found in robots.txt\n    \"\"\"\n</code></pre>"},{"location":"documentation/#methodfunction-docstrings","title":"Method/Function Docstrings","text":"<pre><code>def can_fetch(self, url: str, user_agent: str = None) -&gt; bool:\n    \"\"\"Check if a URL can be fetched according to robots.txt rules.\n\n    Args:\n        url: The URL to check against robots.txt rules\n        user_agent: Optional user agent string to use for checking.\n            Defaults to the client's configured user agent.\n\n    Returns:\n        True if the URL is allowed, False otherwise.\n\n    Raises:\n        RobotDisallowedError: If access is denied and raise_on_disallow=True\n        ValueError: If the URL is malformed\n\n    Example:\n        &gt;&gt;&gt; robot = Robot(\"https://example.com/robots.txt\")\n        &gt;&gt;&gt; robot.can_fetch(\"https://example.com/allowed\")\n        True\n        &gt;&gt;&gt; robot.can_fetch(\"https://example.com/disallowed\")\n        False\n</code></pre>"},{"location":"documentation/#property-docstrings","title":"Property Docstrings","text":"<pre><code>@property\ndef sitemaps(self) -&gt; List[str]:\n    \"\"\"List of sitemap URLs found in robots.txt.\n\n    Returns:\n        List of sitemap URLs as strings.\n    \"\"\"\n</code></pre>"},{"location":"documentation/#constructor-docstrings","title":"Constructor Docstrings","text":"<pre><code>def __init__(self, url: str, context: Context = None):\n    \"\"\"Initialize a Robot instance.\n\n    Args:\n        url: URL to the robots.txt file\n        context: Optional context for the request.\n            If not provided, a default context will be created.\n    \"\"\"\n</code></pre>"},{"location":"documentation/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>All public methods, classes, and modules must have docstrings (100% coverage)</li> <li>Private methods with complex logic should have docstrings</li> <li>Simple private methods or properties may omit docstrings</li> <li>The overall project requires a minimum of 95% docstring coverage as measured by interrogate</li> </ul>"},{"location":"documentation/#special-cases","title":"Special Cases","text":""},{"location":"documentation/#private-methods","title":"Private Methods","text":"<p>Private methods (starting with underscore) should have docstrings if they: - Contain complex logic - Are called from multiple places - Would benefit from documentation for maintainers</p>"},{"location":"documentation/#one-line-docstrings","title":"One-line Docstrings","text":"<p>For simple methods with obvious behavior, a one-line docstring is acceptable:</p> <pre><code>def is_allowed(self, url: str) -&gt; bool:\n    \"\"\"Return True if the URL is allowed by robots.txt.\"\"\"\n</code></pre>"},{"location":"documentation/#verification-approach","title":"Verification Approach","text":"<p>Documentation quality is verified through code review rather than automated metrics. Reviewers should confirm that:</p> <ul> <li>Public APIs have clear, complete docstrings</li> <li>Examples are provided for non-obvious usage</li> <li>Type information is present in docstrings</li> <li>Error cases and exceptions are documented</li> </ul>"},{"location":"documentation/#documentation-language-guidelines","title":"Documentation Language Guidelines","text":"<p>When writing documentation, follow these language principles:</p> <ol> <li>Be objective - Avoid subjective descriptors like \"elegant\", \"beautiful\", or \"clever\"</li> <li>Be precise - Focus on what code does, not subjective quality judgments</li> <li>Be technical - Use concrete technical terms rather than metaphorical language</li> <li>Be consistent - Maintain a neutral, professional tone throughout documentation</li> </ol>"},{"location":"documentation/#examples","title":"Examples:","text":"<p>Avoid: <pre><code>\"\"\"This elegant pattern enables seamless chaining of operations.\"\"\"\n</code></pre></p> <p>Better: <pre><code>\"\"\"This design allows operation outputs to serve as inputs to other operations.\"\"\"\n</code></pre></p> <p>Avoid <pre><code>\"\"\"Beautiful integration between components creates a powerful system.\"\"\"\n</code></pre></p> <p>Better <code>Components communicate through well-defined interfaces that enable extensibility.</code></p>"},{"location":"logging/","title":"Logging","text":""},{"location":"logging/#logging-standards-for-ethicrawl","title":"Logging Standards for Ethicrawl","text":"<p>Good logging practices are essential for troubleshooting, monitoring, and understanding application behavior. This document outlines our logging standards for the Ethicrawl codebase.</p>"},{"location":"logging/#log-levels-and-their-uses","title":"Log Levels and Their Uses","text":""},{"location":"logging/#critical-loggingcritical-50","title":"CRITICAL (logging.CRITICAL, 50)","text":"<p>Use for severe errors that prevent core functionality from working.</p>"},{"location":"logging/#when-to-use","title":"When to use:","text":"<ul> <li>Application cannot continue functioning</li> <li>Data corruption or loss has occurred</li> <li>Security breaches or compromises</li> <li>Resource exhaustion that threatens system stability</li> </ul>"},{"location":"logging/#examples","title":"Examples:","text":"<pre><code>logger.critical(f\"Failed to initialize client: {error}\")\nlogger.critical(f\"Persistent storage corruption detected in {file_path}\")\n</code></pre>"},{"location":"logging/#error-loggingerror-40","title":"ERROR (logging.ERROR, 40)","text":"<p>Use for runtime errors that prevent specific operations from completing but don't crash the application.</p>"},{"location":"logging/#when-to-use_1","title":"When to use:","text":"<ul> <li>Failed HTTP requests</li> <li>Failed data processing operations</li> <li>Configuration errors</li> <li>External service unavailability</li> <li>Unexpected exceptions in non-critical paths</li> </ul>"},{"location":"logging/#examples_1","title":"Examples:","text":"<pre><code>logger.error(f\"HTTP request failed: {status_code} {reason}\")\nlogger.error(f\"Failed to parse sitemap at {url}: {error_message}\")\n</code></pre>"},{"location":"logging/#warning-loggingwarning-30","title":"WARNING (logging.WARNING, 30)","text":"<p>Use for conditions that might cause problems but allow operations to continue.</p>"},{"location":"logging/#when-to-use_2","title":"When to use:","text":"<ul> <li>Deprecated feature usage</li> <li>Slow response times</li> <li>Retrying operations after recoverable failures</li> <li>Access denied for certain operations</li> <li>Unexpected data formats that can be handled</li> <li>Rate limiting being applied</li> </ul>"},{"location":"logging/#examples_2","title":"Examples:","text":"<pre><code>logger.warning(f\"URL disallowed by robots.txt: {url}\")\nlogger.warning(f\"Slow response from {domain}: {response_time}s\")\nlogger.warning(f\"Retrying request ({retry_count}/{max_retries})\")\n</code></pre>"},{"location":"logging/#info-logginginfo-20","title":"INFO (logging.INFO, 20)","text":"<p>Use for normal operational events and milestones.</p>"},{"location":"logging/#when-to-use_3","title":"When to use:","text":"<ul> <li>Application startup and shutdown</li> <li>Configuration settings</li> <li>Successful site binding and crawling</li> <li>Processing milestones</li> <li>Summary information about operations</li> <li>Changes to application state</li> </ul>"},{"location":"logging/#examples_3","title":"Examples:","text":"<pre><code>logger.info(f\"Bound to site: {url}\")\nlogger.info(f\"Robots.txt processed: {allowed_count} allowed paths, {disallowed_count} disallowed\")\nlogger.info(f\"Processed {page_count} pages in {duration}s\")\n</code></pre>"},{"location":"logging/#debug-loggingdebug-10","title":"DEBUG (logging.DEBUG, 10)","text":"<p>Use for detailed information useful during development and debugging.</p>"},{"location":"logging/#when-to-use_4","title":"When to use:","text":"<ul> <li>Function entry/exit points</li> <li>Variable values and state changes</li> <li>Decision logic paths</li> <li>Low-level HTTP details</li> <li>Parsing steps</li> <li>Rate limiting details</li> </ul>"},{"location":"logging/#examples_4","title":"Examples:","text":"<pre><code>logger.debug(f\"Processing URL: {url}\")\nlogger.debug(f\"Page found in cache, age: {cache_age}s\")\nlogger.debug(f\"Parser state: {current_state}\")\n</code></pre>"},{"location":"logging/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li>Be Concise and Specific</li> <li>Include exactly what happened and where</li> <li> <p>Use active voice (e.g., \"Failed to connect\" instead of \"Connection failure occurred\")</p> </li> <li> <p>Include Context</p> </li> <li>Always include relevant identifiers (URLs, IDs, component names)</li> <li>Include relevant variable values</li> <li> <p>For errors, include exception messages and/or stack traces</p> </li> <li> <p>Be Consistent</p> </li> <li>Use consistent terminology across similar log messages</li> <li>Use consistent formatting for similar events</li> <li> <p>Use sentence case for log messages (capitalize first word)</p> </li> <li> <p>Avoid Sensitive Information</p> </li> <li>No authentication credentials</li> <li>No personal data</li> <li> <p>No sensitive headers or tokens</p> </li> <li> <p>Use Structured Fields for Machine Parsing</p> </li> <li>Place structured data at the end of the message</li> <li>Use consistent key-value format: <code>key=value</code></li> </ol>"},{"location":"logging/#component-specific-guidelines","title":"Component-Specific Guidelines","text":"<p>Each component should have a consistent logging identity:</p> <ol> <li>Robot/Robots.txt</li> <li>INFO: Robots.txt fetching and parsing results</li> <li>WARNING: Disallowed access attempts</li> <li> <p>ERROR: Failed to fetch/parse robots.txt</p> </li> <li> <p>HTTP Client</p> </li> <li>DEBUG: Request details</li> <li>INFO: Rate limiting information</li> <li>WARNING: Retries and slow responses</li> <li> <p>ERROR: Failed requests</p> </li> <li> <p>Sitemap</p> </li> <li>INFO: Sitemap discovery and parsing</li> <li>WARNING: Malformed but recoverable sitemap content</li> <li>ERROR: Failed sitemap fetching/parsing</li> </ol> <p>By following these guidelines, we'll maintain a consistent and helpful logging strategy across the Ethicrawl codebase.</p>"},{"location":"standards/","title":"Coding Standards","text":""},{"location":"standards/#python-code-standards","title":"Python Code Standards","text":""},{"location":"standards/#general-requirements","title":"General Requirements","text":"<ul> <li>Python Version: We target Python 3.10 and above</li> <li>Code may work on earlier versions but is not tested or supported</li> <li>Line Endings: Use UNIX-style line endings (LF, <code>\\n</code>)</li> <li>Indentation: Use 4 spaces for indentation (no tabs)</li> <li>Maximum Line Length: 88 characters (Black default)</li> </ul>"},{"location":"standards/#code-formatting","title":"Code Formatting","text":"<p>We use Black as our code formatter with default settings:</p> <pre><code># Install Black\npip install black\n\n# Format a single file\nblack path/to/file.py\n\n# Format all Python files in a directory\nblack directory_name/\n</code></pre>"},{"location":"standards/#import-organization","title":"Import Organization","text":"<p>Imports should be grouped in the following order, with a blank line between each group:</p> <ul> <li>Standard library imports</li> <li>Third-party imports</li> <li>Local application imports</li> </ul> <pre><code># Standard library\nimport os\nfrom datetime import datetime\n\n# Third-party\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Local\nfrom ethicrawl.core import Resource\nfrom ethicrawl.config import Config\n</code></pre>"},{"location":"standards/#type-hints","title":"Type Hints","text":"<p>Use type hints for all public methods and functions:</p> <pre><code>def process_url(url: str, timeout: Optional[int] = None) -&gt; Dict[str, Any]:\n    \"\"\"Process the given URL and return results.\"\"\"\n</code></pre>"},{"location":"standards/#methodfunction-docstrings","title":"Method/Function Docstrings","text":"<pre><code>def can_fetch(self, url: str, user_agent: str = None) -&gt; bool:\n    \"\"\"Check if a URL can be fetched according to robots.txt rules.\n\n    Args:\n        url: The URL to check against robots.txt rules\n        user_agent: Optional user agent string to use for checking.\n            Defaults to the client's configured user agent.\n\n    Returns:\n        True if the URL is allowed, False otherwise.\n\n    Raises:\n        RobotDisallowedError: If access is denied and raise_on_disallow=True\n        ValueError: If the URL is malformed\n    \"\"\"\n</code></pre>"},{"location":"standards/#error-handling-style-guide-for-ethicrawl","title":"Error Handling Style Guide for Ethicrawl","text":"<p>A consistent approach to error handling improves code readability and helps developers understand and handle errors effectively. This document outlines our standards for raising exceptions in the Ethicrawl codebase.</p>"},{"location":"standards/#general-principles","title":"General Principles","text":"<ol> <li>Be specific - Use the most specific exception type appropriate for the error</li> <li>Be descriptive - Error messages should help users identify and fix problems</li> <li>Be consistent - Follow the same patterns throughout the codebase</li> </ol>"},{"location":"standards/#standard-exception-types","title":"Standard Exception Types","text":""},{"location":"standards/#typeerror","title":"TypeError","text":"<p>Use for incorrect argument types or invalid operations on types.</p>"},{"location":"standards/#format","title":"Format:","text":"<pre><code>raise TypeError(f\"Expected {expected_type}, got {type(actual).__name__}\")\n</code></pre>"},{"location":"standards/#examples","title":"Examples:","text":"<pre><code>raise TypeError(f\"Expected string, Url, or Resource, got {type(resource).__name__}\")\nraise TypeError(\"headers must be a Headers instance or dictionary\")\n</code></pre>"},{"location":"standards/#valueerror","title":"ValueError","text":"<p>Use for arguments that have the right type but an invalid value.</p>"},{"location":"standards/#format_1","title":"Format:","text":"<pre><code>raise ValueError(f\"{parameter_name} must be {constraint}\")\n</code></pre>"},{"location":"standards/#examples_1","title":"Examples:","text":"<pre><code>raise ValueError(\"jitter must be between 0.0 and 1.0\")\nraise ValueError(\"max_retries cannot be negative\")\n</code></pre>"},{"location":"standards/#domain-specific-exceptions","title":"Domain-Specific Exceptions","text":"<p>Create custom exceptions for domain-specific errors. All custom exceptions should:</p> <ol> <li>Inherit from appropriate base exceptions</li> <li>Include \"Error\" in the class name</li> <li>Be placed in a relevant _error.py module</li> </ol>"},{"location":"standards/#example","title":"Example:","text":"<pre><code>class RobotDisallowedError(ValueError):\n    \"\"\"Raised when access to a URL is disallowed by robots.txt rules.\"\"\"\n</code></pre>"},{"location":"standards/#error-message-guidelines","title":"Error Message Guidelines","text":"<ol> <li>Be specific about what went wrong</li> <li>Provide the invalid value when helpful</li> <li>Suggest a fix when possible</li> <li>Use consistent terminology across similar errors</li> </ol>"},{"location":"standards/#documenting-exceptions","title":"Documenting Exceptions","text":"<pre><code>def function_name():\n    \"\"\"Function description.\n\n    Args:\n        param_name: Parameter description.\n\n    Returns:\n        Return description.\n\n    Raises:\n        ExceptionType: Condition when raised.\n        AnotherException: Another condition.\n    \"\"\"\n</code></pre> <p>Always document exceptions in docstrings:</p>"},{"location":"standards/#error-assertions","title":"Error Assertions","text":"<p>For internal logic verification, use assertions with descriptive messages:</p> <pre><code>assert isinstance(item, Resource), f\"Expected Resource, got {type(item).__name__}\"\n</code></pre> <p>By following these guidelines, we'll maintain a consistent approach to error handling across the Ethicrawl codebase.</p>"},{"location":"api/sitemaps/","title":"Sitemaps","text":"<p>XML sitemap parsing and traversal for discovering website structure.</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexEntry","title":"<code>IndexEntry</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SitemapEntry</code></p> <p>Represents an entry in a sitemap index file.</p> <p>IndexEntry specializes SitemapEntry for use in sitemap index files. Sitemap indexes are XML files that contain references to other sitemap files, allowing websites to organize their sitemaps hierarchically.</p> <p>IndexEntry maintains the same attributes as SitemapEntry (url and lastmod) but provides specialized string representation appropriate for index entries.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the sitemap file (inherited from Resource)</p> <code>lastmod</code> <code>str | None</code> <p>Last modification date of the sitemap (inherited from SitemapEntry)</p> Example <p>from ethicrawl.core import Url from ethicrawl.sitemaps import IndexEntry index = IndexEntry( ...     Url(\"https://example.com/sitemap-products.xml\"), ...     lastmod=\"2023-06-15T14:30:00Z\" ... ) str(index) 'https://example.com/sitemap-products.xml (last modified: 2023-06-15T14:30:00Z)' repr(index) \"SitemapIndexEntry(url='https://example.com/sitemap-products.xml', lastmod='2023-06-15T14:30:00Z')\"</p> Source code in <code>ethicrawl/sitemaps/index_entry.py</code> <pre><code>@dataclass\nclass IndexEntry(SitemapEntry):\n    \"\"\"Represents an entry in a sitemap index file.\n\n    IndexEntry specializes SitemapEntry for use in sitemap index files.\n    Sitemap indexes are XML files that contain references to other sitemap\n    files, allowing websites to organize their sitemaps hierarchically.\n\n    IndexEntry maintains the same attributes as SitemapEntry (url and lastmod)\n    but provides specialized string representation appropriate for index entries.\n\n    Attributes:\n        url: URL of the sitemap file (inherited from Resource)\n        lastmod: Last modification date of the sitemap (inherited from SitemapEntry)\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.core import Url\n        &gt;&gt;&gt; from ethicrawl.sitemaps import IndexEntry\n        &gt;&gt;&gt; index = IndexEntry(\n        ...     Url(\"https://example.com/sitemap-products.xml\"),\n        ...     lastmod=\"2023-06-15T14:30:00Z\"\n        ... )\n        &gt;&gt;&gt; str(index)\n        'https://example.com/sitemap-products.xml (last modified: 2023-06-15T14:30:00Z)'\n        &gt;&gt;&gt; repr(index)\n        \"SitemapIndexEntry(url='https://example.com/sitemap-products.xml', lastmod='2023-06-15T14:30:00Z')\"\n    \"\"\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation for debugging.\n\n        Returns:\n            String representation showing class name and field values\n        \"\"\"\n        return f\"SitemapIndexEntry(url='{str(self.url)}', lastmod={repr(self.lastmod)})\"\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexDocument","title":"<code>IndexDocument</code>","text":"<p>               Bases: <code>SitemapDocument</code></p> <p>Specialized parser for sitemap index documents.</p> <p>This class extends the SitemapDocument class to handle sitemap indexes, which are XML documents containing references to other sitemap files. It validates that the document is a proper sitemap index and extracts all sitemap references as IndexEntry objects.</p> <p>IndexDocument enforces type safety for its entries collection, ensuring that only IndexEntry objects can be added.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>ResourceList</code> <p>ResourceList of IndexEntry objects representing sitemap references</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.sitemaps import IndexDocument context = Context(Resource(\"https://example.com\")) sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; ...  ...    ...     https://example.com/sitemap1.xml ...     2023-06-15T14:30:00Z ...    ... ''' index = IndexDocument(context, sitemap_xml) len(index.entries) 1 str(index.entries[0]) 'https://example.com/sitemap1.xml (last modified: 2023-06-15T14:30:00Z)'</p> Source code in <code>ethicrawl/sitemaps/index_document.py</code> <pre><code>class IndexDocument(SitemapDocument):\n    \"\"\"Specialized parser for sitemap index documents.\n\n    This class extends the SitemapDocument class to handle sitemap indexes,\n    which are XML documents containing references to other sitemap files.\n    It validates that the document is a proper sitemap index and extracts\n    all sitemap references as IndexEntry objects.\n\n    IndexDocument enforces type safety for its entries collection, ensuring\n    that only IndexEntry objects can be added.\n\n    Attributes:\n        entries: ResourceList of IndexEntry objects representing sitemap references\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.context import Context\n        &gt;&gt;&gt; from ethicrawl.core import Resource\n        &gt;&gt;&gt; from ethicrawl.sitemaps import IndexDocument\n        &gt;&gt;&gt; context = Context(Resource(\"https://example.com\"))\n        &gt;&gt;&gt; sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n        ... &lt;sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;\n        ...   &lt;sitemap&gt;\n        ...     &lt;loc&gt;https://example.com/sitemap1.xml&lt;/loc&gt;\n        ...     &lt;lastmod&gt;2023-06-15T14:30:00Z&lt;/lastmod&gt;\n        ...   &lt;/sitemap&gt;\n        ... &lt;/sitemapindex&gt;'''\n        &gt;&gt;&gt; index = IndexDocument(context, sitemap_xml)\n        &gt;&gt;&gt; len(index.entries)\n        1\n        &gt;&gt;&gt; str(index.entries[0])\n        'https://example.com/sitemap1.xml (last modified: 2023-06-15T14:30:00Z)'\n    \"\"\"\n\n    def __init__(self, context: Context, document: str | None = None) -&gt; None:\n        \"\"\"Initialize a sitemap index document parser.\n\n        Args:\n            context: Context for logging and resource resolution\n            document: Optional XML sitemap index content to parse\n\n        Raises:\n            ValueError: If the document is not a valid sitemap index\n            SitemapError: If the document cannot be parsed\n        \"\"\"\n        super().__init__(context, document)\n        self._logger.debug(\"Creating IndexDocument instance\")\n\n        if document is not None:\n            _localname = etree.QName(self._root.tag).localname\n            if _localname != SITEMAPINDEX:\n                raise ValueError(f\"Expected a root {SITEMAPINDEX} got {_localname}\")\n            self._entries = self._parse_index_sitemap(document)\n            self._logger.debug(\n                \"Parsed sitemap index with %d entries\", len(self._entries)\n            )\n\n    def _parse_index_sitemap(self, document) -&gt; ResourceList:\n        \"\"\"Parse sitemap references from a sitemap index.\n\n        Extracts all &lt;sitemap&gt; elements and their &lt;loc&gt; and &lt;lastmod&gt;\n        children, creating IndexEntry objects for each valid reference.\n\n        Args:\n            document: XML document string to parse\n\n        Returns:\n            ResourceList containing IndexEntry objects for each sitemap reference\n        \"\"\"\n        sitemaps: ResourceList = ResourceList()\n\n        nsmap = {\"\": self.SITEMAP_NS}\n        _root = etree.fromstring(document.encode(\"utf-8\"), parser=self._parser)\n\n        # Find all sitemap elements\n        sitemap_elements = _root.findall(\".//sitemap\", namespaces=nsmap)\n        self._logger.debug(\n            \"Found %d sitemap references in index\", len(sitemap_elements)\n        )\n\n        for sitemap_elem in sitemap_elements:\n            try:\n                # Get the required loc element\n                loc_elem = sitemap_elem.find(\"loc\", namespaces=nsmap)\n                if loc_elem is None or not loc_elem.text:\n                    continue\n\n                # Get optional lastmod element\n                lastmod_elem = sitemap_elem.find(\"lastmod\", namespaces=nsmap)\n\n                # Create IndexEntry object (only loc and lastmod)\n                index = IndexEntry(\n                    url=Url(loc_elem.text),\n                    lastmod=lastmod_elem.text if lastmod_elem is not None else None,\n                )\n\n                sitemaps.append(index)\n            except ValueError as exc:  # pragma: no cover\n                self._logger.warning(\"Error parsing sitemap reference: %s\", exc)\n        return sitemaps\n\n    @property\n    def entries(self) -&gt; ResourceList:\n        \"\"\"Get the sitemaps in this index.\n\n        Returns:\n            ResourceList of IndexEntry objects representing sitemap references\n        \"\"\"\n        return self._entries\n\n    @entries.setter\n    def entries(self, entries: ResourceList) -&gt; None:\n        \"\"\"Set the sitemaps in this index.\n\n        Args:\n            entries: List of sitemap references as IndexEntry objects\n\n        Raises:\n            TypeError: If entries is not a ResourceList or contains non-IndexEntry objects\n        \"\"\"\n        if not isinstance(entries, ResourceList):\n            raise TypeError(f\"Expected a ResourceList, got {type(entries).__name__}\")\n\n        # Validate all items are of correct type\n        for entry in entries:\n            if not isinstance(entry, IndexEntry):\n                raise TypeError(f\"Expected IndexEntry, got {type(entry).__name__}\")\n\n        self._logger.debug(\"Setting %d entries in sitemap index\", len(entries))\n        self._entries = entries\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexDocument.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Get the type of sitemap document.</p> <p>Determines the type based on the root element's local name.</p> <p>Returns:</p> Type Description <code>str</code> <p>String indicating the sitemap type:</p> <code>str</code> <ul> <li>'sitemapindex': A sitemap index containing references to other sitemaps</li> </ul> <code>str</code> <ul> <li>'urlset': A sitemap containing page URLs</li> </ul> <code>str</code> <ul> <li>'unsupported': Any other type of document</li> </ul> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If no document has been loaded</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.IndexDocument.entries","title":"<code>entries: ResourceList</code>  <code>property</code> <code>writable</code>","text":"<p>Get the sitemaps in this index.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList of IndexEntry objects representing sitemap references</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapEntry","title":"<code>SitemapEntry</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Base class for entries found in XML sitemaps.</p> <p>SitemapEntry extends Resource to represent entries from XML sitemaps with their additional metadata. It maintains the URL identity pattern while adding sitemap-specific attributes like the last modification date.</p> <p>This class handles validation of W3C datetime formats and provides appropriate string representation of sitemap entries.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the sitemap entry (inherited from Resource)</p> <code>lastmod</code> <code>str | None</code> <p>Last modification date string in W3C format (optional)</p> Example <p>from ethicrawl.core import Url from ethicrawl.sitemaps import SitemapEntry entry = SitemapEntry( ...     Url(\"https://example.com/page1\"), ...     lastmod=\"2023-06-15T14:30:00Z\" ... ) str(entry) 'https://example.com/page1 (last modified: 2023-06-15T14:30:00Z)'</p> Source code in <code>ethicrawl/sitemaps/sitemap_entry.py</code> <pre><code>@dataclass\nclass SitemapEntry(Resource):\n    \"\"\"Base class for entries found in XML sitemaps.\n\n    SitemapEntry extends Resource to represent entries from XML sitemaps with\n    their additional metadata. It maintains the URL identity pattern while\n    adding sitemap-specific attributes like the last modification date.\n\n    This class handles validation of W3C datetime formats and provides\n    appropriate string representation of sitemap entries.\n\n    Attributes:\n        url: URL of the sitemap entry (inherited from Resource)\n        lastmod: Last modification date string in W3C format (optional)\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.core import Url\n        &gt;&gt;&gt; from ethicrawl.sitemaps import SitemapEntry\n        &gt;&gt;&gt; entry = SitemapEntry(\n        ...     Url(\"https://example.com/page1\"),\n        ...     lastmod=\"2023-06-15T14:30:00Z\"\n        ... )\n        &gt;&gt;&gt; str(entry)\n        'https://example.com/page1 (last modified: 2023-06-15T14:30:00Z)'\n    \"\"\"\n\n    lastmod: str | None = None\n\n    @staticmethod\n    def _validate_lastmod(value: str | None) -&gt; str | None:\n        \"\"\"\n        Validate lastmod date format using standard datetime.\n\n        Args:\n            value: Date string in W3C format\n\n        Returns:\n            str: Validated date string\n\n        Raises:\n            ValueError: If date format is invalid\n        \"\"\"\n        if not value:\n            return None\n\n        if not isinstance(value, str):\n            raise TypeError(f\"expected lastmod to be str, got {type(value).__name__}\")\n\n        # Strip whitespace\n        value = value.strip()\n\n        # Try standard formats for W3C datetime\n        formats = [\n            \"%Y-%m-%d\",  # YYYY-MM-DD\n            \"%Y-%m-%dT%H:%M:%S\",  # YYYY-MM-DDThh:mm:ss\n            \"%Y-%m-%dT%H:%M:%SZ\",  # YYYY-MM-DDThh:mm:ssZ\n            \"%Y-%m-%dT%H:%M:%S%z\",  # YYYY-MM-DDThh:mm:ss+hh:mm (no colon)\n            \"%Y-%m-%dT%H:%M:%S%:z\",  # YYYY-MM-DDThh:mm:ss+hh:mm (with colon)\n            \"%Y-%m-%dT%H:%M:%S.%fZ\",  # YYYY-MM-DDThh:mm:ss.ssssssZ (with microseconds)\n            \"%Y-%m-%dT%H:%M:%S.%f\",  # YYYY-MM-DDThh:mm:ss.ssssss (with microseconds, no Z)\n        ]\n\n        # Try each format\n        for fmt in formats:\n            try:\n                # If parse succeeds, the date is valid\n                datetime.strptime(value, fmt)\n                return value\n            except ValueError:\n                continue\n\n        raise ValueError(f\"Invalid lastmod date format: {value}\")\n\n    def __post_init__(self):\n        \"\"\"Validate fields after initialization.\n\n        Validates the lastmod date format if provided, ensuring it\n        conforms to one of the accepted W3C datetime formats.\n\n        Raises:\n            ValueError: If lastmod format is invalid\n            TypeError: If lastmod is not a string\n        \"\"\"\n        super().__post_init__()  # Call Resource.__post_init__ first\n        self.lastmod = self._validate_lastmod(self.lastmod)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation of the sitemap entry.\n\n        Returns:\n            URL string with last modification date if available\n        \"\"\"\n        if self.lastmod:\n            return f\"{str(self.url)} (last modified: {self.lastmod})\"\n        return f\"{str(self.url)}\"\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument","title":"<code>SitemapDocument</code>","text":"<p>Parser and representation of XML sitemap documents.</p> <p>This class handles the parsing, validation, and extraction of entries from XML sitemap documents, supporting both sitemap index files and urlset files. It implements security best practices for XML parsing to prevent common vulnerabilities like XXE attacks.</p> <p>SitemapDocument distinguishes between sitemap indexes (which contain references to other sitemaps) and urlsets (which contain actual page URLs), extracting the appropriate entries in each case.</p> <p>Attributes:</p> Name Type Description <code>SITEMAP_NS</code> <p>The official sitemap namespace URI</p> <code>entries</code> <code>ResourceList</code> <p>ResourceList containing the parsed sitemap entries</p> <code>type</code> <code>str</code> <p>The type of sitemap (sitemapindex, urlset, or unsupported)</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.sitemaps import SitemapDocument context = Context(Resource(\"https://example.com\")) sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; ...  ...    ...     https://example.com/page1 ...     2023-06-15T14:30:00Z ...    ... ''' sitemap = SitemapDocument(context, sitemap_xml) sitemap.type 'urlset' len(sitemap.entries) 1</p> Source code in <code>ethicrawl/sitemaps/sitemap_document.py</code> <pre><code>class SitemapDocument:\n    \"\"\"Parser and representation of XML sitemap documents.\n\n    This class handles the parsing, validation, and extraction of entries from\n    XML sitemap documents, supporting both sitemap index files and urlset files.\n    It implements security best practices for XML parsing to prevent common\n    vulnerabilities like XXE attacks.\n\n    SitemapDocument distinguishes between sitemap indexes (which contain references\n    to other sitemaps) and urlsets (which contain actual page URLs), extracting\n    the appropriate entries in each case.\n\n    Attributes:\n        SITEMAP_NS: The official sitemap namespace URI\n        entries: ResourceList containing the parsed sitemap entries\n        type: The type of sitemap (sitemapindex, urlset, or unsupported)\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.context import Context\n        &gt;&gt;&gt; from ethicrawl.core import Resource\n        &gt;&gt;&gt; from ethicrawl.sitemaps import SitemapDocument\n        &gt;&gt;&gt; context = Context(Resource(\"https://example.com\"))\n        &gt;&gt;&gt; sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n        ... &lt;urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;\n        ...   &lt;url&gt;\n        ...     &lt;loc&gt;https://example.com/page1&lt;/loc&gt;\n        ...     &lt;lastmod&gt;2023-06-15T14:30:00Z&lt;/lastmod&gt;\n        ...   &lt;/url&gt;\n        ... &lt;/urlset&gt;'''\n        &gt;&gt;&gt; sitemap = SitemapDocument(context, sitemap_xml)\n        &gt;&gt;&gt; sitemap.type\n        'urlset'\n        &gt;&gt;&gt; len(sitemap.entries)\n        1\n    \"\"\"\n\n    SITEMAP_NS = \"http://www.sitemaps.org/schemas/sitemap/0.9\"\n\n    def __init__(self, context: Context, document: str | None = None) -&gt; None:\n        \"\"\"Initialize a sitemap document parser with security protections.\n\n        Sets up the XML parser with security features to prevent common\n        XML vulnerabilities and optionally parses a provided document.\n\n        Args:\n            context: Context for logging and resource resolution\n            document: Optional XML sitemap content to parse immediately\n\n        Raises:\n            SitemapError: If the provided document cannot be parsed\n        \"\"\"\n        self._context = context\n        self._logger = self._context.logger(\"sitemap.document\")\n        # Add debug logging\n        self._logger.debug(\"Creating new SitemapDocument instance\")\n        self._entries: ResourceList = ResourceList()\n        self._parser = etree.XMLParser(\n            resolve_entities=False,  # Prevent XXE attacks\n            no_network=True,  # Prevent external resource loading\n            dtd_validation=False,  # Don't validate DTDs\n            load_dtd=False,  # Don't load DTDs at all\n            huge_tree=False,  # Prevent XML bomb attacks\n        )\n        if document is not None:\n            self._root = self._validate(document)\n\n    def _escape_unescaped_ampersands(self, xml_document: str) -&gt; str:\n        \"\"\"Escape unescaped ampersands in XML content.\n\n        Args:\n            xml_document: Raw XML string that may contain unescaped ampersands\n\n        Returns:\n            XML string with properly escaped ampersands\n        \"\"\"\n        pattern = r\"&amp;(?!(?:[a-zA-Z]+|#[0-9]+|#x[0-9a-fA-F]+);)\"\n        return sub(pattern, \"&amp;amp;\", xml_document)\n\n    def _validate(self, document: str) -&gt; etree._Element:\n        \"\"\"Validate and parse a sitemap XML document.\n\n        This method:\n        1. Escapes unescaped ampersands in the document\n        2. Parses the XML using the secure parser\n        3. Validates that it uses the correct sitemap namespace\n\n        Args:\n            document: XML document string to parse\n\n        Returns:\n            The parsed XML element tree root\n\n        Raises:\n            SitemapError: If the document has invalid XML or incorrect namespace\n        \"\"\"\n        document = self._escape_unescaped_ampersands(\n            document\n        )  # TODO: might want to move this to the HttpClient\n        try:\n            _element = etree.fromstring(document.encode(\"utf-8\"), parser=self._parser)\n            if _element.nsmap[None] != SitemapDocument.SITEMAP_NS:\n                self._logger.error(\n                    \"Required default namespace not found: %s\",\n                    SitemapDocument.SITEMAP_NS,\n                )\n                raise SitemapError(\n                    f\"Required default namespace not found: {SitemapDocument.SITEMAP_NS}\"\n                )\n            return _element\n        except Exception as e:\n            self._logger.error(\"Invalid XML syntax: %s\", e)\n            raise SitemapError(f\"Invalid XML syntax: {str(e)}\") from e\n\n    @property\n    def entries(self) -&gt; ResourceList:\n        \"\"\"Get the list of entries extracted from the sitemap.\n\n        Returns:\n            ResourceList containing SitemapEntry objects (either IndexEntry\n            or UrlsetEntry depending on the sitemap type)\n        \"\"\"\n        return self._entries\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Get the type of sitemap document.\n\n        Determines the type based on the root element's local name.\n\n        Returns:\n            String indicating the sitemap type:\n            - 'sitemapindex': A sitemap index containing references to other sitemaps\n            - 'urlset': A sitemap containing page URLs\n            - 'unsupported': Any other type of document\n\n        Raises:\n            SitemapError: If no document has been loaded\n        \"\"\"\n        if not hasattr(self, \"_root\"):  # pragma: no cover\n            raise SitemapError(\"No root name\")\n\n        localname = etree.QName(self._root.tag).localname\n        if localname in [SITEMAPINDEX, URLSET]:\n            self._logger.debug(\"Identified sitemap type: %s\", localname)\n            return localname\n\n        self._logger.warning(\"Unsupported sitemap document type: %s\", localname)\n        return \"unsupported\"\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument.entries","title":"<code>entries: ResourceList</code>  <code>property</code>","text":"<p>Get the list of entries extracted from the sitemap.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList containing SitemapEntry objects (either IndexEntry</p> <code>ResourceList</code> <p>or UrlsetEntry depending on the sitemap type)</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapDocument.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Get the type of sitemap document.</p> <p>Determines the type based on the root element's local name.</p> <p>Returns:</p> Type Description <code>str</code> <p>String indicating the sitemap type:</p> <code>str</code> <ul> <li>'sitemapindex': A sitemap index containing references to other sitemaps</li> </ul> <code>str</code> <ul> <li>'urlset': A sitemap containing page URLs</li> </ul> <code>str</code> <ul> <li>'unsupported': Any other type of document</li> </ul> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If no document has been loaded</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser","title":"<code>SitemapParser</code>","text":"<p>Recursive parser for extracting URLs from sitemap documents.</p> <p>This class handles the traversal of sitemap structures, including nested sitemap indexes, to extract all page URLs. It implements:</p> <ul> <li>Recursive traversal of sitemap indexes</li> <li>Depth limiting to prevent excessive recursion</li> <li>Cycle detection to prevent infinite loops</li> <li>URL deduplication</li> <li>Multiple input formats (IndexDocument, ResourceList, etc.)</li> </ul> <p>Attributes:</p> Name Type Description <code>context</code> <p>Context with client for fetching sitemaps and logging</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource, Url from ethicrawl.sitemaps import SitemapParser context = Context(Resource(Url(\"https://example.com\"))) parser = SitemapParser(context)</p> Source code in <code>ethicrawl/sitemaps/sitemap_parser.py</code> <pre><code>class SitemapParser:\n    \"\"\"Recursive parser for extracting URLs from sitemap documents.\n\n    This class handles the traversal of sitemap structures, including nested\n    sitemap indexes, to extract all page URLs. It implements:\n\n    - Recursive traversal of sitemap indexes\n    - Depth limiting to prevent excessive recursion\n    - Cycle detection to prevent infinite loops\n    - URL deduplication\n    - Multiple input formats (IndexDocument, ResourceList, etc.)\n\n    Attributes:\n        context: Context with client for fetching sitemaps and logging\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.context import Context\n        &gt;&gt;&gt; from ethicrawl.core import Resource, Url\n        &gt;&gt;&gt; from ethicrawl.sitemaps import SitemapParser\n        &gt;&gt;&gt; context = Context(Resource(Url(\"https://example.com\")))\n        &gt;&gt;&gt; parser = SitemapParser(context)\n        &gt;&gt;&gt; # Parse from a single sitemap URL\n        &gt;&gt;&gt; urls = parser.parse([Resource(Url(\"https://example.com/sitemap.xml\"))])\n        &gt;&gt;&gt; print(f\"Found {len(urls)} URLs in sitemap\")\n    \"\"\"\n\n    def __init__(self, context: Context):\n        \"\"\"Initialize the sitemap parser.\n\n        Args:\n            context: Context with client for fetching sitemaps and logging\n        \"\"\"\n        self._context = context\n        self._logger = self._context.logger(\"sitemap\")\n\n    def parse(\n        self,\n        root: IndexDocument | ResourceList | list[Resource] | None = None,\n    ) -&gt; ResourceList:\n        \"\"\"Parse sitemap(s) and extract all contained URLs.\n\n        This is the main entry point for sitemap parsing. It accepts various\n        input formats and recursively extracts all URLs from the sitemap(s).\n\n        Args:\n            root: Source to parse, which can be:\n                - IndexDocument: Pre-parsed sitemap index\n                - ResourceList: List of resources to fetch as sitemaps\n                - list[Resource]: List of resources to fetch as sitemaps\n                - None: Use the context's base URL for robots.txt discovery\n\n        Returns:\n            ResourceList containing all page URLs found in the sitemap(s)\n\n        Raises:\n            SitemapError: If a sitemap cannot be fetched or parsed\n        \"\"\"\n        self._logger.debug(\"Starting sitemap parsing\")\n\n        if isinstance(root, IndexDocument):\n            self._logger.debug(\"Parsing from provided IndexDocument\")\n            document = root\n        else:\n            # Handle different input types properly\n            if isinstance(root, ResourceList):\n                # Already a ResourceList, use directly\n                resources = root\n            else:\n                # Convert other list-like objects or None\n                resources = ResourceList(root or [])\n\n            document = IndexDocument(self._context)\n            for resource in resources:\n                document.entries.append(IndexEntry(resource.url))\n\n        return self._traverse(document, 0)\n\n    def _get(self, resource: Resource) -&gt; IndexDocument | SitemapDocument:\n        \"\"\"Fetch and parse a sitemap document from a resource.\n\n        Retrieves the resource using the context's client and attempts\n        to parse it as a sitemap document, determining the correct type\n        (index or urlset).\n\n        Args:\n            resource: Resource to fetch and parse\n\n        Returns:\n            Parsed sitemap document (either IndexDocument or UrlsetDocument)\n\n        Raises:\n            SitemapError: If the document cannot be fetched or parsed\n        \"\"\"\n        assert isinstance(self._context.client, Client)\n        self._logger.debug(\"Fetching sitemap from %s\", resource.url)\n        response = self._context.client.get(resource)\n\n        # Handle different response types\n        if hasattr(response, \"text\"):\n            self._logger.debug(\"Using text attribute from response\")\n            content = response.text  # pyright: ignore[reportAttributeAccessIssue]\n        elif hasattr(response, \"content\") and isinstance(response.content, str):\n            self._logger.debug(\"Using string content attribute from response\")\n            content = response.content\n        elif hasattr(response, \"content\"):\n            # Content attribute that needs decoding\n            content = response.content.decode(\"utf-8\")\n        else:\n            # Fallback - convert response to string\n            content = str(response)\n\n        document = SitemapDocument(self._context, content)\n        if document.type == URLSET:\n            return UrlsetDocument(self._context, content)\n        elif document.type == SITEMAPINDEX:\n            return IndexDocument(self._context, content)\n        self._logger.warning(\n            \"Unknown sitemap type with root element: %s\", document.type\n        )\n        raise SitemapError(f\"Unknown sitemap type with root element: {document.type}\")\n\n    def _traverse(\n        self, document: IndexDocument | SitemapDocument, depth: int = 0, visited=None\n    ) -&gt; ResourceList:\n        \"\"\"Recursively traverse a sitemap document and extract URLs.\n\n        Handles depth limiting and crawls nested sitemaps up to the\n        configured maximum depth.\n\n        Args:\n            document: Sitemap document to traverse\n            depth: Current recursion depth\n            visited: Set of already processed sitemap URLs to prevent cycles\n\n        Returns:\n            ResourceList containing all URLs found in the traverse\n        \"\"\"\n        # Collection of all found URLs\n        if not isinstance(document, IndexDocument):  # we shouldn't be here\n            return ResourceList()\n        max_depth = Config().sitemap.max_depth\n        all_urls: ResourceList = ResourceList([])\n\n        # Initialize visited set if this is the first call\n        if visited is None:\n            visited = set()\n\n        # Check if we've reached maximum depth\n        if depth &gt;= max_depth:\n            self._logger.warning(\n                \"Maximum recursion depth (%d) reached, stopping traversal\", max_depth\n            )\n            # Return empty ResourceList instead of None\n            return ResourceList()\n\n        self._logger.debug(\n            \"Traversing IndexDocument at depth %d, has %d items\",\n            depth,\n            len(document.entries),\n        )\n\n        for item in document.entries:\n            # Process each entry and collect any URLs found\n            urls = self._process_entry(item, depth, visited)\n            all_urls.extend(urls)\n\n        return all_urls\n\n    def _process_entry(\n        self, item: IndexEntry, depth: int, visited: set\n    ) -&gt; ResourceList:\n        \"\"\"Process a single sitemap entry, handling cycles and recursion.\n\n        Args:\n            item: Sitemap entry to process\n            depth: Current recursion depth\n            visited: Set of already processed sitemap URLs\n\n        Returns:\n            ResourceList of URLs found in this entry (and any nested entries)\n        \"\"\"\n        url_str = str(item.url)\n\n        # Check for cycles - skip if we've seen this URL before\n        if url_str in visited:\n            self._logger.warning(\n                \"Cycle detected: %s has already been processed\", url_str\n            )\n            return ResourceList()\n\n        self._logger.debug(\"Processing item: %s\", item.url)\n        document = self._get(Resource(item.url))\n\n        # Mark this URL as visited\n        visited.add(url_str)\n\n        if document.type == SITEMAPINDEX:\n            self._logger.debug(\n                \"Found index sitemap with %d items\", len(document.entries)\n            )\n            return self._traverse(document, depth + 1, visited)\n        elif document.type == URLSET:\n            self._logger.debug(\"Found urlset with %d URLs\", len(document.entries))\n            return document.entries\n\n        # Empty list for any unhandled cases\n        return ResourceList()  # pragma: no cover\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser--parse-from-a-single-sitemap-url","title":"Parse from a single sitemap URL","text":"<p>urls = parser.parse([Resource(Url(\"https://example.com/sitemap.xml\"))]) print(f\"Found {len(urls)} URLs in sitemap\")</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.SitemapParser.parse","title":"<code>parse(root: IndexDocument | ResourceList | list[Resource] | None = None) -&gt; ResourceList</code>","text":"<p>Parse sitemap(s) and extract all contained URLs.</p> <p>This is the main entry point for sitemap parsing. It accepts various input formats and recursively extracts all URLs from the sitemap(s).</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>IndexDocument | ResourceList | list[Resource] | None</code> <p>Source to parse, which can be: - IndexDocument: Pre-parsed sitemap index - ResourceList: List of resources to fetch as sitemaps - list[Resource]: List of resources to fetch as sitemaps - None: Use the context's base URL for robots.txt discovery</p> <code>None</code> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList containing all page URLs found in the sitemap(s)</p> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If a sitemap cannot be fetched or parsed</p> Source code in <code>ethicrawl/sitemaps/sitemap_parser.py</code> <pre><code>def parse(\n    self,\n    root: IndexDocument | ResourceList | list[Resource] | None = None,\n) -&gt; ResourceList:\n    \"\"\"Parse sitemap(s) and extract all contained URLs.\n\n    This is the main entry point for sitemap parsing. It accepts various\n    input formats and recursively extracts all URLs from the sitemap(s).\n\n    Args:\n        root: Source to parse, which can be:\n            - IndexDocument: Pre-parsed sitemap index\n            - ResourceList: List of resources to fetch as sitemaps\n            - list[Resource]: List of resources to fetch as sitemaps\n            - None: Use the context's base URL for robots.txt discovery\n\n    Returns:\n        ResourceList containing all page URLs found in the sitemap(s)\n\n    Raises:\n        SitemapError: If a sitemap cannot be fetched or parsed\n    \"\"\"\n    self._logger.debug(\"Starting sitemap parsing\")\n\n    if isinstance(root, IndexDocument):\n        self._logger.debug(\"Parsing from provided IndexDocument\")\n        document = root\n    else:\n        # Handle different input types properly\n        if isinstance(root, ResourceList):\n            # Already a ResourceList, use directly\n            resources = root\n        else:\n            # Convert other list-like objects or None\n            resources = ResourceList(root or [])\n\n        document = IndexDocument(self._context)\n        for resource in resources:\n            document.entries.append(IndexEntry(resource.url))\n\n    return self._traverse(document, 0)\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetEntry","title":"<code>UrlsetEntry</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SitemapEntry</code></p> <p>Represents an entry in a sitemap urlset file.</p> <p>UrlsetEntry specializes SitemapEntry for standard sitemap URL entries that contain page URLs with metadata. These entries represent actual content pages on a website, as opposed to index entries that point to other sitemap files.</p> <p>In addition to the URL and lastmod attributes inherited from SitemapEntry, UrlsetEntry adds support for: - changefreq: How frequently the page is likely to change - priority: Relative importance of this URL (0.0-1.0)</p> <p>All attributes are validated during initialization to ensure they conform to the sitemap protocol specification.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>Url</code> <p>URL of the page (inherited from Resource)</p> <code>lastmod</code> <code>str | None</code> <p>Last modification date (inherited from SitemapEntry)</p> <code>changefreq</code> <code>str | None</code> <p>Update frequency (always, hourly, daily, weekly, etc.)</p> <code>priority</code> <code>float | str | None</code> <p>Relative importance value from 0.0 to 1.0</p> Example <p>from ethicrawl.core import Url from ethicrawl.sitemaps import UrlsetEntry entry = UrlsetEntry( ...     Url(\"https://example.com/page1\"), ...     lastmod=\"2023-06-15T14:30:00Z\", ...     changefreq=\"weekly\", ...     priority=0.8 ... ) str(entry) 'https://example.com/page1 | last modified: 2023-06-15T14:30:00Z | frequency: weekly | priority: 0.8'</p> Source code in <code>ethicrawl/sitemaps/urlset_entry.py</code> <pre><code>@dataclass\nclass UrlsetEntry(SitemapEntry):\n    \"\"\"Represents an entry in a sitemap urlset file.\n\n    UrlsetEntry specializes SitemapEntry for standard sitemap URL entries\n    that contain page URLs with metadata. These entries represent actual\n    content pages on a website, as opposed to index entries that point\n    to other sitemap files.\n\n    In addition to the URL and lastmod attributes inherited from SitemapEntry,\n    UrlsetEntry adds support for:\n    - changefreq: How frequently the page is likely to change\n    - priority: Relative importance of this URL (0.0-1.0)\n\n    All attributes are validated during initialization to ensure they\n    conform to the sitemap protocol specification.\n\n    Attributes:\n        url: URL of the page (inherited from Resource)\n        lastmod: Last modification date (inherited from SitemapEntry)\n        changefreq: Update frequency (always, hourly, daily, weekly, etc.)\n        priority: Relative importance value from 0.0 to 1.0\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.core import Url\n        &gt;&gt;&gt; from ethicrawl.sitemaps import UrlsetEntry\n        &gt;&gt;&gt; entry = UrlsetEntry(\n        ...     Url(\"https://example.com/page1\"),\n        ...     lastmod=\"2023-06-15T14:30:00Z\",\n        ...     changefreq=\"weekly\",\n        ...     priority=0.8\n        ... )\n        &gt;&gt;&gt; str(entry)\n        'https://example.com/page1 | last modified: 2023-06-15T14:30:00Z | frequency: weekly | priority: 0.8'\n    \"\"\"\n\n    changefreq: str | None = None\n    priority: float | str | None = None\n\n    _valid_change_freqs = [\n        \"always\",\n        \"hourly\",\n        \"daily\",\n        \"weekly\",\n        \"monthly\",\n        \"yearly\",\n        \"never\",\n    ]\n\n    @staticmethod\n    def _validate_priority(value: str | float | int | None = None) -&gt; float | None:\n        \"\"\"\n        Validate and convert priority value.\n\n        Args:\n            value: Priority value as string or float\n\n        Returns:\n            float: Normalized priority value\n\n        Raises:\n            ValueError: If priority is not between 0.0 and 1.0\n        \"\"\"\n        if value is None:\n            return None\n\n        # Convert string to float if needed\n        if isinstance(value, str):\n            try:\n                value = float(value)\n            except ValueError as exc:\n                raise TypeError(\n                    f\"Priority must be a number, got '{type(value).__name__}'\"\n                ) from exc\n\n        # Always convert to float (handles integers)\n        value = float(value)\n\n        # Validate range\n        if not (0.0 &lt;= value &lt;= 1.0):\n            raise ValueError(f\"Priority must be between 0.0 and 1.0, got {value}\")\n\n        return value\n\n    @staticmethod\n    def _validate_changefreq(value: str | None = None) -&gt; str | None:\n        \"\"\"\n        Validate and normalize change frequency value.\n\n        Args:\n            value: Change frequency string or None\n\n        Returns:\n            str: Normalized change frequency (lowercase, stripped) or None\n\n        Raises:\n            TypeError: If changefreq is not a string\n            ValueError: If changefreq is not one of the valid values\n        \"\"\"\n        if value is None:\n            return None\n\n        if not isinstance(value, str):\n            raise TypeError(f\"changefreq must be a string, got {type(value).__name__}\")\n\n        # Normalize: strip and lowercase\n        normalized = value.strip().lower()\n\n        # Validate against valid frequencies\n        valid_freqs = [\n            \"always\",\n            \"hourly\",\n            \"daily\",\n            \"weekly\",\n            \"monthly\",\n            \"yearly\",\n            \"never\",\n        ]\n\n        if normalized not in valid_freqs:\n            raise ValueError(\n                f\"Invalid change frequency: '{value}'. Must be one of: {', '.join(valid_freqs)}\"\n            )\n\n        return normalized\n\n    def __post_init__(self):\n        \"\"\"Validate fields after initialization.\n\n        Calls the parent class validation, then validates and normalizes\n        the changefreq and priority attributes if they are provided.\n\n        Raises:\n            ValueError: If any field contains invalid values\n            TypeError: If any field has an incorrect type\n        \"\"\"\n        super().__post_init__()  # Call parent's validation\n\n        # Validate changefreq\n        if self.changefreq is not None:\n            self.changefreq = self._validate_changefreq(self.changefreq)\n\n        # Validate priority\n        if self.priority is not None:\n            self.priority = self._validate_priority(self.priority)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\n\n        Creates a pipe-separated string containing the URL and any available\n        metadata (lastmod, changefreq, priority).\n\n        Returns:\n            Formatted string with URL and metadata\n        \"\"\"\n        parts = [str(self.url)]\n\n        if self.lastmod:\n            parts.append(f\"last modified: {self.lastmod}\")\n        if self.changefreq:\n            parts.append(f\"frequency: {self.changefreq}\")\n        if self.priority is not None:\n            parts.append(f\"priority: {self.priority}\")\n\n        return \" | \".join(parts)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation for debugging.\n\n        Returns:\n            String representation showing class name and all field values\n        \"\"\"\n        return (\n            f\"SitemapUrlsetEntry(url='{str(self.url)}', lastmod={repr(self.lastmod)}, \"\n            f\"changefreq={repr(self.changefreq)}, priority={repr(self.priority)})\"\n        )\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetDocument","title":"<code>UrlsetDocument</code>","text":"<p>               Bases: <code>SitemapDocument</code></p> <p>Specialized parser for sitemap urlset documents.</p> <p>This class extends SitemapDocument to handle urlset sitemaps, which contain page URLs with metadata like change frequency and priority. It validates that the document is a proper urlset and extracts all URL references as UrlsetEntry objects.</p> <p>UrlsetDocument supports only the core sitemap protocol specification elements (loc, lastmod, changefreq, priority) and does not handle any sitemap protocol extensions.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>ResourceList</code> <p>ResourceList of UrlsetEntry objects representing page URLs</p> Example <p>from ethicrawl.context import Context from ethicrawl.core import Resource from ethicrawl.sitemaps import UrlsetDocument context = Context(Resource(\"https://example.com\")) sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; ...  ...    ...     https://example.com/page1 ...     2023-06-15T14:30:00Z ...     weekly ...     0.8 ...    ... ''' urlset = UrlsetDocument(context, sitemap_xml) len(urlset.entries) 1 entry = urlset.entries[0] entry.priority '0.8'</p> Source code in <code>ethicrawl/sitemaps/urlset_document.py</code> <pre><code>class UrlsetDocument(SitemapDocument):\n    \"\"\"Specialized parser for sitemap urlset documents.\n\n    This class extends SitemapDocument to handle urlset sitemaps,\n    which contain page URLs with metadata like change frequency and priority.\n    It validates that the document is a proper urlset and extracts all\n    URL references as UrlsetEntry objects.\n\n    UrlsetDocument supports only the core sitemap protocol specification\n    elements (loc, lastmod, changefreq, priority) and does not handle any\n    sitemap protocol extensions.\n\n    Attributes:\n        entries: ResourceList of UrlsetEntry objects representing page URLs\n\n    Example:\n        &gt;&gt;&gt; from ethicrawl.context import Context\n        &gt;&gt;&gt; from ethicrawl.core import Resource\n        &gt;&gt;&gt; from ethicrawl.sitemaps import UrlsetDocument\n        &gt;&gt;&gt; context = Context(Resource(\"https://example.com\"))\n        &gt;&gt;&gt; sitemap_xml = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n        ... &lt;urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;\n        ...   &lt;url&gt;\n        ...     &lt;loc&gt;https://example.com/page1&lt;/loc&gt;\n        ...     &lt;lastmod&gt;2023-06-15T14:30:00Z&lt;/lastmod&gt;\n        ...     &lt;changefreq&gt;weekly&lt;/changefreq&gt;\n        ...     &lt;priority&gt;0.8&lt;/priority&gt;\n        ...   &lt;/url&gt;\n        ... &lt;/urlset&gt;'''\n        &gt;&gt;&gt; urlset = UrlsetDocument(context, sitemap_xml)\n        &gt;&gt;&gt; len(urlset.entries)\n        1\n        &gt;&gt;&gt; entry = urlset.entries[0]\n        &gt;&gt;&gt; entry.priority\n        '0.8'\n    \"\"\"\n\n    def __init__(self, context: Context, document: str | None = None) -&gt; None:\n        \"\"\"Initialize a urlset sitemap document parser.\n\n        Args:\n            context: Context for logging and resource resolution\n            document: Optional XML urlset content to parse\n\n        Raises:\n            ValueError: If the document is not a valid urlset\n            SitemapError: If the document cannot be parsed\n        \"\"\"\n        super().__init__(context, document)\n        self._logger.debug(\"Creating UrlsetDocument instance\")\n\n        if document is not None:\n            _localname = etree.QName(self._root.tag).localname\n            if _localname != URLSET:\n                raise ValueError(f\"Expected a root {URLSET} got {_localname}\")\n            self._entries = self._parse_urlset_sitemap(document)\n            self._logger.debug(\"Parsed urlset with %d entries\", len(self._entries))\n\n    def _parse_urlset_sitemap(self, document) -&gt; ResourceList:\n        \"\"\"Parse page URLs from a urlset sitemap.\n\n        Extracts all &lt;url&gt; elements and their children (&lt;loc&gt;, &lt;lastmod&gt;,\n        &lt;changefreq&gt;, &lt;priority&gt;), creating UrlsetEntry objects for each\n        valid URL entry.\n\n        Args:\n            document: XML document string to parse\n\n        Returns:\n            ResourceList containing UrlsetEntry objects for each URL\n        \"\"\"\n        urlset: ResourceList = ResourceList()\n\n        nsmap = {\"\": self.SITEMAP_NS}\n        _root = etree.fromstring(document.encode(\"utf-8\"), parser=self._parser)\n\n        # Find all url elements\n        url_elements = _root.findall(\".//url\", namespaces=nsmap)\n        self._logger.debug(\"Found %d URL entries in urlset\", len(url_elements))\n\n        for url_elem in url_elements:\n            try:\n                loc_elem = url_elem.find(\"loc\", namespaces=nsmap)\n                if loc_elem is None or not loc_elem.text:\n                    continue\n\n                # Get optional elements\n                lastmod_elem = url_elem.find(\"lastmod\", namespaces=nsmap)\n                changefreq_elem = url_elem.find(\"changefreq\", namespaces=nsmap)\n                priority_elem = url_elem.find(\"priority\", namespaces=nsmap)\n\n                url = UrlsetEntry(\n                    url=Url(loc_elem.text),\n                    lastmod=lastmod_elem.text if lastmod_elem is not None else None,\n                    changefreq=(\n                        changefreq_elem.text if changefreq_elem is not None else None\n                    ),\n                    priority=(\n                        priority_elem.text if priority_elem is not None else None\n                    ),\n                )\n\n                urlset.append(url)\n            except ValueError as e:  # pragma: no cover\n                self._logger.warning(\"Error parsing sitemap reference: %s\", e)\n        return urlset\n\n    @property\n    def entries(self) -&gt; ResourceList:\n        \"\"\"Get the URLs in this urlset.\n\n        Returns:\n            ResourceList of UrlsetEntry objects representing page URLs\n        \"\"\"\n        return self._entries\n\n    @entries.setter\n    def entries(self, entries: ResourceList) -&gt; None:\n        \"\"\"Set the URLs in this urlset.\n\n        Args:\n            entries: List of page URLs as UrlsetEntry objects\n\n        Raises:\n            TypeError: If entries is not a ResourceList or contains non-UrlsetEntry objects\n        \"\"\"\n        if not isinstance(entries, ResourceList):\n            raise TypeError(\"entries must be a ResourceList\")\n        for entry in entries:\n            if not isinstance(entry, UrlsetEntry):\n                raise TypeError(\"entries must contain only UrlsetEntry objects\")\n        self._entries = entries\n</code></pre>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetDocument.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Get the type of sitemap document.</p> <p>Determines the type based on the root element's local name.</p> <p>Returns:</p> Type Description <code>str</code> <p>String indicating the sitemap type:</p> <code>str</code> <ul> <li>'sitemapindex': A sitemap index containing references to other sitemaps</li> </ul> <code>str</code> <ul> <li>'urlset': A sitemap containing page URLs</li> </ul> <code>str</code> <ul> <li>'unsupported': Any other type of document</li> </ul> <p>Raises:</p> Type Description <code>SitemapError</code> <p>If no document has been loaded</p>"},{"location":"api/sitemaps/#ethicrawl.sitemaps.UrlsetDocument.entries","title":"<code>entries: ResourceList</code>  <code>property</code> <code>writable</code>","text":"<p>Get the URLs in this urlset.</p> <p>Returns:</p> Type Description <code>ResourceList</code> <p>ResourceList of UrlsetEntry objects representing page URLs</p>"},{"location":"examples/01_basic_usage/","title":"Basic Usage","text":"<p>This guide demonstrates the fundamental operations with Ethicrawl: creating a crawler, binding to a site, making requests, and cleaning up resources.</p>"},{"location":"examples/01_basic_usage/#creating-an-ethicrawl-instance","title":"Creating an Ethicrawl Instance","text":"<p>First, import the main <code>Ethicrawl</code> class and create an instance:</p> <pre><code>from ethicrawl import Ethicrawl\n\n# Create a new crawler instance\ncrawler = Ethicrawl()\n</code></pre>"},{"location":"examples/01_basic_usage/#binding-to-a-website","title":"Binding to a Website","text":"<pre><code># Basic Usage\n\nThis guide demonstrates the fundamental operations with Ethicrawl: creating a crawler, binding to a site, making requests, and cleaning up resources.\n\n## Creating an Ethicrawl Instance\n\nFirst, import the main `Ethicrawl` class and create an instance:\n\n```python\nfrom ethicrawl import Ethicrawl\n\n# Create a new crawler instance\nethicrawl = Ethicrawl()\n</code></pre>"},{"location":"examples/01_basic_usage/#binding-to-a-website_1","title":"Binding to a Website","text":"<p>Before making requests, you must bind ethicrawl to a website. This establishes the domain context:</p> <pre><code># Bind to a website\nsite = \"https://www.bbc.co.uk/\"\nethicrawl.bind(site)\n\n# Check if we're successfully bound\nif ethicrawl.bound:\n    print(\"Crawler is bound to the BBC website\")\n</code></pre>"},{"location":"examples/01_basic_usage/#make-a-request-to-the-homepage","title":"Make a request to the homepage","text":"<pre><code>response = ethicrawl.get(site)\n\nprint(f\"Status code: {response.status_code}\")\nprint(f\"Content type: {response.headers.get('Content-Type')}\")\nprint(f\"Content length: {len(response.text)}\")\n</code></pre>"},{"location":"examples/01_basic_usage/#robots-support","title":"Robots support","text":"<p>The BBC website has an exclusion rule for search;</p> <p><code>Disallow: /search/</code></p> <pre><code>from ethicrawl.error import RobotsDisallowedError\n\nurl = site + \"/search/\"\n\ntry:\n    ethicrawl.get(url)\nexcept RobotsDisallowedError:\n    print(f\"The url f{url} was disallowed by robots.txt\")\n</code></pre>"},{"location":"examples/01_basic_usage/#cleanup","title":"Cleanup","text":"<p>Unbind to the website when you are finished with it, this will release all resources.</p> <pre><code>ethicrawl.unbind()\n</code></pre>"}]}