<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>ethicrawl.ethicrawl API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ethicrawl.ethicrawl</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ethicrawl.ethicrawl.ensure_bound"><code class="name flex">
<span>def <span class="ident">ensure_bound</span></span>(<span>func)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ensure_bound(func):
    &#34;&#34;&#34;
    Decorator to ensure the Ethicrawl instance is bound to a site.

    Raises:
        RuntimeError: If the instance is not bound to a site
    &#34;&#34;&#34;

    @wraps(func)
    def wrapper(self, *args, **kwargs):
        if not self.bound:
            raise RuntimeError(
                &#34;Operation requires binding to a site first. &#34;
                &#34;Call bind(url, client) before using this method.&#34;
            )
        return func(self, *args, **kwargs)

    return wrapper</code></pre>
</details>
<div class="desc"><p>Decorator to ensure the Ethicrawl instance is bound to a site.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If the instance is not bound to a site</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ethicrawl.ethicrawl.Ethicrawl"><code class="flex name class">
<span>class <span class="ident">Ethicrawl</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Ethicrawl:
    &#34;&#34;&#34;Main entry point for ethical web crawling operations.

    This class provides a simplified interface for crawling websites while respecting
    robots.txt rules, rate limits, and domain boundaries. It manages the lifecycle
    of crawling operations through binding to domains and provides access to robots.txt
    and sitemap functionality.

    Attributes:
        config (Config): Configuration settings for crawling behavior
        robots (Robot): Handler for robots.txt rules (available after binding)
        sitemaps (SitemapParser): Parser for XML sitemaps (available after binding)
        logger (Logger): Logger instance for this crawler (available after binding)
        bound (bool): Whether the crawler is currently bound to a site

    Example:
        &gt;&gt;&gt; from ethicrawl import Ethicrawl
        &gt;&gt;&gt; crawler = Ethicrawl()
        &gt;&gt;&gt; crawler.bind(&#34;https://example.com&#34;)
        &gt;&gt;&gt; response = crawler.get(&#34;https://example.com/about&#34;)
        &gt;&gt;&gt; print(response.status_code)
        200
        &gt;&gt;&gt; # Find URLs in sitemap
        &gt;&gt;&gt; urls = crawler.sitemaps.parse()
        &gt;&gt;&gt; crawler.unbind()  # Clean up when done
    &#34;&#34;&#34;

    def _get_root_domain(self) -&gt; DomainContext:
        &#34;&#34;&#34;Get the root domain context with type safety.

        Returns:
            The root domain context

        Raises:
            RuntimeError: If the root domain is not set
        &#34;&#34;&#34;
        if not hasattr(self, &#34;_root_domain&#34;) or self._root_domain is None:
            raise RuntimeError(&#34;Root domain not initialized&#34;)
        return cast(DomainContext, self._root_domain)

    def bind(self, url: str | Url | Resource, client: HttpClient | None = None) -&gt; bool:
        &#34;&#34;&#34;Bind the crawler to a specific website domain.

        Binding establishes the primary domain context with its robots.txt handler,
        client configuration, and sets up logging for operations on this domain.

        Args:
            url: The base URL of the site to crawl (string, Url, or Resource)
            client: HTTP client to use for requests. Defaults to a standard HttpClient

        Returns:
            bool: True if binding was successful

        Raises:
            ValueError: If URL is invalid
            RuntimeError: If already bound to a different site
        &#34;&#34;&#34;
        if self.bound:
            root_domain = self._get_root_domain()
            raise RuntimeError(
                f&#34;Already bound to {root_domain.context.resource.url} - unbind() first&#34;
            )

        self._root_domain: DomainContext | None = None
        self._whitelist: dict[str, DomainContext] = {}

        if isinstance(url, Resource):
            url = url.url
        url = Url(str(url), validate=True)
        resource = Resource(url)
        client = client or HttpClient()
        context = Context(resource, client)

        # Use DomainContext for the root domain
        self._root_domain = DomainContext(context=context)
        self.logger.info(&#34;Successfully bound to %s&#34;, url)
        return True

    def unbind(self) -&gt; bool:
        &#34;&#34;&#34;Unbind the crawler from its current site.

        This releases resources and allows the crawler to be bound to a different site.
        It removes all domain contexts, cached resources, and resets the crawler state.

        Returns:
            bool: True if unbinding was successful
        &#34;&#34;&#34;
        # Find all instance attributes starting with underscore
        if self.bound:
            domain = self._get_root_domain().context.resource.url.netloc
            self.logger.info(&#34;Unbinding from %s&#34;, domain)

        private_attrs = [attr for attr in vars(self) if attr.startswith(&#34;_&#34;)]

        # Delete each private attribute
        for attr in private_attrs:
            delattr(self, attr)

        # Verify unbinding was successful
        return not hasattr(self, &#34;_root_domain&#34;)

    @ensure_bound
    def whitelist(self, url: str | Url, client: HttpClient | None = None) -&gt; bool:
        &#34;&#34;&#34;
        Whitelist an additional domain for crawling.

        By default, Ethicrawl will only request URLs from the bound domain.
        Whitelisting allows accessing resources from other domains (like CDNs).

        Args:
            url (str or Url): URL from the domain to whitelist
            client (HttpClient, optional): Client to use for this domain

        Returns:
            bool: True if whitelisting was successful

        Raises:
            RuntimeError: If not bound to a primary site
        &#34;&#34;&#34;
        if isinstance(url, Resource):
            url = url.url
        url = Url(str(url), validate=True)

        domain = url.netloc
        root_domain = self._get_root_domain()
        context = Context(Resource(url), client or root_domain.context.client)

        self._whitelist[domain] = DomainContext(context=context)
        self.logger.info(&#34;Whitelisted domain: %s&#34;, domain)
        return True

    @property
    def bound(self) -&gt; bool:
        &#34;&#34;&#34;Check if currently bound to a site.

        Returns:
            bool: True if the crawler is bound to a domain, False otherwise
        &#34;&#34;&#34;
        return hasattr(self, &#34;_root_domain&#34;) and self._root_domain is not None

    @property
    def config(self) -&gt; Config:
        &#34;&#34;&#34;Access the configuration settings for this crawler.

        Returns:
            Config: The configuration object with settings for all crawler components
        &#34;&#34;&#34;
        return Config()

    @property
    @ensure_bound
    def logger(self) -&gt; logging_Logger:
        &#34;&#34;&#34;Get the logger for the current bound domain.

        This logger is configured according to the settings in Config.logger.

        Returns:
            Logger: Configured logger instance

        Raises:
            RuntimeError: If not bound to a site
        &#34;&#34;&#34;
        root_domain = self._get_root_domain()
        return root_domain.context.logger(&#34;&#34;)

    @property
    @ensure_bound
    def robots(self) -&gt; Robot:
        &#34;&#34;&#34;Access the robots.txt handler for the bound domain.

        The Robot instance manages fetching, parsing, and enforcing
        robots.txt rules for the current domain.

        Returns:
            Robot: The robots.txt handler for this domain

        Raises:
            RuntimeError: If not bound to a site
        &#34;&#34;&#34;
        root_domain = self._get_root_domain()
        return root_domain.robot

    @property
    @ensure_bound
    def sitemaps(self) -&gt; SitemapParser:
        &#34;&#34;&#34;Access the sitemap parser for the bound domain.

        The parser is created on first access and cached for subsequent calls.
        It provides methods to extract URLs from XML sitemaps.

        Returns:
            SitemapParser: Parser for handling XML sitemaps

        Raises:
            RuntimeError: If not bound to a site
        &#34;&#34;&#34;
        if not hasattr(self, &#34;_sitemap&#34;):
            root_domain = self._get_root_domain()
            self._sitemap = SitemapParser(root_domain.context)
        return self._sitemap

    @ensure_bound
    def get(
        self,
        url: str | Url | Resource,
        headers: Headers | dict | None = None,
    ) -&gt; Response | HttpResponse:
        &#34;&#34;&#34;Make an HTTP GET request to the specified URL, respecting robots.txt rules
        and domain whitelisting.

        This method enforces ethical crawling by:
        - Checking that the domain is allowed (primary or whitelisted)
        - Verifying the URL is permitted by robots.txt rules
        - Using the appropriate client for the domain

        Args:
            url: URL to fetch (string, Url, or Resource)
            headers: Additional headers for this request

        Returns:
            Response or HttpResponse: The response from the server

        Raises:
            ValueError: If URL is from a non-whitelisted domain or disallowed by robots.txt
            RuntimeError: If not bound to a site
            TypeError: If url parameter is not a string, Url, or Resource
        &#34;&#34;&#34;
        # Handle different types of URL input
        if isinstance(url, Resource):
            resource = url
        elif isinstance(url, (str, Url)):
            resource = Resource(Url(str(url)))
        else:
            raise TypeError(
                f&#34;Expected string, Url, or Resource, got {type(url).__name__}&#34;
            )

        self.logger.debug(&#34;Preparing to fetch %s&#34;, resource.url)

        # Get domain from URL
        target_domain = resource.url.netloc

        # Check if domain is allowed
        root_domain = self._get_root_domain()
        domain_ctx = (
            root_domain
            if target_domain == root_domain.context.resource.url.netloc
            else self._whitelist.get(target_domain)
        )

        if domain_ctx is None:
            # Fix incorrect curly brace string formatting
            self.logger.warning(&#34;Domain not allowed: %s&#34;, target_domain)
            raise ValueError(f&#34;Domain not allowed: {target_domain}&#34;)
        else:
            self.logger.debug(&#34;Using domain context for %s&#34;, target_domain)

        context = domain_ctx.context
        robot = domain_ctx.robot

        # Extract User-Agent from headers if present (for robots.txt checking)
        user_agent = None
        if headers:
            headers = Headers(headers)
            user_agent = headers.get(&#34;User-Agent&#34;)

        # See if we can fetch the resource
        if robot.can_fetch(resource, user_agent=user_agent):
            self.logger.debug(&#34;Request permitted by robots.txt policy&#34;)

        # Use the domain&#39;s context to get its client
        if isinstance(context.client, HttpClient):
            return context.client.get(resource, headers=headers)
        return context.client.get(resource)</code></pre>
</details>
<div class="desc"><p>Main entry point for ethical web crawling operations.</p>
<p>This class provides a simplified interface for crawling websites while respecting
robots.txt rules, rate limits, and domain boundaries. It manages the lifecycle
of crawling operations through binding to domains and provides access to robots.txt
and sitemap functionality.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>Config</code></dt>
<dd>Configuration settings for crawling behavior</dd>
<dt><strong><code>robots</code></strong> :&ensp;<code>Robot</code></dt>
<dd>Handler for robots.txt rules (available after binding)</dd>
<dt><strong><code>sitemaps</code></strong> :&ensp;<code>SitemapParser</code></dt>
<dd>Parser for XML sitemaps (available after binding)</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>Logger</code></dt>
<dd>Logger instance for this crawler (available after binding)</dd>
<dt><strong><code>bound</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the crawler is currently bound to a site</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from ethicrawl import Ethicrawl
&gt;&gt;&gt; crawler = Ethicrawl()
&gt;&gt;&gt; crawler.bind(&quot;https://example.com&quot;)
&gt;&gt;&gt; response = crawler.get(&quot;https://example.com/about&quot;)
&gt;&gt;&gt; print(response.status_code)
200
&gt;&gt;&gt; # Find URLs in sitemap
&gt;&gt;&gt; urls = crawler.sitemaps.parse()
&gt;&gt;&gt; crawler.unbind()  # Clean up when done
</code></pre></div>
<h3>Instance variables</h3>
<dl>
<dt id="ethicrawl.ethicrawl.Ethicrawl.bound"><code class="name">prop <span class="ident">bound</span> : bool</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bound(self) -&gt; bool:
    &#34;&#34;&#34;Check if currently bound to a site.

    Returns:
        bool: True if the crawler is bound to a domain, False otherwise
    &#34;&#34;&#34;
    return hasattr(self, &#34;_root_domain&#34;) and self._root_domain is not None</code></pre>
</details>
<div class="desc"><p>Check if currently bound to a site.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if the crawler is bound to a domain, False otherwise</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.config"><code class="name">prop <span class="ident">config</span> : <a title="ethicrawl.config.config.Config" href="config/config.html#ethicrawl.config.config.Config">Config</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def config(self) -&gt; Config:
    &#34;&#34;&#34;Access the configuration settings for this crawler.

    Returns:
        Config: The configuration object with settings for all crawler components
    &#34;&#34;&#34;
    return Config()</code></pre>
</details>
<div class="desc"><p>Access the configuration settings for this crawler.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Config</code></dt>
<dd>The configuration object with settings for all crawler components</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.logger"><code class="name">prop <span class="ident">logger</span> : logging.Logger</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
@ensure_bound
def logger(self) -&gt; logging_Logger:
    &#34;&#34;&#34;Get the logger for the current bound domain.

    This logger is configured according to the settings in Config.logger.

    Returns:
        Logger: Configured logger instance

    Raises:
        RuntimeError: If not bound to a site
    &#34;&#34;&#34;
    root_domain = self._get_root_domain()
    return root_domain.context.logger(&#34;&#34;)</code></pre>
</details>
<div class="desc"><p>Get the logger for the current bound domain.</p>
<p>This logger is configured according to the settings in Config.logger.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Logger</code></dt>
<dd>Configured logger instance</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If not bound to a site</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.robots"><code class="name">prop <span class="ident">robots</span> : <a title="ethicrawl.robots.robot.Robot" href="robots/robot.html#ethicrawl.robots.robot.Robot">Robot</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
@ensure_bound
def robots(self) -&gt; Robot:
    &#34;&#34;&#34;Access the robots.txt handler for the bound domain.

    The Robot instance manages fetching, parsing, and enforcing
    robots.txt rules for the current domain.

    Returns:
        Robot: The robots.txt handler for this domain

    Raises:
        RuntimeError: If not bound to a site
    &#34;&#34;&#34;
    root_domain = self._get_root_domain()
    return root_domain.robot</code></pre>
</details>
<div class="desc"><p>Access the robots.txt handler for the bound domain.</p>
<p>The Robot instance manages fetching, parsing, and enforcing
robots.txt rules for the current domain.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Robot</code></dt>
<dd>The robots.txt handler for this domain</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If not bound to a site</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.sitemaps"><code class="name">prop <span class="ident">sitemaps</span> : <a title="ethicrawl.sitemaps.sitemap_parser.SitemapParser" href="sitemaps/sitemap_parser.html#ethicrawl.sitemaps.sitemap_parser.SitemapParser">SitemapParser</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
@ensure_bound
def sitemaps(self) -&gt; SitemapParser:
    &#34;&#34;&#34;Access the sitemap parser for the bound domain.

    The parser is created on first access and cached for subsequent calls.
    It provides methods to extract URLs from XML sitemaps.

    Returns:
        SitemapParser: Parser for handling XML sitemaps

    Raises:
        RuntimeError: If not bound to a site
    &#34;&#34;&#34;
    if not hasattr(self, &#34;_sitemap&#34;):
        root_domain = self._get_root_domain()
        self._sitemap = SitemapParser(root_domain.context)
    return self._sitemap</code></pre>
</details>
<div class="desc"><p>Access the sitemap parser for the bound domain.</p>
<p>The parser is created on first access and cached for subsequent calls.
It provides methods to extract URLs from XML sitemaps.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SitemapParser</code></dt>
<dd>Parser for handling XML sitemaps</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If not bound to a site</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ethicrawl.ethicrawl.Ethicrawl.bind"><code class="name flex">
<span>def <span class="ident">bind</span></span>(<span>self,<br>url: str | <a title="ethicrawl.core.url.Url" href="core/url.html#ethicrawl.core.url.Url">Url</a> | <a title="ethicrawl.core.resource.Resource" href="core/resource.html#ethicrawl.core.resource.Resource">Resource</a>,<br>client: <a title="ethicrawl.client.http.http_client.HttpClient" href="client/http/http_client.html#ethicrawl.client.http.http_client.HttpClient">HttpClient</a> | None = None) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bind(self, url: str | Url | Resource, client: HttpClient | None = None) -&gt; bool:
    &#34;&#34;&#34;Bind the crawler to a specific website domain.

    Binding establishes the primary domain context with its robots.txt handler,
    client configuration, and sets up logging for operations on this domain.

    Args:
        url: The base URL of the site to crawl (string, Url, or Resource)
        client: HTTP client to use for requests. Defaults to a standard HttpClient

    Returns:
        bool: True if binding was successful

    Raises:
        ValueError: If URL is invalid
        RuntimeError: If already bound to a different site
    &#34;&#34;&#34;
    if self.bound:
        root_domain = self._get_root_domain()
        raise RuntimeError(
            f&#34;Already bound to {root_domain.context.resource.url} - unbind() first&#34;
        )

    self._root_domain: DomainContext | None = None
    self._whitelist: dict[str, DomainContext] = {}

    if isinstance(url, Resource):
        url = url.url
    url = Url(str(url), validate=True)
    resource = Resource(url)
    client = client or HttpClient()
    context = Context(resource, client)

    # Use DomainContext for the root domain
    self._root_domain = DomainContext(context=context)
    self.logger.info(&#34;Successfully bound to %s&#34;, url)
    return True</code></pre>
</details>
<div class="desc"><p>Bind the crawler to a specific website domain.</p>
<p>Binding establishes the primary domain context with its robots.txt handler,
client configuration, and sets up logging for operations on this domain.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong></dt>
<dd>The base URL of the site to crawl (string, Url, or Resource)</dd>
<dt><strong><code>client</code></strong></dt>
<dd>HTTP client to use for requests. Defaults to a standard HttpClient</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if binding was successful</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If URL is invalid</dd>
<dt><code>RuntimeError</code></dt>
<dd>If already bound to a different site</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self,<br>url: str | <a title="ethicrawl.core.url.Url" href="core/url.html#ethicrawl.core.url.Url">Url</a> | <a title="ethicrawl.core.resource.Resource" href="core/resource.html#ethicrawl.core.resource.Resource">Resource</a>,<br>headers: <a title="ethicrawl.core.headers.Headers" href="core/headers.html#ethicrawl.core.headers.Headers">Headers</a> | dict | None = None) ‑> <a title="ethicrawl.client.response.Response" href="client/response.html#ethicrawl.client.response.Response">Response</a> | <a title="ethicrawl.client.http.http_response.HttpResponse" href="client/http/http_response.html#ethicrawl.client.http.http_response.HttpResponse">HttpResponse</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@ensure_bound
def get(
    self,
    url: str | Url | Resource,
    headers: Headers | dict | None = None,
) -&gt; Response | HttpResponse:
    &#34;&#34;&#34;Make an HTTP GET request to the specified URL, respecting robots.txt rules
    and domain whitelisting.

    This method enforces ethical crawling by:
    - Checking that the domain is allowed (primary or whitelisted)
    - Verifying the URL is permitted by robots.txt rules
    - Using the appropriate client for the domain

    Args:
        url: URL to fetch (string, Url, or Resource)
        headers: Additional headers for this request

    Returns:
        Response or HttpResponse: The response from the server

    Raises:
        ValueError: If URL is from a non-whitelisted domain or disallowed by robots.txt
        RuntimeError: If not bound to a site
        TypeError: If url parameter is not a string, Url, or Resource
    &#34;&#34;&#34;
    # Handle different types of URL input
    if isinstance(url, Resource):
        resource = url
    elif isinstance(url, (str, Url)):
        resource = Resource(Url(str(url)))
    else:
        raise TypeError(
            f&#34;Expected string, Url, or Resource, got {type(url).__name__}&#34;
        )

    self.logger.debug(&#34;Preparing to fetch %s&#34;, resource.url)

    # Get domain from URL
    target_domain = resource.url.netloc

    # Check if domain is allowed
    root_domain = self._get_root_domain()
    domain_ctx = (
        root_domain
        if target_domain == root_domain.context.resource.url.netloc
        else self._whitelist.get(target_domain)
    )

    if domain_ctx is None:
        # Fix incorrect curly brace string formatting
        self.logger.warning(&#34;Domain not allowed: %s&#34;, target_domain)
        raise ValueError(f&#34;Domain not allowed: {target_domain}&#34;)
    else:
        self.logger.debug(&#34;Using domain context for %s&#34;, target_domain)

    context = domain_ctx.context
    robot = domain_ctx.robot

    # Extract User-Agent from headers if present (for robots.txt checking)
    user_agent = None
    if headers:
        headers = Headers(headers)
        user_agent = headers.get(&#34;User-Agent&#34;)

    # See if we can fetch the resource
    if robot.can_fetch(resource, user_agent=user_agent):
        self.logger.debug(&#34;Request permitted by robots.txt policy&#34;)

    # Use the domain&#39;s context to get its client
    if isinstance(context.client, HttpClient):
        return context.client.get(resource, headers=headers)
    return context.client.get(resource)</code></pre>
</details>
<div class="desc"><p>Make an HTTP GET request to the specified URL, respecting robots.txt rules
and domain whitelisting.</p>
<p>This method enforces ethical crawling by:
- Checking that the domain is allowed (primary or whitelisted)
- Verifying the URL is permitted by robots.txt rules
- Using the appropriate client for the domain</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong></dt>
<dd>URL to fetch (string, Url, or Resource)</dd>
<dt><strong><code>headers</code></strong></dt>
<dd>Additional headers for this request</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Response</code> or <code>HttpResponse</code></dt>
<dd>The response from the server</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If URL is from a non-whitelisted domain or disallowed by robots.txt</dd>
<dt><code>RuntimeError</code></dt>
<dd>If not bound to a site</dd>
<dt><code>TypeError</code></dt>
<dd>If url parameter is not a string, Url, or Resource</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.unbind"><code class="name flex">
<span>def <span class="ident">unbind</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unbind(self) -&gt; bool:
    &#34;&#34;&#34;Unbind the crawler from its current site.

    This releases resources and allows the crawler to be bound to a different site.
    It removes all domain contexts, cached resources, and resets the crawler state.

    Returns:
        bool: True if unbinding was successful
    &#34;&#34;&#34;
    # Find all instance attributes starting with underscore
    if self.bound:
        domain = self._get_root_domain().context.resource.url.netloc
        self.logger.info(&#34;Unbinding from %s&#34;, domain)

    private_attrs = [attr for attr in vars(self) if attr.startswith(&#34;_&#34;)]

    # Delete each private attribute
    for attr in private_attrs:
        delattr(self, attr)

    # Verify unbinding was successful
    return not hasattr(self, &#34;_root_domain&#34;)</code></pre>
</details>
<div class="desc"><p>Unbind the crawler from its current site.</p>
<p>This releases resources and allows the crawler to be bound to a different site.
It removes all domain contexts, cached resources, and resets the crawler state.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if unbinding was successful</dd>
</dl></div>
</dd>
<dt id="ethicrawl.ethicrawl.Ethicrawl.whitelist"><code class="name flex">
<span>def <span class="ident">whitelist</span></span>(<span>self,<br>url: str | <a title="ethicrawl.core.url.Url" href="core/url.html#ethicrawl.core.url.Url">Url</a>,<br>client: <a title="ethicrawl.client.http.http_client.HttpClient" href="client/http/http_client.html#ethicrawl.client.http.http_client.HttpClient">HttpClient</a> | None = None) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@ensure_bound
def whitelist(self, url: str | Url, client: HttpClient | None = None) -&gt; bool:
    &#34;&#34;&#34;
    Whitelist an additional domain for crawling.

    By default, Ethicrawl will only request URLs from the bound domain.
    Whitelisting allows accessing resources from other domains (like CDNs).

    Args:
        url (str or Url): URL from the domain to whitelist
        client (HttpClient, optional): Client to use for this domain

    Returns:
        bool: True if whitelisting was successful

    Raises:
        RuntimeError: If not bound to a primary site
    &#34;&#34;&#34;
    if isinstance(url, Resource):
        url = url.url
    url = Url(str(url), validate=True)

    domain = url.netloc
    root_domain = self._get_root_domain()
    context = Context(Resource(url), client or root_domain.context.client)

    self._whitelist[domain] = DomainContext(context=context)
    self.logger.info(&#34;Whitelisted domain: %s&#34;, domain)
    return True</code></pre>
</details>
<div class="desc"><p>Whitelist an additional domain for crawling.</p>
<p>By default, Ethicrawl will only request URLs from the bound domain.
Whitelisting allows accessing resources from other domains (like CDNs).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code> or <code>Url</code></dt>
<dd>URL from the domain to whitelist</dd>
<dt><strong><code>client</code></strong> :&ensp;<code>HttpClient</code>, optional</dt>
<dd>Client to use for this domain</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if whitelisting was successful</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If not bound to a primary site</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ethicrawl" href="index.html">ethicrawl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ethicrawl.ethicrawl.ensure_bound" href="#ethicrawl.ethicrawl.ensure_bound">ensure_bound</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ethicrawl.ethicrawl.Ethicrawl" href="#ethicrawl.ethicrawl.Ethicrawl">Ethicrawl</a></code></h4>
<ul class="two-column">
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.bind" href="#ethicrawl.ethicrawl.Ethicrawl.bind">bind</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.bound" href="#ethicrawl.ethicrawl.Ethicrawl.bound">bound</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.config" href="#ethicrawl.ethicrawl.Ethicrawl.config">config</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.get" href="#ethicrawl.ethicrawl.Ethicrawl.get">get</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.logger" href="#ethicrawl.ethicrawl.Ethicrawl.logger">logger</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.robots" href="#ethicrawl.ethicrawl.Ethicrawl.robots">robots</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.sitemaps" href="#ethicrawl.ethicrawl.Ethicrawl.sitemaps">sitemaps</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.unbind" href="#ethicrawl.ethicrawl.Ethicrawl.unbind">unbind</a></code></li>
<li><code><a title="ethicrawl.ethicrawl.Ethicrawl.whitelist" href="#ethicrawl.ethicrawl.Ethicrawl.whitelist">whitelist</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
